// [
    {
        "id": "BAJfDMr7SOVXqPIV6D7Y",
        "title": "Introducing an Optimization for Valid JSON Generation in SGLang",
        "markdown": "## Introducing an Optimization for Valid JSON Generation in SGLang\n\n[https://twitter.com/lmsysorg/status/1754516251123257385](https://twitter.com/lmsysorg/status/1754516251123257385)\n\nControlling LLMs to always generate valid JSON is crucial for many applications. We've introduced an optimization in SGLang that achieves this goal and also:\n\n- Makes JSON decoding 3x faster than normal decoding, utilizing a compressed finite state machine\n- Supports any regular‚Ä¶ [https://t.co/iwiOZOa1Qt[https://t.co/bCbbmryMFi](https://t.co/bCbbmryMFi)](https://t.co/iwiOZOa1Qt)\n\nHere is another example using LLaVA! You can use this feature to extract structured information from images. [https://t.co/sjEugHKoGj](https://t.co/sjEugHKoGj)\n\nThe code was released several weeks ago and is now undergoing testing by multiple users. We welcome your feedback and any interesting use cases you'd like to share.\n",
        "tags": [],
        "created": "2024-02-05T16:11:02.493Z",
        "updated": "2024-02-05T16:11:02.493Z"
    },
    {
        "id": "TsWWUf6lI4LUtHp6U8qp",
        "title": "RAG Cheatsheet v2: A Comprehensive Guide to Troubleshooting RAG Systems",
        "markdown": "## RAG Cheatsheet v2: A Comprehensive Guide to Troubleshooting RAG Systems\n\n[https://twitter.com/jerryjliu0/status/1753923945789903351](https://twitter.com/jerryjliu0/status/1753923945789903351)\n\nRAG Cheatsheet v2 üí´\n\nWe're excited to highlight an expanded RAG cheatsheet by @wenqi_glantz which outlines twelve pain points across the stages of data ingestion, retrieval, querying, and the solutions for each pain point.\n\nThere's too many parameters to tune in a RAG system and‚Ä¶ [https://t.co/GoLJJATZ4A](https://t.co/GoLJJATZ4A)\n\n![](https://pbs.twimg.com/media/GFcv2zfbUAAeweq.jpg)[https://t.co/EWkcUhC44o](https://t.co/EWkcUhC44o)\n\n@jerryjliu0 @wenqi_glantz\n",
        "tags": [],
        "created": "2024-02-05T14:42:06.503Z",
        "updated": "2024-02-05T14:42:06.503Z"
    },
    {
        "id": "BSY8Ms0r6A8T5kMB5ZD0",
        "title": "Excited to release Natural-SQL-7B!",
        "markdown": "## Excited to release Natural-SQL-7B!\n\n[https://twitter.com/calebfahlgren/status/1754246846535340317](https://twitter.com/calebfahlgren/status/1754246846535340317)\n\nExcited to release Natural-SQL-7B!\n\nA new, very strong Text to SQL model that is my best fine tune yet. You can see how it does on the SQL-Eval benchmark by @defogdata\n\n[https://t.co/m1r3gIFL1R[https://t.co/Hg3r0Tgvcl](https://t.co/Hg3r0Tgvcl)](https://t.co/m1r3gIFL1R)\n\n![](https://pbs.twimg.com/media/GFhR-_7XgAARRig.jpg)\n\nAll the GGUFs for the peopleü§ù\n[https://t.co/9Vm1wlo4Hb](https://t.co/9Vm1wlo4Hb)\n\nHopefully I can get it on @ollama soon!\n",
        "tags": [],
        "created": "2024-02-05T03:44:02.474Z",
        "updated": "2024-02-05T03:44:02.474Z"
    },
    {
        "id": "UHJNQcTBvpuh4uNEdWds",
        "title": "\"Introducing Rawdog: A Natural Language CLI for Python Scripts\"",
        "markdown": "## \"Introducing Rawdog: A Natural Language CLI for Python Scripts\"\n\n[https://twitter.com/granawkins/status/1753380448502440261](https://twitter.com/granawkins/status/1753380448502440261)\n\nI built a natural language CLI.\n\nIt generates Python scripts to answer your question, then auto-executes them in the cwd. You will not believe how capable this simple pattern is. Rawdogging gpt-4 from the command line.\n\nRawdog.\n1/ [https://t.co/0cYV2Lw8fz](https://t.co/0cYV2Lw8fz)\n\nConversational git, docker, matplotlib, pandas..\n\nSummarize, scan or refactor basically any type of data\n\n‚ÄúFollow the instructions in readme to set this up..‚Äù\n\n‚ÄúFind all my venvs and plot their sizes‚Äù\n\n‚ÄúDo I have Ruby installed?‚Äù\n2/\n\nRawdog doesn‚Äôt use RAG!\n\nIt can ‚Äúselect its own context‚Äù by printing things to the output and then printing ‚ÄòCONTINUE‚Äô (from its script). The output is added to the conversation, then it rawdogs the same prompt again.\n3/ \n\n![](https://pbs.twimg.com/media/GFVBzmAbkAAh5Bt.png)\n\nRawdog‚Äôs outputs are 100% interpretable, because you (and it) can see the script that generated them. If you ask, it‚Äôll tell you exactly what it did and why.\n4/ [https://t.co/NdUwx9FSWk](https://t.co/NdUwx9FSWk)\n\nHere‚Äôs the full system prompt:\n\nYou are a command-line coding assistant called Rawdog that generates and auto-executes Python scripts.\n\nA typical interaction goes like this:\n1. The user gives you a natural language PROMPT.\n2. You:\ni. Determine what needs to be done\nii. Write a‚Ä¶ [https://t.co/HYVpG9qZs2](https://t.co/HYVpG9qZs2)\n\nHere‚Äôs the repo: [https://t.co/Y2DQjpKv6K](https://t.co/Y2DQjpKv6K)\n\nYou can install it with pip:\n > pip install rawdog-ai\nstart a conversation:\n > rawdog\nor do single-shot:\n > rawdog your question here\nreview/approve each script before running with:\n > rawdog --dry-run\n\n6/\n\nThis project came out of the Mentat hackathon last week - was a funny experiment that turned out to be super useful. Look out for integrations with the Mentat coding assistant soon üëÄ\n[https://t.co/yutGsDpkqb](https://t.co/yutGsDpkqb)\n7/7\n\nPS if you're on hackernews: [https://t.co/YfEc6xG2YC](https://t.co/YfEc6xG2YC)\n\nPPS - it's official: [https://t.co/AOqvsPegMi](https://t.co/AOqvsPegMi)\n",
        "tags": [],
        "created": "2024-02-03T18:59:03.832Z",
        "updated": "2024-02-03T18:59:03.832Z"
    },
    {
        "id": "n6PERGhGTOsCceYr1olr",
        "title": "One-Click RAG Pipeline Generator",
        "markdown": "## One-Click RAG Pipeline Generator\n\n[https://twitter.com/llama_index/status/1753478149743284395](https://twitter.com/llama_index/status/1753478149743284395)\n\nOne-Click RAG Pipeline Generator\n\nWe‚Äôre excited to feature RAGArch by @HarshadSurya1c, which provides a comprehensive @streamlit UI to pick and choose all the core components of a RAG system: LLM, embedding model, chunking, vector store.\n\nIn one click you will 1) create a fully‚Ä¶ [https://t.co/MH6DbsAQpb[https://t.co/4omNwxq24C](https://t.co/4omNwxq24C)](https://t.co/MH6DbsAQpb)\n\n@llama_index @HarshadSurya1c @streamlit\n",
        "tags": [],
        "created": "2024-02-02T18:07:03.432Z",
        "updated": "2024-02-02T18:07:03.432Z"
    },
    {
        "id": "tqDIGzhAyEym5w8gYrgC",
        "title": "OpenCopilot: The Ultimate Tool for Product Automation and User Assistance",
        "markdown": "## OpenCopilot: The Ultimate Tool for Product Automation and User Assistance\n\n[https://twitter.com/ikbenbasha/status/1752241769683829048](https://twitter.com/ikbenbasha/status/1752241769683829048)\n\nOpenCopilot is at 4.6K GitHub stars! (bye bye BETA)‚≠ê‚≠ê\n\n[https://t.co/r5w0Peszjd](https://t.co/r5w0Peszjd)\n\nAnd we are live on ProductHunt!üê±\n\n‚û°Ô∏è It's like Siri but for your product.\n\nIntegrate a Text/Voice AI User Assistant, and Workflow Automation into your product, all in one powerful tool.‚ú®\n\nWe‚Ä¶ [https://t.co/X2iTMpKv6W](https://t.co/X2iTMpKv6W)\n\n@ikbenbasha\n",
        "tags": [],
        "created": "2024-01-31T00:37:03.289Z",
        "updated": "2024-01-31T00:37:03.289Z"
    },
    {
        "id": "zJkZr0DhnvUhJdEhqJNQ",
        "title": "Use this to evaluate LLM SQL generation:",
        "markdown": "Use this to evaluate LLM SQL generation:\n[https://github.com/defog-ai/sql-eval](https://github.com/defog-ai/sql-eval)\n\nUse this to compare Vanna AI vs something like defog sql\n",
        "tags": [],
        "created": "2024-01-30T15:56:36.920Z",
        "updated": "2024-01-30T15:57:03.099Z"
    },
    {
        "id": "Kx7l3V6i4kvj1nbEbdKf",
        "title": "https://github.com/billmei/every-chatgpt-gui",
        "markdown": "[https://github.com/billmei/every-chatgpt-gui](https://github.com/billmei/every-chatgpt-gui)\n",
        "tags": [],
        "created": "2024-01-27T18:20:37.893Z",
        "updated": "2024-01-27T18:20:37.893Z"
    },
    {
        "id": "6ZjnyV7jf2D1X13vcsXs",
        "title": "Introducing RAG CLI - A Command-Line Tool for Indexing and Searching Files",
        "markdown": "## Introducing RAG CLI - A Command-Line Tool for Indexing and Searching Files\n\n[https://twitter.com/llama_index/status/1750950516925079777](https://twitter.com/llama_index/status/1750950516925079777)\n\nIntroducing RAG CLI üßë\u200düíªüîé - a dead-simple command-line tool that allows you to RAG literally any file on your local machine.\n\nIndex any files including glob patterns, such as `$ llamaindex-cli rag --files \"./docs/**/*.rst‚Äù`\n\nTo search simply do `$ llamaindex-cli rag --question‚Ä¶ [https://t.co/B0VPw2swJ1[https://t.co/QYW21JTwFi](https://t.co/QYW21JTwFi)](https://t.co/B0VPw2swJ1)\n\n@llama_index\n",
        "tags": [],
        "created": "2024-01-27T01:31:02.721Z",
        "updated": "2024-01-27T01:31:02.721Z"
    },
    {
        "id": "1eQ7iOALbQZDX61BccYo",
        "title": "\"Build a Slack Bot for Organizational Communication: A Step-by-Step Guide\"",
        "markdown": "## \"Build a Slack Bot for Organizational Communication: A Step-by-Step Guide\"\n\n[https://twitter.com/llama_index/status/1750568734140567611](https://twitter.com/llama_index/status/1750568734140567611)\n\n‚ú®New OSS repo‚ú® Build a Slack bot that listens to your conversations, learns from them, and can answer questions about what's going on across your organization!\n\nIn @seldo's step-by-step guide you'll learn how to\n‚û°Ô∏è Build a @SlackHQ bot from scratch\n‚û°Ô∏è Install it to your Slack‚Ä¶ [https://t.co/E8KJNeoXfr](https://t.co/E8KJNeoXfr)\n\n![](https://pbs.twimg.com/media/GEtE29iakAEJ1UR.jpg)\n\n@llama_index @seldo @SlackHQ\n",
        "tags": [],
        "created": "2024-01-25T17:46:02.354Z",
        "updated": "2024-01-25T17:46:02.354Z"
    },
    {
        "id": "2aerdzfd8eILRMoLWOSZ",
        "title": "Using RAG for Advanced Text-to-SQL Generation with Vanna AI",
        "markdown": "## Using RAG for Advanced Text-to-SQL Generation with Vanna AI\n\n[https://twitter.com/llama_index/status/1750196064660127848](https://twitter.com/llama_index/status/1750196064660127848)\n\nUse RAG to build advanced text-to-SQL\n\nVanna AI (@zain_hoda) is a project that gained overnight popularity for SQL generation given its easy-to-use but powerful interface that uses RAG for better performance:\n\n1Ô∏è‚É£ Store and index DDL/table schemas\n2Ô∏è‚É£ Store and index text‚Ä¶ [https://t.co/NVGPa4OnYS](https://t.co/NVGPa4OnYS)\n\n![](https://pbs.twimg.com/media/GEnyCJvawAAX84c.jpg)\n\n@llama_index @zain_hoda\n",
        "tags": [],
        "created": "2024-01-24T19:51:02.392Z",
        "updated": "2024-01-24T19:51:02.392Z"
    },
    {
        "id": "L29e5aqyvVIcPueUBoHQ",
        "title": "Advanced Chat Bot Built with LlamaIndex: A Closer Look at MemGPT",
        "markdown": "## Advanced Chat Bot Built with LlamaIndex: A Closer Look at MemGPT\n\n[https://twitter.com/llama_index/status/1749935634033348693](https://twitter.com/llama_index/status/1749935634033348693)\n\nBasic chat is easy, but a delightful chat experience has a lot of subtleties! In today's OSS project spotlight, [https://t.co/COUh9LAKzR](https://t.co/COUh9LAKzR) from @charlespacker is an advanced chat bot built with LlamaIndex.\n\nMemGPT uses function-calling by LLMs under the hood to maintain multiple‚Ä¶ [https://t.co/Zd0d9Aczk3[https://t.co/iZ0mRQUBbq](https://t.co/iZ0mRQUBbq)](https://t.co/Zd0d9Aczk3)\n\n@llama_index @charlespacker\n",
        "tags": [],
        "created": "2024-01-24T18:23:03.377Z",
        "updated": "2024-01-24T18:23:03.377Z"
    },
    {
        "id": "9oHOrrR5yGQ29vd0IRbp",
        "title": "AWS Knowledge Bases for RAG: A Fully Managed System for Private Data Retrieval",
        "markdown": "## AWS Knowledge Bases for RAG: A Fully Managed System for Private Data Retrieval\n\n[https://twitter.com/BraceSproul/status/1750212202341237033](https://twitter.com/BraceSproul/status/1750212202341237033)\n\nüß† AWS Knowledge Bases For RAG üß†\n\nAWS Knowledge Base for Bedrock is a fully managed system for RAG applications and private data retrieval, now available thanks to a community PR in @LangChainAI JS/TS ü¶úüîó\n\nStore your proprietary data in AWS, point Knowledge Base to that data,‚Ä¶ [https://t.co/HLQw1OFHc6](https://t.co/HLQw1OFHc6)\n\n![](https://pbs.twimg.com/media/GEoALlAa4AII4Og.jpg)\n\nSee the full documentation here:\n[https://t.co/SONuANhHlk](https://t.co/SONuANhHlk)\n\nAnd install the latest version of `@langchain/community` here:\n[https://t.co/2lPdWsmC4w](https://t.co/2lPdWsmC4w)\n",
        "tags": [],
        "created": "2024-01-24T18:09:03.646Z",
        "updated": "2024-01-24T18:09:03.646Z"
    },
    {
        "id": "Lz7PwpVqcbQSK3DNn0tP",
        "title": "A Live Indexing Pipeline for RAG",
        "markdown": "## A Live Indexing Pipeline for RAG\n\n[https://twitter.com/llama_index/status/1748515130885968274](https://twitter.com/llama_index/status/1748515130885968274)\n\nA Live Indexing Pipeline for RAG\n\nSuppose you‚Äôre building a production RAG app on AWS, and you want it to always have access to the latest information‚ö°Ô∏è (instead of waiting a few hours for a batch indexing pipeline to run). Here‚Äôs how you can do that üí°\n\n@kiennt_ has one of the‚Ä¶ [https://t.co/mV0m6AtVsl](https://t.co/mV0m6AtVsl)\n\n![](https://pbs.twimg.com/media/GEP5NjXaUAA5eWk.jpg)\n\n@llama_index @kiennt_\n",
        "tags": [],
        "created": "2024-01-20T03:30:03.026Z",
        "updated": "2024-01-20T03:30:03.026Z"
    },
    {
        "id": "yKEV8Z79ENjRsjsrFbhk",
        "title": "https://johnthenerd.com/blog/local-llm-assistant/",
        "markdown": "[https://johnthenerd.com/blog/local-llm-assistant/](https://johnthenerd.com/blog/local-llm-assistant/)\n",
        "tags": [],
        "created": "2024-01-14T19:56:55.848Z",
        "updated": "2024-01-14T19:56:55.848Z"
    },
    {
        "id": "IMp3sPWWnlD8eYNO4iW1",
        "title": "Improving Text Splitting Techniques for RAG in 2024",
        "markdown": "## Improving Text Splitting Techniques for RAG in 2024\n\n[https://twitter.com/jerryjliu0/status/1745486856291266821](https://twitter.com/jerryjliu0/status/1745486856291266821)\n\nText splitting for RAG has been quite hacky. Let‚Äôs fix that in 2024!\nüö´ Broke: fixed chunk sizes\n‚úÖ Woke: Document-specific parsing, decoupled chunks for retrieval and synthesis, adaptive chunking\n\nI like @GregKamradt‚Äôs semantic chunking technique - it‚Äôs clever, fast (w/‚Ä¶ [https://t.co/mKqiU7B3kx[https://t.co/CqSrLqlfvh](https://t.co/CqSrLqlfvh)](https://t.co/mKqiU7B3kx)\n\nThese thoughts are similra to @thesephist a few months ago: [https://t.co/X4Fl1AwKlB](https://t.co/X4Fl1AwKlB)\n",
        "tags": [],
        "created": "2024-01-12T06:15:02.969Z",
        "updated": "2024-01-12T06:15:02.969Z"
    },
    {
        "id": "8ayYUFDxG9nTouzLMXQj",
        "title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM",
        "markdown": "## SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM\n\n[https://twitter.com/Nik__V__/status/1731840557553496410](https://twitter.com/Nik__V__/status/1731840557553496410)\n\nSplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM\n\n[https://t.co/NRxmwdK39s](https://t.co/NRxmwdK39s)\n\nWe extend Gaussian Splatting to solve SLAM, i.e., automatically calculate the camera poses when fitting the Gaussian scene from RGB-D videos.\n\nTry it on your own iPhone capture today! üßµüëá [https://t.co/cyznoWSFuN](https://t.co/cyznoWSFuN)\n\nAccurate camera poses are super important for Gaussian Splatting but are annoying and slow to obtain (e.g., with COLMAP). This prevents real-time applications like Robotics and VR/AR.\n\nWith SplaTAM, we obtain accurate camera poses while fitting the scene with Gaussian Splatting. [https://t.co/aPg00wTFIe](https://t.co/aPg00wTFIe)\n\nOn many difficult scenes, our approach works much better than all previous SOTA SLAM approaches. Below, we show a difficult texture-less scene where ORB-SLAM 3 completely fails, whereas ours achieves almost perfect results (first two tweets above). [https://t.co/YyzA4IPFbm](https://t.co/YyzA4IPFbm)\n\nWe have released code for a real-time, online streaming demo, where for the first time, you can fit a Gaussian Splatting Scene live while it‚Äôs being recorded, without requiring any camera poses as input.\n\n[https://t.co/kEUXWEV27d](https://t.co/kEUXWEV27d)\n\nImagine all of the uses in Robotics & AR/VR. [https://t.co/Ot7Ao0zCqY](https://t.co/Ot7Ao0zCqY)\n\nThe key is to render a visibility silhouette from the Gaussian Scene to determine areas that have and haven‚Äôt been seen before.\n\nUsing this, we can ignore unknown areas when optimizing for a new camera pose and also determine where to add new Gaussians during densification. [https://t.co/pS4GduQQ0D](https://t.co/pS4GduQQ0D)\n\nAwesome collaboration with Jay (@JayKarhade), Krishna (@_krishna_murthy), Gengshaan (@gengshanY), Basti (@smash0190 @AirLabCMU), Deva Ramanan (@RamananDeva) and Jonathon (@JonathonLuiten).\n\nThanks for their amazing support during this project! [https://t.co/WxcXZz9x87](https://t.co/WxcXZz9x87)\n\nWant to learn more about SplaTAM?\n\nCheck out our explainer video üí°: [https://t.co/1HZSY5ViXg](https://t.co/1HZSY5ViXg)\n",
        "tags": [],
        "created": "2024-01-11T14:42:04.536Z",
        "updated": "2024-01-11T14:42:04.536Z"
    },
    {
        "id": "EGGYrvbBxo8bnmiq9IsX",
        "title": "Using RAGatouille to Effortlessly Rerank Documents with ColBERT",
        "markdown": "## Using RAGatouille to Effortlessly Rerank Documents with ColBERT\n\n[https://twitter.com/bclavie/status/1745151278018076985](https://twitter.com/bclavie/status/1745151278018076985)\n\n\"I wish I could use the best retrieval model as a reranker in my RAG pipeline, but I don't have time to redesign it!\" \n\nWe've all been there and thought that, right?\n\nü™§RAGatouille now lets you do it effortlessly! Retrieve docs as usual&let ColBERT rerank them in a single line‚¨áÔ∏è \n\n![](https://pbs.twimg.com/media/GDfTglbXQAAK6e4.jpg)\n\nOf course, there's a mini-example too, w/ a typical case where ColBERT can catch information single-vectors might not: [https://t.co/QAmqLr1heb](https://t.co/QAmqLr1heb)\n\nThough I hear @hwchase17 has got something cooking to showcase how to use this in @LangChainAI to make it even simpler üëÄ\n",
        "tags": [],
        "created": "2024-01-11T14:35:04.624Z",
        "updated": "2024-01-11T14:35:04.624Z"
    },
    {
        "id": "zZJ2fbagXJWQ44J9ew36",
        "title": "Using OLLAMA to Get Structured JSON Output from LLM Models",
        "markdown": "## Using OLLAMA to Get Structured JSON Output from LLM Models\n\n[https://twitter.com/llama_index/status/1744853380953158038](https://twitter.com/llama_index/status/1744853380953158038)\n\nGetting structured output like JSON out of an LLM is both super-useful and sometimes tricky! In this video @andrejusb walks us through using @OLLAMA to run a local model and use Pydantic classes to get structured JSON out of invoices:\n[https://t.co/16EAggnURi](https://t.co/16EAggnURi)\n\nCheck out the open‚Ä¶ [https://t.co/oe0mVuZSYE](https://t.co/oe0mVuZSYE)\n\n![](https://pbs.twimg.com/media/GDb1n_2acAAP54z.jpg)\n\n@llama_index @andrejusb @OLLAMA\n",
        "tags": [],
        "created": "2024-01-09T23:00:03.019Z",
        "updated": "2024-01-09T23:00:03.019Z"
    },
    {
        "id": "vUySUJkJJN9Dwx84ZOq9",
        "title": "Fine-tuning a Mistral 7B in MLX on WhatsApp Chat History Locally",
        "markdown": "## Fine-tuning a Mistral 7B in MLX on WhatsApp Chat History Locally\n\n[https://twitter.com/awnihannun/status/1744497202389754254](https://twitter.com/awnihannun/status/1744497202389754254)\n\nFun example: Fine tune a Mistral 7B in MLX locally on your WhatsApp chat history.\n\nCompletely local, no need to send data anywhere.\n\nCode: [https://t.co/hrA7nEcPOZ[https://t.co/jk9bMyCPKJ](https://t.co/jk9bMyCPKJ)](https://t.co/hrA7nEcPOZ)\n\n@awnihannun\n",
        "tags": [],
        "created": "2024-01-09T18:18:02.849Z",
        "updated": "2024-01-09T18:18:02.849Z"
    },
    {
        "id": "H4cEnPSy3mzg4ofM5RU8",
        "title": "https://www.reddit.com/r/LocalLLaMA/s/RUrt2Jc8rm",
        "markdown": "[https://www.reddit.com/r/LocalLLaMA/s/RUrt2Jc8rm](https://www.reddit.com/r/LocalLLaMA/s/RUrt2Jc8rm)\n",
        "tags": [],
        "created": "2024-01-09T05:16:11.605Z",
        "updated": "2024-01-09T05:16:11.605Z"
    },
    {
        "id": "T6jS372pR0Brru6yTYuP",
        "title": "The Power of Scripts: My Experience at GitHub",
        "markdown": "## The Power of Scripts: My Experience at GitHub\n\n[https://twitter.com/HamelHusain/status/1744466145401598396](https://twitter.com/HamelHusain/status/1744466145401598396)\n\nI learned lots of wonderful things when I worked at GitHub, but the stickiest is scripts-to-rule them-all\n\n[https://t.co/sfYxYjskZy](https://t.co/sfYxYjskZy)\n\n@HamelHusain\n",
        "tags": [],
        "created": "2024-01-09T01:14:02.215Z",
        "updated": "2024-01-09T01:14:02.215Z"
    },
    {
        "id": "GTbA8W96lYMCBpybH9In",
        "title": "Introducing Ingestion Pipeline: A New Way to Manage Documents in @llama_index",
        "markdown": "## Introducing Ingestion Pipeline: A New Way to Manage Documents in @llama_index\n\n[https://twitter.com/clusteredbytes/status/1744081053008965846](https://twitter.com/clusteredbytes/status/1744081053008965846)\n\n**ü™ÑSmart summary:**\nIngestion Pipeline is a new and improved way to ingest and manage documents in @llama_index. It supports applying a series of transformations on documents, caching those transformations, and managing ever-changing documents. Transformations are the building blocks of Ingestion Pipeline, and each transformation takes a list of nodes and returns another list of nodes after making the desired modifications. Ingestion Pipeline also supports caching, document management, and persisting. Check out the official documentation and blog for more details.\n-------\n\nIngestion Pipeline is a new and improved way to ingest and manage documents in @llama_index\n\nIt supports:\n\n- applying a series of transformation on documents\n- caching those transformations\n- managing ever-changing documents etc.\n\nLet's see how to use it üëáüßµ \n\n![](https://pbs.twimg.com/media/GDQ4eNRWkAA5Way.jpg)\n\nTransformations are the building blocks of Ingestion Pipeline.\n\nEach transformation takes a list of nodes, and returns another list of nodes after making the desired modifications to them.\n\nWe define the transformations while instantiating the pipeline itself. \n\n![](https://pbs.twimg.com/media/GDQ4fKrW8AASreb.png)\n\nThese are the transformations we can use:\n\n1. TextSplitter\n2. NodeParser\n3. MetadataExtractor\n4. Any embedding model\n\nWe can also create custom transformations. Guide on this is coming soon.\n\nOutput of one transformation is the input to the next one.\n\nCaching\n\nEach (transformation-it's input nodes list) pair is cached in the pipeline.\n\nWhen we run the same transformation on the same list of input nodes, the output nodes will be fetched from cache.\n\nWe can also use remote cache i.e. Redis \n\n![](https://pbs.twimg.com/media/GDQ4f8-XsAA4H2g.png)\n\nTo make sure that we don‚Äôt run a transformation on same document multiple times, Ingestion pipeline uses the document id and the hash of the document content to handle duplicates.\n\nwe need to pass a docstore to the pipeline to enable this. \n\n![](https://pbs.twimg.com/media/GDQ4gV4WIAAQz0f.png)\n\nThe hashes of existing documents will be compared to the hashes of input documents. and unchanged documents will be excluded from transformation.\n\nThere are 3 strategies for document management.\n\n1. duplicates only\n2. upsert\n3. upsert and delete\n\nGuide on them coming soon.\n\nIf we pass a vectorstore to the pipeline, then it will automatically add the final output nodes from the transformation sequence to that vectorstore.\n\nwe can then use that populated vectorstore to create a vectorstore index. \n\n![](https://pbs.twimg.com/media/GDQ4hOAXgAAIqY6.png)\n\nPersisting\n\nIngestion pipeline allows persisting the cache and the docstore to a folder (./pipeline_storage by default)\n\nafter defining the pipeline, we need to load it using pipeline.load() \n\n![](https://pbs.twimg.com/media/GDQ4iBEXsAA1tmv.png)\n\nThus ingestion pipeline makes it really efficient and intuitive to ingest and manage documents and apply a sequence of transformation on them üöÄ\n\nCheckout the official documentation\n[https://t.co/JnmVHLp2FP](https://t.co/JnmVHLp2FP)\n\nDetails about ingestion pipeline and other cool RAG guides on my blog:\n[https://t.co/nXUoSbJzFr](https://t.co/nXUoSbJzFr)\n\nThanks for reading.\n\nI write about AI, LLMs, RAG etc. and try to make complex topics as easy as possible. \n\nStay tuned for more ! üî• #AI\n #RAG\n [https://t.co/XJbf7Wwbro](https://t.co/XJbf7Wwbro)\n",
        "tags": [
            "AI",
            "RAG"
        ],
        "created": "2024-01-08T03:24:05.182Z",
        "updated": "2024-01-08T03:24:05.182Z"
    },
    {
        "id": "zTixXh8io2QDV9ujXWhG",
        "title": "https://www.reddit.com/r/StableDiffusion/s/TVTpB559QY",
        "markdown": "[https://www.reddit.com/r/StableDiffusion/s/TVTpB559QY](https://www.reddit.com/r/StableDiffusion/s/TVTpB559QY)\n",
        "tags": [],
        "created": "2024-01-04T20:06:30.803Z",
        "updated": "2024-01-04T20:06:30.803Z"
    },
    {
        "id": "1do7I4fXVIOkI3VVIm6h",
        "title": "Building an AI Shopping Assistant with LlamaIndex Agents",
        "markdown": "## Building an AI Shopping Assistant with LlamaIndex Agents\n\n[https://twitter.com/llama_index/status/1742970871713783847](https://twitter.com/llama_index/status/1742970871713783847)\n\n**ü™ÑSmart summary:**\nLlamaIndex agents can be used to create an AI-powered shopping assistant that can match accessories to items you already own. This blog post from @dkiedanski and Lucas Micol of @tryolabs shows how APIs can be used to create this assistant.\n-------\n\nUse LlamaIndex agents to build an AI-powered shopping assistant that can take a picture of an item you already own and pair it with (weather-appropriate) accessories!\n\nWe love this blog post from @dkiedanski and Lucas Micol of @tryolabs that shows how you can take APIs and turn‚Ä¶ [https://t.co/XiNdkjksNm](https://t.co/XiNdkjksNm)\n\n![](https://pbs.twimg.com/media/GDBEl4baYAMQHWB.jpg)\n\n@llama_index @dkiedanski @tryolabs\n",
        "tags": [],
        "created": "2024-01-04T19:25:04.063Z",
        "updated": "2024-01-04T19:25:04.063Z"
    },
    {
        "id": "v5csKSa2KLlOMRCEqkj2",
        "title": "AI Product ideas",
        "markdown": "AI Product ideas\n\n- A smart picture frame that cycles through AI generated pictures on your account\n- Give it access to your Google photo album, and select from a drop down of LoRAs in different styles\n- Recreate pics in album using LoRA, serve via API\n- Frame pulls from API however often, gets random remix'ed pic back.\n- Target boomers\n\n\n- Upload your pic/head shot\n- Select 'scenario' - old west, sci-fi, fantasy, etc\n- Train dreambooth on headshot w/ scenario background \n- Generate link to purchase pics as a book from pic to book service\n",
        "tags": [],
        "created": "2024-01-03T23:06:45.815Z",
        "updated": "2024-01-03T23:09:55.110Z"
    },
    {
        "id": "uAMaZmFSBKhy8vEwgSyS",
        "title": "Introducing Private AI: Chat with Your Data Privately and Free of Cost",
        "markdown": "## Introducing Private AI: Chat with Your Data Privately and Free of Cost\n\n[https://twitter.com/taranjeetio/status/1742214295755354530](https://twitter.com/taranjeetio/status/1742214295755354530)\n\n**ü™ÑSmart summary:**\nIntroducing Private AI, a free and simple way to privately chat with your data without the need for an internet connection. Powered by LangChainAI and GPT4All, this open source code allows users to ingest any folder on their machine and chat with it, regardless of file type. This is especially useful for sensitive and personal data such as finances and important documents.\n-------\n\nIntroducing Private AI - privately chat with your data\nusing @embedchain\n\n‚Ä¢ All data stays on your machine\n‚Ä¢ Works on CPU machine\n‚Ä¢ Simple (4 lines of code)\n‚Ä¢ No internet required\n‚Ä¢ Free of cost\n\nPowered by @LangChainAI & GPT4All from @nomic_ai\n\nCommon use case of RAG is chat with your data but main concern is privacy, and cost.\n\n‚Ä¢ Users worry if their data would be use to train the next model.\n‚Ä¢¬†Users don‚Äôt want to spend money.\n\nThanks to open source that we have good models like GPT4All, Llama, @MistralAI.\n\nToday, we are introducing the simplest flow to create a RAG app which will ingest any folder on your machine and help you chat with it.\n\n‚Ä¢ Just import & instantiate the app\n‚Ä¢ Add the entire folder\n‚Ä¢ Start chatting \n\n![](https://pbs.twimg.com/media/GC2WrbnawAAQBMs.jpg)\n\nIn above example, I have taken a personal folder which contains data related to my finances\n\n‚Ä¢ Bank statement as csv\n‚Ä¢ 2 credit card statements as pdf\n‚Ä¢ Financial goals as markdown \n\n![](https://pbs.twimg.com/media/GC2WsKbbUAAUi_U.jpg)\n\nWe support all types of format, so you don‚Äôt need to worry about file types.\n\nAlso all data stays on your machine and you can chat with it by turning off your internet.\n\nThis is really helpful for sensitive and personal data like finances, important documents, etc.\n\nWe have open sourced the entire code here:\n\n[https://t.co/DBZDiylXd2](https://t.co/DBZDiylXd2)\n\nDo check it out and share your feedback in the comments.\n",
        "tags": [],
        "created": "2024-01-03T15:28:04.341Z",
        "updated": "2024-01-03T15:28:04.341Z"
    },
    {
        "id": "BG2gaykV9HFimEz6qqJK",
        "title": "Launching a Repo for a Production ETL Pipeline for RAG/LLM Apps",
        "markdown": "## Launching a Repo for a Production ETL Pipeline for RAG/LLM Apps\n\n[https://twitter.com/llama_index/status/1742226327900721168](https://twitter.com/llama_index/status/1742226327900721168)\n\n**ü™ÑSmart summary:**\nToday, a new repository was launched that allows users to quickly and easily set up a production ETL pipeline for their RAG/LLM app. This architecture bundles LlamaIndex with other popular tools, allowing users to index thousands of documents in seconds, and orders of magnitude faster than running on a laptop.\n-------\n\nToday we‚Äôre launching a repo that lets you setup a production ETL pipeline for your RAG/LLM app üí´\n\nIndex thousands of documents in seconds ‚ö°Ô∏è (and orders of magnitude faster than running on your laptop).\n\nIt‚Äôs a full architecture which bundles LlamaIndex with other popular‚Ä¶ [https://t.co/TbW6D6PwwK](https://t.co/TbW6D6PwwK)\n\n![](https://pbs.twimg.com/media/GC2ha_CbIAAg_nH.png)\n\n@llama_index\n",
        "tags": [],
        "created": "2024-01-02T17:40:04.463Z",
        "updated": "2024-01-02T17:40:04.463Z"
    },
    {
        "id": "igLK9A8nILrSOoTx8Nca",
        "title": "Building a Context-Augmented Food Delivery Agent",
        "markdown": "## Building a Context-Augmented Food Delivery Agent\n\n[https://twitter.com/llama_index/status/1741987664272986534](https://twitter.com/llama_index/status/1741987664272986534)\n\n**ü™ÑSmart summary:**\nThis tweet features a repository by lucastonon that shows how to build a RAG agent that can look up relevant restaurants and perform in-browser actions such as opening a page, getting the menu, and adding food.\n-------\n\nContext-Augmented Agent for Food Delivery ü§ñüçî\n\nWe‚Äôre excited to feature a full-stack repo by lucastonon showing you how to build a RAG agent that can not only look up relevant restaurants, but directly perform in-browser actions like opening a page, getting the menu, adding food‚Ä¶ [https://t.co/cLJ653RTnT[https://t.co/nWzjGrG2jC](https://t.co/nWzjGrG2jC)](https://t.co/cLJ653RTnT)\n\n@llama_index\n",
        "tags": [],
        "created": "2024-01-02T05:38:04.080Z",
        "updated": "2024-01-02T05:38:04.080Z"
    },
    {
        "id": "6jaextWCgoVZ8ybgPTCs",
        "title": "Building a Full-Stack RAG Chatbot Web App",
        "markdown": "## Building a Full-Stack RAG Chatbot Web App\n\n[https://twitter.com/llama_index/status/1741562214853837296](https://twitter.com/llama_index/status/1741562214853837296)\n\n**ü™ÑSmart summary:**\nThis thread introduces a full-stack RAG starter pack, which includes MongoDB, Flask, Render, and Next.js. It provides a guide to building a chatbot web app with storage, backend/frontend, and deployment to the web. The mongodb-demo repo by @seldo is also included.\n-------\n\nFull-stack RAG Starter Pack üîéüì¶ - with @MongoDB, Flask, @render, and @nextjs \n\nIt‚Äôs one thing to build RAG in a notebook, it‚Äôs another thing to build a full-stack chatbot web app, with storage, backend/frontend, and deployed to the web üåê\n\nOur mongodb-demo repo by @seldo is an‚Ä¶ [https://t.co/uRyhWyfKi3[https://t.co/FxNFZ6QWMc](https://t.co/FxNFZ6QWMc)](https://t.co/uRyhWyfKi3)\n\n@llama_index @MongoDB @render @nextjs @seldo\n",
        "tags": [],
        "created": "2024-01-01T00:09:04.748Z",
        "updated": "2024-01-01T00:09:04.748Z"
    },
    {
        "id": "vjDGL0VuDVa2DPfq04u1",
        "title": "Amazing Open-Source AI Chat - MIT Licensed!",
        "markdown": "## Amazing Open-Source AI Chat - MIT Licensed!\n\n[https://twitter.com/phuctm97/status/1734413225071092025](https://twitter.com/phuctm97/status/1734413225071092025)\n\n**ü™ÑSmart summary:**\nThis open-source AI chat is impressive and has a MIT license, which means someone can self-host it and make a profit from it.\n-------\n\nMan, this open-source AI chat is almost too good to be true. And it's MIT licensed ü§Ø.\n\n[https://t.co/HahwiY5GrT](https://t.co/HahwiY5GrT)\n\nIt‚Äôs literally better than many paid ChatGPT UI. And the fact that it‚Äôs MIT licensed is interesting, someone can just self host and make profit from it ü§®\n",
        "tags": [],
        "created": "2023-12-12T15:36:31.161Z",
        "updated": "2023-12-12T15:36:31.161Z"
    },
    {
        "id": "irmUhLZeJIrgphOiTavA",
        "title": "Visualizing Text Splitting and Chunking Strategies with ChunkViz.com",
        "markdown": "## Visualizing Text Splitting and Chunking Strategies with ChunkViz.com\n\n[https://twitter.com/GregKamradt/status/1733208049513611339](https://twitter.com/GregKamradt/status/1733208049513611339)\n\n**ü™ÑSmart summary:**\nI built a tool called ChunkViz.com to visualize text chunking strategies. It features 4 different splitters from LangChainAI and is visually pleasing to tinker with. I'm also writing a post on chunking philosophies which will be published on my website. However, I have a beef with the tool as it trims the chunks, making it hard to reconstruct the visualization.\n-------\n\nvisualizing text splitting & chunking strategies\n\nChunkViz .com \n\nI thought I remembered a tool to visualize text chunking, but I couldn't find it, so I built one\n\nI didn't realize it would be so visually pleasing to tinker with\n\n4 different @LangChainAI splitters featured [https://t.co/vN2kdIBgxd[https://t.co/4VywxHdqAa](https://t.co/4VywxHdqAa)](https://t.co/vN2kdIBgxd)\n\nHead over to [https://t.co/NqskxGf5aX](https://t.co/NqskxGf5aX) to try it out\n\nThis is part of a post I'm doing on chunking philosophies. I'll publish on [https://t.co/pkIRUAAzQ0](https://t.co/pkIRUAAzQ0)\n\nCode: [https://t.co/uwxRRGMIiC](https://t.co/uwxRRGMIiC)\n\nI have beef though, returned chunks are .trim()'d, so they aren't exactly what the user passes in!\n\nIt made reconstructing the viz hard üòâ\n",
        "tags": [],
        "created": "2023-12-10T14:41:18.513Z",
        "updated": "2023-12-10T14:41:18.513Z"
    },
    {
        "id": "9dmk7e3bgHPDft2DmO3y",
        "title": "Introducing AutoTranslateDoc: Translate Documentation to Over 15+ Languages!",
        "markdown": "## Introducing AutoTranslateDoc: Translate Documentation to Over 15+ Languages!\n\n[https://twitter.com/jerryjliu0/status/1732926141118472448](https://twitter.com/jerryjliu0/status/1732926141118472448)\n\n**ü™ÑSmart summary:**\nWe have open-sourced a new project that can translate documentation in any GitHub repository to over 15 languages, including Chinese, Spanish, and French. We have tested this on our own LlamaIndex.TS docs and it is now available. Thanks to Haotianzh for the translation and Seldo for helping to make it available in APAC time.\n-------\n\nIntroducing AutoTranslateDoc ‚úçÔ∏èüåê\n\nWe‚Äôre open-sourcing a new project that can translate the documentation in any @github repo to over 15+ languages: Chinese üá®üá≥, Spanish üá™üá∏, French üá´üá∑, and more. \n\nWe dogfooded this on our own LlamaIndex.TS docs, and as a result it‚Äôs now available‚Ä¶ [https://t.co/K3GPDWkFUC](https://t.co/K3GPDWkFUC)\n\n![](https://pbs.twimg.com/media/GAyXEwCaQAAEZ3c.jpg)[https://t.co/eSbqGy3KTh](https://t.co/eSbqGy3KTh)\n\nThanks to @Haotianzh for the translation üòâ\n\nAnd @seldo for helping to line this up for APAC time \n\n[https://t.co/G0D3euwrM3](https://t.co/G0D3euwrM3)\n",
        "tags": [],
        "created": "2023-12-08T05:33:05.156Z",
        "updated": "2023-12-08T05:33:05.156Z"
    },
    {
        "id": "19coWNdpRT5emysWbECD",
        "title": "Poor RAG Performance: The Impact of Naive Chunking Strategies",
        "markdown": "## Poor RAG Performance: The Impact of Naive Chunking Strategies\n\n[https://twitter.com/jerryjliu0/status/1732503009891127676](https://twitter.com/jerryjliu0/status/1732503009891127676)\n\n**ü™ÑSmart summary:**\nPoor RAG performance is caused by naive chunking strategies due to the arbitrary nature of defining chunking boundaries, which are independent of the relevant context. An example of this is when the relevant context is at the beginning or end of a chunk.\n-------\n\nNaive chunking strategies cause poor RAG performance. But why? ü§î (short üßµ)\n\nA big reason is that defining a chunking boundaries is completely arbitrary and independent of the relevant context üí°\n\nExample: if the relevant context is at the beginning/end of your chunk, then‚Ä¶ [https://t.co/0nmykafoK4](https://t.co/0nmykafoK4)\n\n![](https://pbs.twimg.com/media/GAsWIhFacAAdxGp.png)\n\n@jerryjliu0\n",
        "tags": [],
        "created": "2023-12-07T04:47:04.235Z",
        "updated": "2023-12-07T04:47:04.235Z"
    },
    {
        "id": "4fpTwqm3SvqFwXcqLhFM",
        "title": "Automatically Handle Incremental Data Updates with @llama_index",
        "markdown": "## Automatically Handle Incremental Data Updates with @llama_index\n\n[https://twitter.com/jerryjliu0/status/1732122735722303771](https://twitter.com/jerryjliu0/status/1732122735722303771)\n\n**ü™ÑSmart summary:**\nLlama Index now offers automated incremental data updates for production-scale data, eliminating the need for manual duplicate checking and upsert logic. Check out their guide for more information.\n-------\n\nA key requirement for building RAG for production-scale data is handling incremental data updates.\n\nIt's now handled automatically in @llama_index, without you needing to write boilerplate code for duplicate checking/upsert logic yourself! \n\nCheckout our guide on this topic:‚Ä¶ [https://t.co/VmutwQkxEh](https://t.co/VmutwQkxEh)\n\n![](https://pbs.twimg.com/media/GAm8cBHb0AAfDUG.jpg)[https://t.co/v8nOuSuQ5u](https://t.co/v8nOuSuQ5u)\n\n@jerryjliu0 @llama_index\n",
        "tags": [],
        "created": "2023-12-06T18:37:04.337Z",
        "updated": "2023-12-06T18:37:04.337Z"
    },
    {
        "id": "SUaO43BaTIbsXfI5YoBD",
        "title": "https://docs.phidata.com/examples/rag-llm-app",
        "markdown": "[https://docs.phidata.com/examples/rag-llm-app](https://docs.phidata.com/examples/rag-llm-app)\n",
        "tags": [],
        "created": "2023-12-04T21:37:27.317Z",
        "updated": "2023-12-04T21:37:27.317Z"
    },
    {
        "id": "qzXXJVmIVfGt0plXypOu",
        "title": "Transforming Minimal Sketches into Realistic Renders with Interior AI",
        "markdown": "## Transforming Minimal Sketches into Realistic Renders with Interior AI\n\n[https://twitter.com/levelsio/status/1731685220259078225](https://twitter.com/levelsio/status/1731685220259078225)\n\n**ü™ÑSmart summary:**\nAn interior designer/architect provided minimal sketches which were then used to generate realistic renders with the help of Interior AI v2, surprising the friend who posted the thread.\n-------\n\nMy friend got these very minimal sketches from an interior designer / architect\n\nI put them in Interior AI v2 and got these renders back\n\nDidn't think it'd work with such minimal sketches \n\n![](https://pbs.twimg.com/media/GAguLNQWwAAy1ny.jpg)\n\n@levelsio\n",
        "tags": [],
        "created": "2023-12-04T17:10:04.456Z",
        "updated": "2023-12-04T17:10:04.456Z"
    },
    {
        "id": "IMapQRP7eM2TwQWyWO5c",
        "title": "Create Your Own Text Summarization App in 20 Lines of Code",
        "markdown": "## Create Your Own Text Summarization App in 20 Lines of Code\n\n[https://twitter.com/itsafiz/status/1730912075353116841](https://twitter.com/itsafiz/status/1730912075353116841)\n\n**ü™ÑSmart summary:**\nThis thread provides instructions on how to create a text summarization app using Huggingface models and LangChain AI in less than 20 lines of code. It also provides instructions on how to install LangChain and Huggingface_hub before running the code. Finally, it encourages readers to follow the author for more Python and LLM content and to like and retweet the first tweet to share with friends.\n-------\n\nText Summarization using LLMs and LangChain üöÄ\n\nDo you know you create your own text summarization app using @huggingface models and @LangChainAI in less than 20 lines of code? \n\nComplete source code üßµüëá \n\n![](https://pbs.twimg.com/media/GAVtS9oaUAAp96y.jpg)\n\nMake sure that you have installed langchain and huggingface_hub before running the code. \n\n![](https://pbs.twimg.com/media/GAVvLzEawAAmF9G.jpg)\n\nI hope you find this post useful!\n\nif yes, please follow me @itsafiz for more Python and LLM content. \n\nLike and RT the first tweet to share with your friends. \n\n[https://t.co/Rcrx8fWP5X](https://t.co/Rcrx8fWP5X)\n",
        "tags": [],
        "created": "2023-12-04T17:05:05.871Z",
        "updated": "2023-12-04T17:05:05.872Z"
    },
    {
        "id": "ZGyDaKXkACYUyWjyCn9D",
        "title": "Introducing Llama Datasets: Benchmark Your RAG Pipelines for Different Use Cases",
        "markdown": "## Introducing Llama Datasets: Benchmark Your RAG Pipelines for Different Use Cases\n\n[https://twitter.com/llama_index/status/1731718080223707148](https://twitter.com/llama_index/status/1731718080223707148)\n\n**ü™ÑSmart summary:**\nToday, we are introducing Llama Datasets, a set of datasets contributed by the community that allow users to easily benchmark their RAG pipelines for different use cases. It is difficult to define a single right evaluation dataset for a LLM use case, so users can pick and choose the datasets that best fit their needs.\n-------\n\nToday we‚Äôre introducing Llama Datasets ü¶ôüìù - a set of community-contributed datasets that allow users to easily benchmark their RAG pipelines for different use cases.\n\nIt‚Äôs hard to define a single right eval dataset for your LLM use case. Instead, you can pick and choose the‚Ä¶ [https://t.co/iMP7yeVp9I[https://t.co/6fw3bZC6s6](https://t.co/6fw3bZC6s6)](https://t.co/iMP7yeVp9I)\n\n@llama_index\n",
        "tags": [],
        "created": "2023-12-04T16:57:04.995Z",
        "updated": "2023-12-04T16:57:04.995Z"
    },
    {
        "id": "eAS5iSTnIURuq5aDNzFR",
        "title": "Marker: A 10x Faster and More Accurate PDF to Markdown Converter",
        "markdown": "## Marker: A 10x Faster and More Accurate PDF to Markdown Converter\n\n[https://twitter.com/VikParuchuri/status/1730357379194400803](https://twitter.com/VikParuchuri/status/1730357379194400803)\n\n**ü™ÑSmart summary:**\nMarker is a pdf to markdown converter that is 10x faster than nougat, more accurate outside arXiv, and has low hallucination risk. It uses 4 models - column detector, layout detector, nougat, postprocessor - and is optimized for throughput. It is easy to parallelize and can be adjusted to speed up processing each individual PDF. Marker has limitations, including only supporting languages similar to English and being noncommercial.\n-------\n\nI'm excited to ship marker - a pdf to markdown converter that is 10x faster than nougat, more accurate outside arXiv, and has low hallucination risk. Marker is optimized for throughput, like converting LLM pretrain data.\n\nFind it here - [https://t.co/t3V2uTYfbM](https://t.co/t3V2uTYfbM) . \n\n![](https://pbs.twimg.com/media/GAN0s74bYAAsEZq.png)\n\nNougat is an amazing model, but is slow and hallucination-prone (1.5% of pages in arXiv, 5%+ outside) due to autoregressive decoding.\n\nMarker converts and cleans text incrementally. It uses 4 models - column detector, layout detector, nougat, postprocessor. It OCRs if needed.\n\nTo benchmark, I found some documents that had parallel latex and pdf versions, then converted the latex to markdown. 1/2 from arXiv, 1/2 textbooks.\n\nI compared the references to the converted versions and computed a 0-100 alignment/accuracy score.\n\nMarker is 10x faster than nougat per-page, and is more accurate outside of arXiv (think* are non-arXiv books, rest are arXiv). Both use max 3GB of VRAM in this benchmark (run on A6000). \n\n![](https://pbs.twimg.com/media/GAN0yLcaUAExN5D.png)\n\nMarker is easy to parallelize, and throughput scales mostly linearly with parallel workers. There's even a script that will run conversion across multiple GPUs.\n\nYou can also adjust batch sizes to speed up processing each individual PDF, at the cost of more VRAM usage.\n\nPDF is a tricky format - just blobs of text with positions. No info about how blobs link together.\n\nThe challenges are:\n\n- Ordering text in multi-column layouts\n- Knowing if the text you extract is corrupt\n- Handling different types of blocks (indenting code, etc)\n\nI tried many methods to find the text block order, even training a model to directly predict the order (input bounding boxes, output the order). This was based on the data from layoutreader - [https://t.co/6CMgulVH1Y](https://t.co/6CMgulVH1Y) .\n\nThis wasn't accurate, and could degenerate into repetition.\n\nI then labeled a dataset myself (somewhat noisily giving entire pdfs the same label) and trained a model to detect the number of columns. Then find vertical column breaks, and order blocks within columns.\n\nThe column detector is here - [https://t.co/7eISckecZw](https://t.co/7eISckecZw) .\n\nI use heuristics, like misspell rate, to determine if we need to use tesseract for OCR. Ocrmypdf is more accurate than tesseract alone, since it applies transforms to the page first.\n\nOCRing all text is less accurate than extracting text, since OCR introduces errors.\n\nAfter the text has been extracted and ordered, I use a layout segmenter to detect block types (text, equation, table, etc).\n\nI finetuned layoutlmv3 with the DocLayNet dataset from IBM - [https://t.co/SB21zVCcBq](https://t.co/SB21zVCcBq) .\n\nThe final model is here - [https://t.co/x6iUcyJENP](https://t.co/x6iUcyJENP) .\n\nEquations are converted to Latex with nougat. By only running nougat over equations, hallucination surface area is reduced significantly.\n\nI'd like to finetune nougat to operate on smaller regions (it's not optimized for this use case, although it does very well).\n\nHeuristics are applied to clean and join blocks. Tables, code, equations, etc, are handled differently. The hardest part is indent blocks - I need to retrain the layout detector for this.\n\nThis gets us most of the way there, but heuristics can't handle all edge cases.\n\nI trained a postprocessor model that takes in the almost-final text and finalizes it by deleting artifacts, adding spaces, etc.\n\nThe model is a token classifier, to detect if each input token needed a space in front of it, to be deleted, etc.\n\nFor training data, we need to clean the text, align the cleaned text to the original text, then align both with the tokens from an LLM.\n\nOne problem is that most tokenizers are destructive in some way - they remove whitespace, replace some characters with unk, etc.\n\nI tried bloom, which has a big vocab, and did not remove whitespace like mdeberta and others.\n\nBut bloom codes some unicode characters as unknown, which made it unsuitable. (although I really wanted it to work!)\n\nThe bloom-based postprocessor is here - [https://t.co/EEK0m7hiMk](https://t.co/EEK0m7hiMk) .\n\nI used byt5, which tokenizes unicode characters into bytes. This is less efficient, but can encode any utf-8 char.\n\nOne character can be several tokens (one token = one byte), so I recombine tokens and process each character.\n\nThe model is here - [https://t.co/JtmtQkzAmI](https://t.co/JtmtQkzAmI)\n\nThe postprocessor is an area that could use some work. Cleaning up how I align the original and cleaned sequences is something I want to work on, as well as collecting more training data.\n\nMarker has limitations, including:\n\n- It only support languages similar to English. Chinese, Hindi, etc, will not work.\n- It is noncommercial due to nougat and layoutlm licensing.\n- Not all equations will be converted to latex.\n\nMarker wouldn't be possible without some amazing open source work, including, but not limited to:\n\n- LayoutLMv3 from Microsoft\n- Nougat by Meta(@lukas_blecher)\n- DocLayNet from IBM\n- ByT5\n- OCRmyPDF + tesseract\n- PyMuPDF\n\nThank you so much for this work.\n\nI think building entire pipelines/applications is a big opportunity for open source AI. Pipelines can be much more effective than single models, but it's much more common to finetune a model than it is to chain several models together.\n\nI trained/fine-tuned all models on 4 A6000s. I was excited to see how far I could get with relatively limited GPU resources!\n\nAnyways, I hope you find this useful! If you do try marker out, please let me know how it went for you. I've tried it across a wide range of PDFs, but there are so many edge cases.\n",
        "tags": [],
        "created": "2023-12-01T05:24:05.960Z",
        "updated": "2023-12-01T05:24:05.960Z"
    },
    {
        "id": "FPwCv0bi67o5DtQWg0g9",
        "title": "Insanely Fast Whisper with Speaker Diarisation Now Available!",
        "markdown": "## Insanely Fast Whisper with Speaker Diarisation Now Available!\n\n[https://twitter.com/reach_vb/status/1729251580371689821](https://twitter.com/reach_vb/status/1729251580371689821)\n\n**ü™ÑSmart summary:**\nSpeaker Diarisation is now available with Pyannote library, which is 100% local and works on Mac or Nvidia GPUs. It provides fast transcriptions and speaker segmentations. To use it, install the project mentioned in the thread. A logo has been added to the project, which was generated from SDXL.\n-------\n\nInsanely fast whisper now with Speaker Diarisation! üî•\n\n100% local and works on your Mac or on Nvidia GPUs.\n\nAll thanks to @hbredin's Pyannote library, you can now get blazingly fast transcriptions and speaker segmentations! ‚ö°Ô∏è\n\nHere's how you can use it too:\n\npipx install‚Ä¶ [https://t.co/zoIvHYNbde[https://t.co/pAbXIGObFA](https://t.co/pAbXIGObFA)](https://t.co/zoIvHYNbde)\n\nWant to see how this works? Check out the project below:\n\n[https://t.co/fbrZW1r8DH](https://t.co/fbrZW1r8DH)\n\nOh, don't worry; I already invalidated the HF token ü§ó\n\nAdded a logo! Let's fkn gooo!üöÑüî•\n\n(generated from SDXL of course) \n\n![](https://pbs.twimg.com/media/F_-iLLiXUAA2BD-.jpg)\n",
        "tags": [],
        "created": "2023-11-28T21:12:05.660Z",
        "updated": "2023-11-28T21:12:05.660Z"
    },
    {
        "id": "CnvSoATmWeE0GKaiao8i",
        "title": "Recreating 3D Scenes Using Eye Reflections",
        "markdown": "## Recreating 3D Scenes Using Eye Reflections\n\n[https://twitter.com/ToughSf/status/1728518927133614393](https://twitter.com/ToughSf/status/1728518927133614393)\n\n**ü™ÑSmart summary:**\nA new technique has been developed that can recreate 3D scenes using the reflections in a person's eyes. A single photo is enough, but accuracy is improved with multiple reflections from multiple angles, like a video capturing moving eyes.\n-------\n\nRecreating 3D scenes using the reflections in your eyes:\n[https://t.co/6aifDeY1Rt](https://t.co/6aifDeY1Rt)\n\nA single photo is enough, but it becomes much more accurate with multiple reflections from multiple angles, like a video capturing moving eyes. [https://t.co/XwoqIkxT1z](https://t.co/XwoqIkxT1z)\n\n@ToughSf\n",
        "tags": [],
        "created": "2023-11-28T19:09:04.981Z",
        "updated": "2023-11-28T19:09:04.981Z"
    },
    {
        "id": "dJwheJPssKEnKm8XY39X",
        "title": "Unlocking Advanced Retrieval with SEVEN LlamaPacks",
        "markdown": "## Unlocking Advanced Retrieval with SEVEN LlamaPacks\n\n[https://twitter.com/llama_index/status/1729303619760259463](https://twitter.com/llama_index/status/1729303619760259463)\n\n**ü™ÑSmart summary:**\nWe have created seven advanced retrieval LlamaPacks as templates to help you build advanced RAG. It is now easier than ever to try each technique with just one line of code. The full set of techniques includes Hybrid Fusion (vector +...).\n-------\n\nWe‚Äôve created SEVEN advanced retrieval LlamaPacks as templates to help you build advanced RAG üî•\n\nIt‚Äôs never been easier; instead of following a notebook, you can now try each technique in ~1 line of code ‚ö°Ô∏è\n\nHere‚Äôs the full set of techniques below:\n‚≠êÔ∏è Hybrid Fusion (vector +‚Ä¶ [https://t.co/CaZXuf6byS](https://t.co/CaZXuf6byS)\n\n![](https://pbs.twimg.com/media/F_-4eI1boAA8BW_.png)\n\n@llama_index\n",
        "tags": [],
        "created": "2023-11-28T16:48:05.429Z",
        "updated": "2023-11-28T16:48:05.429Z"
    },
    {
        "id": "in5ur6wBueXS3BzwwA02",
        "title": "Achieving 76% Accuracy on SQL-Eval with GGUF on an M1 Mac",
        "markdown": "## Achieving 76% Accuracy on SQL-Eval with GGUF on an M1 Mac\n\n[https://twitter.com/rishdotblog/status/1728819667152855370](https://twitter.com/rishdotblog/status/1728819667152855370)\n\n**ü™ÑSmart summary:**\nA new 7B model has been developed and tested on an M1 Mac, achieving 76% accuracy on sql-eval. This is lower than GPT-4 and SQLCoder-34B-v2, but is still impressive given that it works locally on a laptop. The model is expected to be released soon as a Mac app.\n-------\n\nRunning our new 7B model 100% locally on an M1 Mac ü§ì\n\n76% accuracy on sql-eval with GGUF. For reference, GPT-4 is 82.5% and SQLCoder-34B-v2 is at 85%\n\nPretty wild that this works locally *on a laptop*! Super excited about getting this on a Mac app soon. [https://t.co/qpqsUPRTxa](https://t.co/qpqsUPRTxa)\n\n![](https://pbs.twimg.com/media/F_3-tzdacAEvPUu.jpg)\n\n@rishdotblog\n",
        "tags": [],
        "created": "2023-11-26T17:32:05.242Z",
        "updated": "2023-11-26T17:32:05.243Z"
    },
    {
        "id": "S9KQ9QQk9XzVnLgsZCaj",
        "title": "Create and Customize Your Own RAG Agent with Natural Language",
        "markdown": "## Create and Customize Your Own RAG Agent with Natural Language\n\n[https://twitter.com/llama_index/status/1726999440974844022](https://twitter.com/llama_index/status/1726999440974844022)\n\n**ü™ÑSmart summary:**\nRAGs is a Streamlit app that allows users to create and customize their own RAG agent and use it to search and retrieve data using natural language. It is inspired by OpenAI GPTs.\n-------\n\nIntroducing RAGs, a @streamlit app that allows you to create and customize your own RAG agent and then use it over your own data, all with natural language üî•\n\nDirectly inspired by @OpenAI GPTs, you can converse with an agent to help you do search/retrieval over any data you‚Ä¶ [https://t.co/A1dE3vQGs9[https://t.co/eebPQqDFM6](https://t.co/eebPQqDFM6)](https://t.co/A1dE3vQGs9)\n\n@llama_index @streamlit @OpenAI\n",
        "tags": [],
        "created": "2023-11-22T03:20:04.295Z",
        "updated": "2023-11-22T03:20:04.295Z"
    },
    {
        "id": "gsQ2Ia8FGV4NyLoKkM48",
        "title": "Generating Training Data for Natural Language Processing Tasks with a RAG Pipeline",
        "markdown": "## Generating Training Data for Natural Language Processing Tasks with a RAG Pipeline\n\n[https://twitter.com/abacaj/status/1726681852801651067](https://twitter.com/abacaj/status/1726681852801651067)\n\n**ü™ÑSmart summary:**\nThis thread suggests a method for using a 13B model to generate new training data for tasks such as Q/A, summarizing, and information extraction. It is recommended to run the model in 4bit over a RAG pipeline, and to use 8bit if compute is available. A paper is provided for reference. Finally, the reader is left to figure out a way to filter poorly generated completions.\n-------\n\nRun this model in 4bit over a RAG pipeline, use it to generate new training data for a specific task you have Q/A, summarizing, information extraction etc. Fine tune the 13B model using the generated data, profit [https://t.co/jsHbANbhCA](https://t.co/jsHbANbhCA)\n\nIf you have the compute do 8bit but 4bit will get you on much cheaper hardware\n\nPaper for reference [https://t.co/GPL1TVjZoI](https://t.co/GPL1TVjZoI)\n\n![](https://pbs.twimg.com/media/F_Zo1K8WMAEVYzc.jpg)\n\nI know I simplified it, of course you need to have a way to filter poorly generated completions - left as an exercise for the reader lol\n",
        "tags": [],
        "created": "2023-11-20T20:02:06.663Z",
        "updated": "2023-11-20T20:02:06.663Z"
    },
    {
        "id": "qcouzoAklLTI0x5zWUbn",
        "title": "A Comprehensive Guide to Building LLM/RAG Apps",
        "markdown": "## A Comprehensive Guide to Building LLM/RAG Apps\n\n[https://twitter.com/llama_index/status/1726280711517438336](https://twitter.com/llama_index/status/1726280711517438336)\n\n**ü™ÑSmart summary:**\nThis guide by @nanonets is a comprehensive resource for building LLM/RAG apps over data, covering 12+ parts including data, models, training, and more. It is concise yet comprehensive and is one of the best guides available.\n-------\n\nWe have over 250+ guides to help you build simple-to-advanced LLM/RAG apps over your data.\n\nBut if you‚Äôre looking for a unified resource, this guide by @nanonets is one of the best that we‚Äôve seen üëá\n\nIt‚Äôs concise yet comprehensive, covering 12+ parts including:\n‚úÖ Data‚Ä¶ [https://t.co/HunnWK1hIo](https://t.co/HunnWK1hIo)\n\n![](https://pbs.twimg.com/media/F_T7JBnbgAAwECJ.jpg)\n\n@llama_index @nanonets\n",
        "tags": [],
        "created": "2023-11-19T17:09:04.100Z",
        "updated": "2023-11-19T17:09:04.100Z"
    },
    {
        "id": "k4FeLugP0KH12q6n0S39",
        "title": "Introducing Codegen: Agent-Driven Software Development for Enterprise Codebases",
        "markdown": "## Introducing Codegen: Agent-Driven Software Development for Enterprise Codebases\n\n[https://twitter.com/mathemagic1an/status/1725193428060025306](https://twitter.com/mathemagic1an/status/1725193428060025306)\n\n**ü™ÑSmart summary:**\nCodegen is a new agent-driven software development platform for enterprise codebases. It has raised $16 million from Thrive Capital and other investors. It has features such as Ticket to Pull Request, which allows users to delegate complex tasks to Codegen and receive a PR/explanation commented on the ticket. It also has a multi-agent system with access to code, docs, Slack, tickets, git, CI, and more. It is merging AI-generated PRs in prod for multimillion-line codebases and is looking\n-------\n\nExcited to share what I've been building üöÄ\n\nIntroducing @codegen\n[https://t.co/3LsrrFitLz](https://t.co/3LsrrFitLz)\n\n‚ö° Agent-driven software development for enterprise codebases ‚ö°\n\nWe've raised $16mm from @ThriveCapital and others to bring this to the world - [https://t.co/LuclcoGB9K](https://t.co/LuclcoGB9K)\n\nüëá More below [https://t.co/HYtfsO2Ify](https://t.co/HYtfsO2Ify)\n\nOur mission at @codegen is \"L5 Software Engineering Automation.\"\n\nThat is - the system you can ask to \"build @netflix\" and it:\n\n‚û°Ô∏è Writes the code\n‚û°Ô∏è Provisions AWS infra\n‚û°Ô∏è Monitors logs & makes fixes\n..\n\n.. performing any task, using all the tools of modern software.\n\n2/\n\nCopilot and \"chat your code\" have already changed the way we program.\n\nThese keep the human in the driver's seat and accelerate them with AI.\n\nBut what's possible when we remove the human as a constraint?\n\n3/\n\nbrings this vision to life by putting AI \"agents\" in the driver's seat üèéÔ∏è\n\nDelegate complex tasks to @codegen and watch as it:\n\n‚ö° Eliminates tech debt\n‚ö° Rapidly builds out boilerplate\n‚ö° Refines/refactors code\n\nAll without leaving your @linear, @github and @SlackHQ\n\n4/\n\nToday, we're excited to debut \"Ticket to Pull Request\" for enterprise codebases.\n\nAdd the \"Codegen\" label to a ticket =>\n\n‚û°Ô∏è @codegen spins up an army of agents to solve the issue\n\n‚û°Ô∏è You receive a PR/explanation commented on the ticket\n\n‚û°Ô∏è Chat, give feedback, etc.\n\n5/ [https://t.co/0AhUiADPQW](https://t.co/0AhUiADPQW)\n\nis built to collaborate with you like human, incorporating feedback and navigating ambiguity through interaction.\n\nHave a question about it's changes? DM @codegen in slack.\n\nHave feedback on a PR? Leave a comment and @codegen will fix it.\n\n6/ [https://t.co/GaY4nEkGFf](https://t.co/GaY4nEkGFf)\n\nWhat happens when you rig up a multi-agent system with access to your code, your docs, your slack, your tickets, git, your CI, and more?\n\nThe tedium of building software recedes and you're left to focus on shipping product.\n\nThe way it should be.\n\n7/ [https://t.co/Lyd0dtPRm9](https://t.co/Lyd0dtPRm9)\n\nToday, we're merging AI-generated PRs in prod for multimillion-line codebases.\n\nWe are the first to do this that we know of at this scale.\n\nAnd we're sprinting towards bringing this to the broader market.\n\n8/\n\nWe're thrilled to announce our raise of $16mm led by @ThriveCapital and a group of god-tier angels:\n\n‚Ä¢¬†Naval @naval\n‚Ä¢ @adamdangelo CEO @ Quora\n‚Ä¢ @mikeyk cofounder @ Instagram\n‚Ä¢ @karimatiyeh CTO @ Ramp\n‚Ä¢ @biilmann CEO @ Netlify\n‚Ä¢ @scottcjohnston CEO @ Docker\n...\n\n9/\n\n‚Ä¢ @zeeg CTO @ Sentry\n‚Ä¢ @ceo_clickhouse CEO @ Clickhouse\n‚Ä¢ @simonlast cofounder @ Notion\n‚Ä¢ @beyang CTO @ Sourcegraph\n‚Ä¢ @joshm & @hursh cofounders @ Browser Co\n‚Ä¢ @__joshma CTO @ Airplane\n‚Ä¢ @cjc COO @ Linear\n...\n\nIt's an all-star team that can't wait to use our product.\n\n10/\n\nWe couldn't be more excited to bring this technology to the world.\n\nDo you have a massive codebase want to focus on shipping more, faster?\n\nReach out on [https://t.co/3LsrrFitLz](https://t.co/3LsrrFitLz) and we'd love to chat.\n\nLastly - this is just the beginning. We have so much to build.\n\nAre you an elite AI hacker?\n\nIs computer science your passion?\n\nDo you connect deeply with the mission of breathing life into software engineering AI?\n\nWe're hiring! Hit us up at hiring@codegen.com üôå\n",
        "tags": [],
        "created": "2023-11-16T17:21:07.689Z",
        "updated": "2023-11-16T17:21:07.689Z"
    },
    {
        "id": "fkCX7jUyZ5yx0iryj03f",
        "title": "Taking a Break to Check Out an AI Presentation Generator",
        "markdown": "## Taking a Break to Check Out an AI Presentation Generator\n\n[https://twitter.com/multikev/status/1724908185361011108](https://twitter.com/multikev/status/1724908185361011108)\n\n**ü™ÑSmart summary:**\nThe author is feeling tired and is excited about the success of a project they are working on. They have released an AI presentation generator and invite the reader to check it out.\n-------\n\nI think I need to go lie down. [https://t.co/wT6a7q6juZ[https://t.co/WxScGQTEKZ](https://t.co/WxScGQTEKZ)](https://t.co/wT6a7q6juZ)\n\nWell, this is taking off. @steveruizok is that what PMF sounds like? üöÄ\n\n-\n\nI don't have a SoundCloud, but we just released a pretty awesome AI presentation generator this week, would love if you checked it out ‚ô•Ô∏è\n\n[https://t.co/K9JVjGXt1F](https://t.co/K9JVjGXt1F)\n",
        "tags": [],
        "created": "2023-11-16T04:49:04.241Z",
        "updated": "2023-11-16T04:49:04.241Z"
    },
    {
        "id": "7meSUZI8FJ4m9a5RfRwl",
        "title": "Create a Real-Time Image Cycle of People's Creations on Ding",
        "markdown": "## Create a Real-Time Image Cycle of People's Creations on Ding\n\n[https://twitter.com/StoopMensch/status/1724645126495150575](https://twitter.com/StoopMensch/status/1724645126495150575)\n\n**ü™ÑSmart summary:**\nThis tweet suggests that people should create a device that cycles through real-time images of what people are creating on Ding, and that it would cost $3200.\n-------\n\nIt's only $3200 actually\n\n[https://t.co/0fPn6N8aZu](https://t.co/0fPn6N8aZu)\n\nyou should make one that cycles through real time images of what people are creating on ding\n",
        "tags": [],
        "created": "2023-11-16T04:45:05.159Z",
        "updated": "2023-11-16T04:45:05.159Z"
    },
    {
        "id": "aEwPSR1tBx4lxSwrX73B",
        "title": "New Supernote",
        "markdown": "\n",
        "tags": [],
        "created": "2023-11-16T04:43:33.839Z",
        "updated": "2023-11-16T04:43:33.839Z"
    },
    {
        "id": "CpAdMxaGhb5zqUfkudhF",
        "title": "Unlocking Production-Ready RAG with Transformation Caching",
        "markdown": "## Unlocking Production-Ready RAG with Transformation Caching\n\n[https://twitter.com/jerryjliu0/status/1724837344543695185](https://twitter.com/jerryjliu0/status/1724837344543695185)\n\n**ü™ÑSmart summary:**\nWe are releasing a new feature called transformation caching which helps make RAG pipelines production-ready by speeding up ingestion. Building production RAG requires parameter tuning for input data, chunk sizes, embedding, and metadata extractors.\n-------\n\nOne of the coolest features we‚Äôre releasing today is transformation caching üí´ - make your RAG pipeline production-ready by speeding up ingestion ‚ö°Ô∏è\n\nBuilding production RAG requires a lot of parameter tuning to get right: input data, chunk sizes, embedding, metadata extractors,‚Ä¶ [https://t.co/0ZadotCY69](https://t.co/0ZadotCY69)\n\n![](https://pbs.twimg.com/media/F-_aZCsbkAAhHck.png)[https://t.co/ThqNjdSmbE](https://t.co/ThqNjdSmbE)\n\n@jerryjliu0\n",
        "tags": [],
        "created": "2023-11-15T18:11:04.903Z",
        "updated": "2023-11-15T18:11:04.903Z"
    },
    {
        "id": "GbtljK9zw38rddxMVQSG",
        "title": "Introducing SQLCoder-34B: Our Most Capable Model Yet!",
        "markdown": "## Introducing SQLCoder-34B: Our Most Capable Model Yet!\n\n[https://twitter.com/defogdata/status/1724488025433145617](https://twitter.com/defogdata/status/1724488025433145617)\n\n**ü™ÑSmart summary:**\nWe are excited to announce the release of SQLCoder-34B, our most powerful model yet. Even without fine-tuning, it outperforms GPT-4 turbo for text to SQL conversion on our open-source SQLEval benchmark. With fine-tuning, we have seen SQLCoder-34B reach near perfect accuracy.\n-------\n\nWe are thrilled to release SQLCoder-34B today! The 34B model is our largest and most capable model yet.\n\nEven when not fine-tuned, it outperforms GPT-4 turbo for text to SQL conversion on our open-source SQLEval benchmark. With fine-tuning, we have seen SQLCoder-34B reach near‚Ä¶ [https://t.co/91CJZK3ASP](https://t.co/91CJZK3ASP)\n\n![](https://pbs.twimg.com/media/F-6bBKybEAAiLXu.jpg)\n\n@defogdata @huggingface\n",
        "tags": [],
        "created": "2023-11-15T05:57:05.076Z",
        "updated": "2023-11-15T05:57:05.076Z"
    },
    {
        "id": "v5ya5WJV5ydkaChXRXMj",
        "title": "Introducing Create-Llama: Generate a Full-Stack Application in Seconds!",
        "markdown": "## Introducing Create-Llama: Generate a Full-Stack Application in Seconds!\n\n[https://twitter.com/llama_index/status/1724481182245785964](https://twitter.com/llama_index/status/1724481182245785964)\n\n**ü™ÑSmart summary:**\nToday, we are excited to announce the release of create-llama, a command line tool that allows users to quickly generate a full-stack LlamaIndex application in either TypeScript or Python. Create-llama also offers the convenience of deploying to either Vercel or other cloud providers.\n-------\n\n‚ú® Big news! ‚ú® Today we're happy to release create-llama, a simple to use command line tool that generates a full-stack LlamaIndex application for you, with your choice of TypeScript or Python as backend! Get started chatting with your data in just a few seconds.\n\nAll you need‚Ä¶ [https://t.co/1XIggGo2WJ](https://t.co/1XIggGo2WJ)\n\n![](https://pbs.twimg.com/media/F-6WY9WbIAAN2Hg.jpg)\n\nA really convenient part of create-llama is that you can pick the type of backend that fits your needs:\n\n1: want a full-stack JS application you can deploy in one click to @vercel? We got you, and it's powered by LlamaIndex.TS, our TypeScript package.\n\n2: want to deploy to a more‚Ä¶ [https://t.co/AeqwqytT8W](https://t.co/AeqwqytT8W)\n",
        "tags": [],
        "created": "2023-11-14T19:15:05.986Z",
        "updated": "2023-11-14T19:15:05.986Z"
    },
    {
        "id": "V5EEXvXB8TzvHDZs937I",
        "title": "SQLCoder Beats GPT-4 for Postgres SQL Generation",
        "markdown": "## SQLCoder Beats GPT-4 for Postgres SQL Generation\n\n[https://twitter.com/rishdotblog/status/1724491602331386031](https://twitter.com/rishdotblog/status/1724491602331386031)\n\n**ü™ÑSmart summary:**\nAfter 3 months of hard work, SQLCoder has been able to outperform GPT-4 in Postgres SQL generation. It is also 2x faster than GPT-4 and GPT-4-Turbo when deployed on a single A100 80GB GPU, and as fast when deployed on 4x A10 GPUs. There is potential for further speed gains with Nvidia's TensorRT LLM.\n-------\n\nWe finally beat GPT-4 for SQL generation, after 3 months of trying! ü§ì\n\nSQLCoder now writes better Postgres SQL than GPT-4. Benchmarks aside, I'm amazed at how well it works even without fine-tuning.\n\nWITH further fine-tuned on a particular schema, it's ridiculously good. We've‚Ä¶ [https://t.co/bSrKJhpwJE](https://t.co/bSrKJhpwJE)\n\n![](https://pbs.twimg.com/media/F-6ekAabIAAccJB.jpg)\n\nEqually importantly, it is 2x faster than GPT-4 and GPT-4-Turbo when deployed on a single A100 80GB GPU. And as fast when deployed on 4x A10 GPUs (using vLLM)\n\nWe haven't had a chance to play with Nvidia's TensorRT LLM, but might get more speed gains with that.\n\nHuge props to‚Ä¶ [https://t.co/MmMjWREgdb](https://t.co/MmMjWREgdb)\n",
        "tags": [],
        "created": "2023-11-14T19:09:06.033Z",
        "updated": "2023-11-14T19:09:06.033Z"
    },
    {
        "id": "8UtEguwx5g7cNKGseRtE",
        "title": "Comprehensive Survey of Embeddings and Rerankers for RAG",
        "markdown": "## Comprehensive Survey of Embeddings and Rerankers for RAG\n\n[https://twitter.com/llama_index/status/1720465247474221178](https://twitter.com/llama_index/status/1720465247474221178)\n\n**ü™ÑSmart summary:**\nA comprehensive survey has been created to analyze the best mix of embeddings and rerankers for RAG. A Colab notebook is provided to auto-generate an evaluation dataset.\n-------\n\nWhich mix of embeddings/rerankers works best for RAG?\n\nThanks to @ravithejads, we‚Äôve created our most comprehensive survey yet, analyzing 7 SOTA embedding models and 3 rerankers üî•.\n\nBest of all, the full Colab notebook is provided so you can 1) auto-generate an eval dataset, and‚Ä¶ [https://t.co/03DtSihI7d](https://t.co/03DtSihI7d)\n\n![](https://pbs.twimg.com/media/F-BRu8ybMAAJcEq.jpg)\n\n@llama_index @ravithejads\n",
        "tags": [],
        "created": "2023-11-03T18:50:05.086Z",
        "updated": "2023-11-03T18:50:05.086Z"
    },
    {
        "id": "9XW8iXfyT6nEAXfZ6I4C",
        "title": "Unlocking the Power of LlamaIndex + Deep Memory",
        "markdown": "## Unlocking the Power of LlamaIndex + Deep Memory\n\n[https://twitter.com/llama_index/status/1720230800304869696](https://twitter.com/llama_index/status/1720230800304869696)\n\n**ü™ÑSmart summary:**\nLlamaIndex + Deep Memory is a powerful and easy to use module that can be used to improve RAG metrics by up to 15%. It works on top of any existing embeddings and can be used by following the link provided.\n-------\n\nLlamaIndex + Deep Memory (@activeloopai)\n\nThis is a super powerful, easy to use module that automatically fine-tunes your document/query embeddings during ingestion time.\n\nGet +15% improvements in RAG metrics üìà, and works on top of any existing embeddings!\n\nTo use this, you‚Ä¶ [https://t.co/5POkJy7wwE](https://t.co/5POkJy7wwE)\n\n![](https://pbs.twimg.com/media/F998qwEbAAAbDif.jpg)\n\n@llama_index @activeloopai\n",
        "tags": [],
        "created": "2023-11-03T00:35:04.094Z",
        "updated": "2023-11-03T00:35:04.094Z"
    },
    {
        "id": "uiMggtTfXR8uFW55g8hp",
        "title": "Introducing LLM Enforcer: Guarantee Compliance for Local LLMs",
        "markdown": "## Introducing LLM Enforcer: Guarantee Compliance for Local LLMs\n\n[https://twitter.com/llama_index/status/1720103157412647265](https://twitter.com/llama_index/status/1720103157412647265)\n\n**ü™ÑSmart summary:**\nWe are excited about the LLM Enforcer repository, which guarantees compliance to JSON or regex for any local LLMs like llama.cpp or Huggingface.\n-------\n\nThere‚Äôs been libraries to enforce structured outputs for OpenAI LLMs (functions, Guidance), but there hasn‚Äôt been as many for local LLMs like llama.cpp / @huggingface.\n\nThat‚Äôs why we‚Äôre so excited about LLM Enforcer repo (@noamgat) - guarantee compliance to JSON or regex for any‚Ä¶ [https://t.co/YUe4gT7Fz3](https://t.co/YUe4gT7Fz3)\n\n![](https://pbs.twimg.com/media/F98H1IibcAA-yh-.png)\n\n@llama_index @huggingface @noamgat\n",
        "tags": [],
        "created": "2023-11-02T17:45:04.344Z",
        "updated": "2023-11-02T17:45:04.344Z"
    },
    {
        "id": "lT9MqtJWiRgRJOH5tCNw",
        "title": "The Benefits of vLLM + awq for LLM Inference",
        "markdown": "## The Benefits of vLLM + awq for LLM Inference\n\n[https://twitter.com/HamelHusain/status/1719882599261413840](https://twitter.com/HamelHusain/status/1719882599261413840)\n\n**ü™ÑSmart summary:**\nvLLM + awq is an efficient and user-friendly solution for LLM inference, offering fast speeds and portability. Documentation for the awq part is limited, but more information can be found at the provided link.\n-------\n\nvLLM + awq is the best solution I've found for LLM inference. \n\n- Fastest speeds competitive with everything I've seen\n- An extremely portable backend that is compatible with many inference servers\n- Super easy to use out of the box\n\nThe awq part isn't documented that well, but‚Ä¶ [https://t.co/NsdLnLMy3S](https://t.co/NsdLnLMy3S)\n\n@HamelHusain\n",
        "tags": [],
        "created": "2023-11-02T04:25:04.932Z",
        "updated": "2023-11-02T04:25:04.932Z"
    },
    {
        "id": "cXHyzHM9IDl8kHbWN0gT",
        "title": "Announcing the Launch of LlamaIndex Chat ü¶ôüí¨",
        "markdown": "## Announcing the Launch of LlamaIndex Chat ü¶ôüí¨\n\n[https://twitter.com/jerryjliu0/status/1719022164203196824](https://twitter.com/jerryjliu0/status/1719022164203196824)\n\n**ü™ÑSmart summary:**\nToday, the launch of LlamaIndex Chat was announced. It is a production-ready, full-stack application where users can create and share LLM bots over their data, and it is open-source with an MIT license.\n-------\n\nToday I‚Äôm excited to announce the launch of LlamaIndex Chat ü¶ôüí¨\n\nIt is two things:\nüî• A production-ready, full-stack application where you can create and share LLM bots over your data üîóü§ñ\nüî• A fully open-source (MIT license) JS/TS application template that you can easily‚Ä¶ [https://t.co/aDDORhNgmU[https://t.co/Q339Q58XzW[https://t.co/MyRgIZxu9t](https://t.co/MyRgIZxu9t)](https://t.co/Q339Q58XzW)](https://t.co/aDDORhNgmU)\n\n@jerryjliu0\n",
        "tags": [],
        "created": "2023-11-02T02:54:04.895Z",
        "updated": "2023-11-02T02:54:04.895Z"
    },
    {
        "id": "dHkbsqZdoSARJQ5Ai9LM",
        "title": "AutoLLM: Unified API, Vector Databases, and Cost Calculation in One Library",
        "markdown": "## AutoLLM: Unified API, Vector Databases, and Cost Calculation in One Library\n\n[https://twitter.com/1littlecoder/status/1719382120500445440](https://twitter.com/1littlecoder/status/1719382120500445440)\n\n**ü™ÑSmart summary:**\nAutoLLM is a new Python library by SafeVideo AI that combines RAG and API into one library. It offers a unified API, 20+ vector databases, cost calculation, and 1-line FastAPI. Links to the library and more information are provided.\n-------\n\nüïµÔ∏è\u200d‚ôÇÔ∏èAutoLLM is a new üêçüêç Library by @safevideo_ai for RAG + API in one library. It builds on top of @llama_index , @LiteLLM and @FastAPI \n\n‚úÖUnified API\n‚úÖ20+ Vector Databases\n‚úÖCost Calculation \n‚úÖ1-Line FastAPI\n\n‚ö°Ô∏è [https://t.co/nbdAbY3wwK](https://t.co/nbdAbY3wwK)\n\nüì¶ - [https://t.co/41W2GOV1RN[https://t.co/ouhDA9yoWQ](https://t.co/ouhDA9yoWQ)](https://t.co/41W2GOV1RN)\n\n@1littlecoder @safevideo_ai @llama_index @LiteLLM @FastAPI\n",
        "tags": [],
        "created": "2023-11-02T02:53:05.118Z",
        "updated": "2023-11-02T02:53:05.118Z"
    },
    {
        "id": "T8CX3JPHDkNoQqysuEQF",
        "title": "https://github.com/swirlai/swirl-search",
        "markdown": "[https://github.com/swirlai/swirl-search](https://github.com/swirlai/swirl-search)\n",
        "tags": [],
        "created": "2023-11-02T02:34:31.976Z",
        "updated": "2023-11-02T02:34:31.977Z"
    },
    {
        "id": "fbvFKp5wRaFkVZGY3tHc",
        "title": "https://www.reddit.com/r/LocalLLaMA/s/DrSdBVPPk2",
        "markdown": "[https://www.reddit.com/r/LocalLLaMA/s/DrSdBVPPk2](https://www.reddit.com/r/LocalLLaMA/s/DrSdBVPPk2)\n",
        "tags": [],
        "created": "2023-11-02T01:04:58.362Z",
        "updated": "2023-11-02T01:04:58.362Z"
    },
    {
        "id": "hrBnpIwVGRSVqWhXQ2ro",
        "title": "Deploying RAG in Production with LangChainAI, qdrant_engine, ApacheAirflow, streamlit, and DVCorg",
        "markdown": "## Deploying RAG in Production with LangChainAI, qdrant_engine, ApacheAirflow, streamlit, and DVCorg\n\n[https://twitter.com/noe_achache/status/1719796631451611187](https://twitter.com/noe_achache/status/1719796631451611187)\n\n**ü™ÑSmart summary:**\nThis thread provides an overview of tools and resources for deploying Retrieval Augmented Generation (RAG) in production, including LangChainAI, qdrant_engine, ApacheAirflow, streamlit, and DVCorg. It also includes a link to a replay of a talk on the topic at GenAI London.\n-------\n\nWant to learn more on deploying RAG (Retrieval Augmented Generation) in Production with:\n- @LangChainAI for a quick start\n- @qdrant_engine\n- @ApacheAirflow\n\nAnd for iterations:\n- @streamlit\n- @DVCorg\n\nReplay of my talk at GenAI London:\n[https://t.co/1dCyjZ5jNu](https://t.co/1dCyjZ5jNu)\n\ncc @mattzcarey \n\n![](https://pbs.twimg.com/media/F93wqadWUAEujxr.jpg)\n\n@noe_achache @LangChainAI @qdrant_engine @ApacheAirflow @streamlit @DVCorg @mattzcarey\n",
        "tags": [],
        "created": "2023-11-02T00:28:06.744Z",
        "updated": "2023-11-02T00:28:06.745Z"
    },
    {
        "id": "QJi8fO7hBvguushtKQDW",
        "title": "Introducing Create-Rubric-App: The Fastest Way to Build AI Applications with React and Type Safety",
        "markdown": "## Introducing Create-Rubric-App: The Fastest Way to Build AI Applications with React and Type Safety\n\n[https://twitter.com/RubricLabs/status/1719812696575529220](https://twitter.com/RubricLabs/status/1719812696575529220)\n\n**ü™ÑSmart summary:**\nCreate Rubric App is a full-stack AI project that can be set up with one command. It is built using Next.js, LangChainAI, Prisma, and TailwindCSS. Agents can call LLMs recursively, allowing for variable-depth scraping, multi-step planning, and self-healing code. The project is open-source and feedback is welcome.\n-------\n\nIntroducing ‚Üí create-rubric-app\n\nCreate Rubric App sets up a full-stack AI project in one command.\n\nIt's the fastest way to start building applications with AI Agents in React with complete type safety.\n\nBuilt using @nextjs @LangChainAI @prisma @tailwindcss [https://t.co/A9MTkRYQnW](https://t.co/A9MTkRYQnW)\n\nTo get started, simply run:\n\nnpx create-rubric-app --ai \n\n![](https://pbs.twimg.com/media/F93gjRfWUAAhlYL.jpg)\n\nWhy Agents?\n\nAgents can call LLMs recursively. \n\nAdapting to new information and creatively solving a whole new class of problems.\n\nFor e.g, use cases like variable-depth scraping, multi-step planning, and self-healing code have become possible through agents.\n\nThe best part? This project is fully open-source.\n\nWe're excited to see what you will build with this.\n\nIf you have feedback, feel free to open a pull-request or an issue in our GitHub repo.\n\nWant to learn more? Read our blog post.\n\n[https://t.co/0MuYRsO3jH](https://t.co/0MuYRsO3jH)\n",
        "tags": [],
        "created": "2023-11-01T23:45:06.195Z",
        "updated": "2023-11-01T23:45:06.195Z"
    },
    {
        "id": "wCozeL04S2WgRkpF768D",
        "title": "Microsoft's Generative AI Education Series for Beginners",
        "markdown": "## Microsoft's Generative AI Education Series for Beginners\n\n[https://twitter.com/omarsar0/status/1719733572603412635](https://twitter.com/omarsar0/status/1719733572603412635)\n\n**ü™ÑSmart summary:**\nMicrosoft has released a series of lessons on generative AI, covering topics such as language models, prompt engineering, text generation, and chat applications.\n-------\n\nüéìGenerative AI for Beginners\n\nAnother great effort by Microsoft on AI education. This one contains a series of lessons on generative AI, including an introduction to LLMs, prompt engineering fundamentals, building text generation/chat applications, and more.‚Ä¶ [https://t.co/lb9YLuouB2](https://t.co/lb9YLuouB2)\n\n![](https://pbs.twimg.com/media/F92383KWIAACLnl.jpg)\n\n@omarsar0\n",
        "tags": [],
        "created": "2023-11-01T15:49:06.463Z",
        "updated": "2023-11-01T15:49:06.463Z"
    },
    {
        "id": "0UDPvlwtMM1mh6GW3eXc",
        "title": "Twitter thread by @GregoryDiamos",
        "markdown": "## Twitter thread by @GregoryDiamos\n\n[https://twitter.com/GregoryDiamos/status/1719364355933053045](https://twitter.com/GregoryDiamos/status/1719364355933053045)\n\nLamini now supports JSON output at full speed. We guarantee that your LLM produces a valid JSON object according to your spec. Works for any LLM we support, e.g. Mistral 7b show below.\n\nAPI Docs: [https://t.co/UHCkmO640s](https://t.co/UHCkmO640s)\n\n![](https://pbs.twimg.com/media/F9xoxGXaMAAbbR6.jpg)\n\n@GregoryDiamos\n",
        "tags": [],
        "created": "2023-10-31T21:46:02.034Z",
        "updated": "2023-10-31T21:46:02.034Z"
    },
    {
        "id": "XDfNwQrJ08HxZ2s6s1zc",
        "title": "Introducing LangChain Templates: Easily Deployable Reference Architectures",
        "markdown": "## Introducing LangChain Templates: Easily Deployable Reference Architectures\n\n[https://twitter.com/LangChainAI/status/1719377131313172556](https://twitter.com/LangChainAI/status/1719377131313172556)\n\n**ü™ÑSmart summary:**\nLangChain Templates is a new collection of reference architectures for various tasks that can be easily deployed, created, shared, downloaded, and customized.\n-------\n\nü¶úüèóÔ∏èLangChain Templates\n\nToday we're excited to announce the release of ‚ú®LangChain Templates‚ú® - a collection of easily deployable reference architectures for a wide variety of tasks\n\nThis is a new way to üñåÔ∏ècreate, ü§ùshare, ‚è¨download, and üí±customize chains and agents\n\n‚ùìHow‚Ä¶ [https://t.co/9441CUzL4a](https://t.co/9441CUzL4a)\n\n![](https://pbs.twimg.com/media/F9x0M8Pa0AAttJd.jpg)\n\n@LangChainAI\n",
        "tags": [],
        "created": "2023-10-31T17:28:04.396Z",
        "updated": "2023-10-31T17:28:04.396Z"
    },
    {
        "id": "zZLqyVKtOBMN4oTobCRC",
        "title": "https://www.reddit.com/r/LocalLLaMA/s/y5CQ5GxBgR",
        "markdown": "[https://www.reddit.com/r/LocalLLaMA/s/y5CQ5GxBgR](https://www.reddit.com/r/LocalLLaMA/s/y5CQ5GxBgR)\n",
        "tags": [],
        "created": "2023-10-30T21:08:00.524Z",
        "updated": "2023-10-30T21:08:00.525Z"
    },
    {
        "id": "uwBaFYw4w3jQGZNqu8vI",
        "title": "The Ultimate Guide to RAG in Production",
        "markdown": "## The Ultimate Guide to RAG in Production\n\n[https://twitter.com/bhutanisanyam1/status/1718295024788783476](https://twitter.com/bhutanisanyam1/status/1718295024788783476)\n\n**ü™ÑSmart summary:**\nThis thread provides a comprehensive guide to RAG in production, with discussion on embedding fine-tuning, re-ranking, and routing requests. It is written by @GokuMohandas and is easily the most complete guide available.\n-------\n\nThe definitive guide to RAG in production! üôè\n\n@GokuMohandas walks us through implementing RAG from scratch, building a scalable app\n\nIt now has updated discussion on embedding fine-tuning, re-ranking and effectively routing requests\n\nI think this is easily the most complete‚Ä¶ [https://t.co/3jQiEDrPmf](https://t.co/3jQiEDrPmf)\n\n![](https://pbs.twimg.com/media/F9icNcCXkAA_WSa.jpg)\n\n@bhutanisanyam1 @GokuMohandas\n",
        "tags": [],
        "created": "2023-10-28T18:41:04.848Z",
        "updated": "2023-10-28T18:41:04.849Z"
    },
    {
        "id": "Nlq2vHHeeyMVv3jsNO0W",
        "title": "Building Your ChatGPT with LangChainAI and AgentLabs in Python",
        "markdown": "## Building Your ChatGPT with LangChainAI and AgentLabs in Python\n\n[https://twitter.com/kevinpiac/status/1717854163239842091](https://twitter.com/kevinpiac/status/1717854163239842091)\n\n**ü™ÑSmart summary:**\nThis tutorial provides a step-by-step guide on how to build a ChatGPT with LangChainAI and AgentLabs using Python. The author encourages readers to provide feedback on the tutorial.\n-------\n\nI just wrote a step-by-step tutorial showing how to build your ChatGPT with @LangChainAI and @agentlabs_ in Python.\n\n[https://t.co/LIY4tvxE29](https://t.co/LIY4tvxE29)\n\nLet me know what you think about it :) [https://t.co/gh8FF2IvIw](https://t.co/gh8FF2IvIw)\n\n@kevinpiac @LangChainAI @agentlabs_\n",
        "tags": [],
        "created": "2023-10-27T19:29:04.378Z",
        "updated": "2023-10-27T19:29:04.378Z"
    },
    {
        "id": "JROt5yrUmKCrULM32txJ",
        "title": "Moving summarize.tech off GPT-3.5 and onto Mistral-7b-instruct: A Journey with Open-Source LLMs",
        "markdown": "## Moving summarize.tech off GPT-3.5 and onto Mistral-7b-instruct: A Journey with Open-Source LLMs\n\n[https://twitter.com/floydophone/status/1715035183228018816](https://twitter.com/floydophone/status/1715035183228018816)\n\n**ü™ÑSmart summary:**\nA side project that uses GPT-3.5 to summarize YouTube videos had grown in popularity, but the creator wanted to switch to an open-source LLM. After trying Vicuna-13b and Llama2, Mistral-7b-instruct was found to be just as fast and high quality as GPT-3.5, and was hosted by Perplexity AI. This tweetstorm is to show that open LLMs are now usable for production apps, and to thank the teams involved.\n-------\n\na few weeks ago, i moved summarize dot tech off of gpt-3.5 and onto @MistralAI's mistral-7b-instruct, an open-source LLM that runs on normie hardware. the model is hosted by @perplexity_ai. it's just as fast, just as high quality, and much cheaper to run. here are the details:\n\nfirst, some context. summarize dot tech is a side project i hacked in a few weekends last year to summarize youtube videos w/ gpt-3 (now gpt-3.5). it ended up growing in popularity and now has over 200,000 monthly active users. here it is in action: [https://t.co/DOd7RU5yJy](https://t.co/DOd7RU5yJy)\n\ngpt-3.5 is great, but i have always been excited about open-source LLMs for all of the stallmanesque reasons. however, the popular open LLMs i tried all had lots of problems. vicuna-13b, a popular choice, was noticeably worse than gpt-3.5. so much so that i could not deploy it.\n\nllama2 was high quality, but the AI alignment teams clearly went way too far. for example, it will infamously refuse to tell you how to kill a linux process because it is \"unethical\" [https://t.co/6pRGFJVWMS](https://t.co/6pRGFJVWMS)\n\nso for a while, i just dropped the idea of using a local model. eventually, i heard that a new model, mistral-7b, had come out, and was really good. so i gave it a shot. i fired up llama-cpp-python and downloaded a quantized mistral-7b-instruct model ([https://t.co/g3QVrJKx2a).](https://t.co/g3QVrJKx2a).)\n\nfrankly, i was blown away. for the first time, the quality of output was indistinguishable from gpt-3.5. additionally it was fast: not only was it usable on CPU, it appeared to be about as fast as gpt-3.5 on a normie graphics card (rtx 3080/a4000). i wanted to make the switch.\n\nhowever, there was the matter of productionizing this thing. i was in the process of building my own serving infrastructure on top of [https://t.co/tMQWpFKQsA's](https://t.co/tMQWpFKQsA's) community GPU cloud when i saw @eladgil's tweet about @perplexity_ai's new API [https://t.co/yx3vAlkx2L](https://t.co/yx3vAlkx2L)\n\ni punched in my credit card, wired up the API in like 5 minutes and it worked great, though i quickly hit their early access rate limits. thankfully, the team was super responsive and i was able to become an early design partner with elevated rate limits and ship to production.\n\none thing that gave me confidence in shipping on top of a brand-new API is that failure handling is straightforward. in the event of a timeout or failed request, i can fall back to gpt-3.5 and the user likely won't notice.\n\nanyway, the point of this tweetstorm is that open LLMs are now usable for production apps, at least in the text summarization domain. if you are interested in learning more i would suggest lurking the LocalLLaMA subreddit [https://t.co/CLhlGtCQN8](https://t.co/CLhlGtCQN8)\n\ni also want to shout out @ggerganov for llama.cpp, @AravSrinivas and @denisyarats for @perplexity_ai, and the @MistralAI team for the model itself. this stuff is cool!\n",
        "tags": [],
        "created": "2023-10-20T13:22:07.069Z",
        "updated": "2023-10-20T13:22:07.069Z"
    },
    {
        "id": "9WtUHxaKMLWeS4zJOxhN",
        "title": "Automating Customer Support with AutoDraft",
        "markdown": "## Automating Customer Support with AutoDraft\n\n[https://twitter.com/nickscamara_/status/1712169811675873564](https://twitter.com/nickscamara_/status/1712169811675873564)\n\n**ü™ÑSmart summary:**\nWe built an AI-powered tool called AutoDraft that can read customer support emails and draft replies based on your own documents. It works by connecting to your Gmail inbox and classifying emails, then using MendableAI to draft a reply. We also connected it to the Slack API so we can be alerted when MendableAI auto-drafts an email. The best part is that it is open-source and easy to build yourself.\n-------\n\nWhat if an AI could read your customer support emails and draft replies to it based on your own docs?\n\nWe quickly build one using [https://t.co/cW7nblG4O5](https://t.co/cW7nblG4O5) @LangChainAI and @Replit and the results were ü§Ø\n\nSay hello to AutoDraft üìù\n\nOpen source. Repl in the üßµ \n\n![](https://pbs.twimg.com/media/F8LXZXtWAAAktv0.jpg)\n\nIt works by connecting to your Gmail inbox and it classifies which emails are of a customer support nature. \n\nIt then uses @mendableai to draft a reply to the email. If the reply is good, it will be saved as a draft. \n\n![](https://pbs.twimg.com/media/F8LYfdEW8AA0EbV.jpg)\n\nWe also connected it to the @SlackAPI so we get alerted when @mendableai auto-drafts an email. That way we can revise and send it.\n\nThe best part is that is open-source and super easy to build it yourself. Here is the Repl and Github links:\n[https://t.co/viVU0hwLSV](https://t.co/viVU0hwLSV)\n[https://t.co/39d2MLsLqi](https://t.co/39d2MLsLqi)\n",
        "tags": [],
        "created": "2023-10-12T20:15:06.392Z",
        "updated": "2023-10-12T20:15:06.392Z"
    },
    {
        "id": "EMb917NfWaV5l0jFn3Et",
        "title": "Finetuning GPT-3.5 Turbo on Chain of Density Summarization",
        "markdown": "## Finetuning GPT-3.5 Turbo on Chain of Density Summarization\n\n[https://twitter.com/LangChainAI/status/1712115247530848492](https://twitter.com/LangChainAI/status/1712115247530848492)\n\n**ü™ÑSmart summary:**\nCharlie G finetuned GPT 3.5 Turbo on a Salesforce paper that produces state-of-the-art summarization results by producing 5 summaries iteratively.\n-------\n\nüéØ Finetuning on \"Chain of Density\"Summarization\n\nA great paper from Salesforce came out ~a month ago that produce SOTA summarization results by producing 5 summaries iteratively, each one denser and more informative than the prior\n\n@__Charlie_G finetuned GPT 3.5 Turbo on these‚Ä¶ [https://t.co/7PfsO3poA3](https://t.co/7PfsO3poA3)\n\n![](https://pbs.twimg.com/media/F8Kh3zKbAAAZYn0.jpg)\n\n@LangChainAI @__Charlie_G\n",
        "tags": [],
        "created": "2023-10-11T15:02:05.512Z",
        "updated": "2023-10-11T15:02:05.513Z"
    },
    {
        "id": "1IyisWcjHP1YRRCPplyv",
        "title": "Build AI Agents with NoCode in Minutes - Try it Now!",
        "markdown": "## Build AI Agents with NoCode in Minutes - Try it Now!\n\n[https://twitter.com/Saboo_Shubham_/status/1711594868923896196](https://twitter.com/Saboo_Shubham_/status/1711594868923896196)\n\n**ü™ÑSmart summary:**\nn8n is a visual UI for building AI agents with LangChain without writing any code. It can be used to automate tasks such as monthly syncs and massive data executions. An example of building an appointment scheduling AI agent is given, and users can try it out and learn more about building AI agents in a free virtual workshop.\n-------\n\nDrag & Drop to build AI agents ü§Ø\n\nIntroducing n8n, a visual UI for building AI agents with LangChain without writing a single line of Python code.\n\nNow build automation around LLM apps with NoCode and give superpowers to ChatGPT. [https://t.co/jfTuVdMudR](https://t.co/jfTuVdMudR)\n\nHow does it work? \n\n1. Connect & Set Up: Choose your n8n experience and set triggers to fetch data across apps.\n\n2. Integrate Using App Nodes: Utilize 400+ app nodes to manage your data.\n\n3. Automate: Let your workflow handle tasks, from monthly sync to massive data executions.\n\nLet's look at an example of building an appointment scheduling AI agent.\n\nScenario:\nIncoming email ‚Üí AI checks if the email is about an appointment ‚Üí If yes, it gets calendar availability ‚Üí AI crafts and send a response email.\n\nLet's look at how you can build this in minutes: \n\n![](https://pbs.twimg.com/media/F8DLVoTWQAAXPd9.jpg)\n\nStep 1: Check if incoming email is about appointment.\n\n1. Add the Email trigger node to detect incoming emails.\n\n2. Add the 'LangChain' node and set it up to evaluate if the email is about an appointment.\n\n3. If the email is about an appointment, move to step 2. \n\n![](https://pbs.twimg.com/media/F8DLcyFXoAEmZE8.jpg)\n\nStep 2: Get Calendar availability and craft a response\n\n1. Add a Calendar app node to check the user's availability.\n\n2. Use the OpenAI Chat model to craft an appropriate response.\n\n3. RSVP Action: Add the Send Email action node to send the response. \n\n![](https://pbs.twimg.com/media/F8DLlFqXQAA0J67.jpg)\n\nWhat are you waiting for?\n\nAutomate your workflows by building your own AI agents with NoCode. Try it out now: [https://t.co/31P9dICI0x](https://t.co/31P9dICI0x)\n\nLearn more about building AI agents in this free virtual workshop happening on Discord. Register now to book your spot: [https://t.co/yRnHjN8gxK](https://t.co/yRnHjN8gxK)\n",
        "tags": [],
        "created": "2023-10-10T16:44:07.446Z",
        "updated": "2023-10-10T16:44:07.446Z"
    },
    {
        "id": "uWjMhJQ8hscPYDPpYiuR",
        "title": "Tutorials and Code Examples for Projects",
        "markdown": "## Tutorials and Code Examples for Projects\n\n[https://twitter.com/skalskip92/status/1710064032994963890](https://twitter.com/skalskip92/status/1710064032994963890)\n\n**ü™ÑSmart summary:**\nWe have YouTube tutorials and code examples for four projects: traffic analysis, counting objects in a zone, counting objects crossing a line, and tracking players on a football field.\n-------\n\nWe have YouTube tutorials for all the projects I just showed and code examples.\n\n- Traffic Analysis: [https://t.co/WFk3oDXdEq](https://t.co/WFk3oDXdEq)\n- Counting Objects in Zone: [https://t.co/Ds8DnhsHXT](https://t.co/Ds8DnhsHXT)\n- Counting Objects Crossing the Line: [https://t.co/bdPKPM7AOy](https://t.co/bdPKPM7AOy)\n\nAnd Tracking Players on the Football Field: [https://t.co/YkbZt0zSj3](https://t.co/YkbZt0zSj3)\n",
        "tags": [],
        "created": "2023-10-06T04:11:04.381Z",
        "updated": "2023-10-06T04:11:04.381Z"
    },
    {
        "id": "1NXdVLpTGm2NgJgME3kk",
        "title": "Open-Sourcing SQLCoder2 and SQLCoder-7B: Outperforming GPT-4 and GPT-3.5",
        "markdown": "## Open-Sourcing SQLCoder2 and SQLCoder-7B: Outperforming GPT-4 and GPT-3.5\n\n[https://twitter.com/rishdotblog/status/1709574909234712963](https://twitter.com/rishdotblog/status/1709574909234712963)\n\n**ü™ÑSmart summary:**\nWe have open-sourced two new models, SQLCoder2 and SQLCoder-7B, which outperform GPT-4 and GPT-3.5 when fine-tuned on a specific database schema or out-of-training-set schemas. SQLCoder2 is a 15B parameter model based on the Starcoder model by BigCodeProject.\n-------\n\nWe just open-sourced SQLCoder2 and SQLCoder-7B! They outperform GPT-4 when fine-tuned on a specific database schema, and outperform GPT-3.5 on out-of-training-set schemas\n\nSQLCoder2 is a 15B parameter model that uses the excellent Starcoder model by @BigCodeProject as a base‚Ä¶ [https://t.co/FVRXyz19nY](https://t.co/FVRXyz19nY)\n\n![](https://pbs.twimg.com/media/F7mbHpYbkAAbsFa.png)\n\n@rishdotblog @BigCodeProject\n",
        "tags": [],
        "created": "2023-10-04T16:33:07.652Z",
        "updated": "2023-10-04T16:33:07.652Z"
    },
    {
        "id": "gUSfCR6yD3r8FUmadPYY",
        "title": "Finding a Mac-Compatible Transcription Package with Diarization Capabilities",
        "markdown": "## Finding a Mac-Compatible Transcription Package with Diarization Capabilities\n\n[https://twitter.com/simonw/status/1709268171344277992](https://twitter.com/simonw/status/1709268171344277992)\n\n**ü™ÑSmart summary:**\nThe author is looking for a transcription package that runs on Mac and can do diarization (labeling snippets SPEAKER1/SPEAKER2/etc). The author has used one package successfully but it is difficult to install and run. The author's favorite GUI tool for running transcriptions on a Mac is missing the automatic diarization component. The author also mentions an impressive Whisper transcription model that runs entirely in the browser.\n-------\n\nAnyone seen a good, easy to use transcription package that runs on Mac (not a cloud service) and can do diarization - labeling snippets SPEAKER1/SPEAKER2/etc?\n\nI've used [https://t.co/acgYeDJ5GT](https://t.co/acgYeDJ5GT) successfully but it's hard to install and run - I want to be able to recommend a GUI\n\nThis is for the classic journalism challenge of running audio from a 3hr long council meeting through a transcription service to make it searchable / summarizable / etc\n\nMy favourite GUI tool for running transcriptions on a Mac right now is [https://t.co/sjjDtWRTdD](https://t.co/sjjDtWRTdD) by @jordibruin - it's fantastic, but it's missing the automatic diarization component (at least at the moment)\n\nImpressive: the Whisper transcription model running entirely in the browser (I had to use Chrome for this as it depends on WebGPU) [https://t.co/gTx0nETURy](https://t.co/gTx0nETURy)\n",
        "tags": [],
        "created": "2023-10-04T03:38:06.724Z",
        "updated": "2023-10-04T03:38:06.724Z"
    },
    {
        "id": "dZji8czPTYzDEvEnbmdZ",
        "title": "Open-Source Full-Stack Template for Production RAG App",
        "markdown": "## Open-Source Full-Stack Template for Production RAG App\n\n[https://twitter.com/llama_index/status/1707773681605419296](https://twitter.com/llama_index/status/1707773681605419296)\n\n**ü™ÑSmart summary:**\nWe recently open-sourced a full-stack template for a production RAG app, with deep integrations with Github Codespaces and Docker. To get the backend up and running, follow the steps in the README. We encourage users to tinker around and customize components, and a full video guide is available.\n-------\n\nA few weeks ago, we open-sourced [https://t.co/d1UwhnTzD8,](https://t.co/d1UwhnTzD8,) a full-stack template for a production RAG app.\n\nThanks to @sourabhd, we added deep integrations with @github codespaces + @Docker - easily spin up production RAG for yourself in the cloud in minutes, w/ zero dep setup ‚ö°Ô∏è [https://t.co/VBzAkPl0Fn](https://t.co/VBzAkPl0Fn)\n\nTo get our backend up and running, simply follow the steps in the full backend README here: [https://t.co/YN5KwAhADZ](https://t.co/YN5KwAhADZ)\n\nWe encourage you to tinker around and see how you can customize different components like the input documents.\n\nA full video guide here: [https://t.co/ZhOln3lBkM](https://t.co/ZhOln3lBkM)\n",
        "tags": [],
        "created": "2023-09-29T17:07:03.354Z",
        "updated": "2023-09-29T17:07:03.354Z"
    },
    {
        "id": "MCJ4Qi9aOYqI9aILM3UV",
        "title": "How to View 3D Gaussian Splatting Scenes in Unity Using Polycam",
        "markdown": "## How to View 3D Gaussian Splatting Scenes in Unity Using Polycam\n\n[https://twitter.com/jonstephens85/status/1707593149856821336](https://twitter.com/jonstephens85/status/1707593149856821336)\n\n**ü™ÑSmart summary:**\nThis thread provides a tutorial on how to use a 3D Gaussian Splatting plugin in Unity, with a link to the Polycam download.\n-------\n\n@cpheinrich @aras_p Shameless plug: Here is my video tutorial on how to use his plugin. You can use the Polycam download!\n\nHow to View 3D Gaussian Splatting Scenes in Unity\n[https://t.co/ou7lfAgjHK](https://t.co/ou7lfAgjHK)\n\n@jonstephens85 @cpheinrich @aras_p\n",
        "tags": [],
        "created": "2023-09-29T14:06:04.569Z",
        "updated": "2023-09-29T14:06:04.569Z"
    },
    {
        "id": "p1TwRr7rVI8Zjg8rWDbi",
        "title": "Unlocking Company Knowledge with a Free and Open Source SlackBot",
        "markdown": "## Unlocking Company Knowledge with a Free and Open Source SlackBot\n\n[https://twitter.com/DanswerAI/status/1707072148291063882](https://twitter.com/DanswerAI/status/1707072148291063882)\n\n**ü™ÑSmart summary:**\nWe have created a free and open source SlackBot that uses natural language processing to answer questions based on company knowledge. It is connected to company tools like Confluence, GitHub, and Slack, and can provide answers in seconds.\n-------\n\nWe built a free and opensource SlackBot that uses  to answer questions based on your company knowledge.\n\nIt connects to company tools like Confluence, GitHub, Slack etc. and always stay up to date.\n\nChange your time-to-answer from hours to seconds.\n[https://t.co/hzHNzdUD9L[https://t.co/QHcWLUj8QO](https://t.co/QHcWLUj8QO)](https://t.co/hzHNzdUD9L)\n\n@DanswerAI\n",
        "tags": [
            "LLMs"
        ],
        "created": "2023-09-28T14:47:05.152Z",
        "updated": "2023-09-28T14:47:05.152Z"
    },
    {
        "id": "TyZKog3wP3BwXHx6axjV",
        "title": "Creating a Website in Less Than a Minute with GPT-4 Vision and Replit",
        "markdown": "## Creating a Website in Less Than a Minute with GPT-4 Vision and Replit\n\n[https://twitter.com/skirano/status/1706823089487491469](https://twitter.com/skirano/status/1706823089487491469)\n\n**ü™ÑSmart summary:**\nWith GPT-4 vision and Replit, it is now possible to create a live website from an image in less than a minute. This new technology promises to make website creation easier and faster than ever before.\n-------\n\nFrom image to live website using GPT-4 vision and @Replit in less than a minute. \n\nThings are about to get so interesting. üî• [https://t.co/Mtbqjbgd5Q](https://t.co/Mtbqjbgd5Q)\n\nüîó Live website:\n[https://t.co/Hd1MyGUp1b](https://t.co/Hd1MyGUp1b)\n",
        "tags": [],
        "created": "2023-09-27T03:45:04.124Z",
        "updated": "2023-09-27T03:45:04.124Z"
    },
    {
        "id": "Pyoh7aR5PWQEviir3uVS",
        "title": "SQLCoder2 Model Matches and Slightly Beats GPT-4 for Complex SQL Generation",
        "markdown": "## SQLCoder2 Model Matches and Slightly Beats GPT-4 for Complex SQL Generation\n\n[https://twitter.com/rishdotblog/status/1706789605838864655](https://twitter.com/rishdotblog/status/1706789605838864655)\n\n**ü™ÑSmart summary:**\nA new 15B parameter SQLCoder2 model has been developed and has been shown to match and slightly beat GPT-4 for complex SQL generation on out-of-training-set schemas. The model is set to be released next week and is an improvement on the previous SQLCoder model which beat GPT-3.5 but lagged behind GPT-4.\n-------\n\nWe just got our 15B parameter SQLCoder2 model to match (and *slightly* beat) GPT-4 for complex SQL generation on out-of-training-set schemas. Releasing the weights (hopefully) next week!\n\nOur previous model ‚Äì SQLCoder ‚Äì beat GPT-3.5 but lagged behind GPT-4. The new model‚Ä¶ [https://t.co/GWz8KabBPf](https://t.co/GWz8KabBPf)\n\n![](https://pbs.twimg.com/media/F6-6JUga0AA8D05.jpg)\n\nUgh the image above got cut off ‚Äì here is the image with the labels \n\n![](https://pbs.twimg.com/media/F6-9CzubMAA2HLL.jpg)\n",
        "tags": [],
        "created": "2023-09-27T03:44:03.876Z",
        "updated": "2023-09-27T03:44:03.876Z"
    },
    {
        "id": "8jmCjeTByY3GhjrGmYFE",
        "title": "Autonomously Generating LLM Agents for Any Goal",
        "markdown": "## Autonomously Generating LLM Agents for Any Goal\n\n[https://twitter.com/bhutanisanyam1/status/1706648060531556580](https://twitter.com/bhutanisanyam1/status/1706648060531556580)\n\n**ü™ÑSmart summary:**\nAutoAgents is a tool that can autonomously generate agents for any goal, eliminating the need for strong prompting and role definition. The code is available online and is readable.\n-------\n\nAutoAgents: Autonomously generate LLM agents for any goal! ü§ñ \n\nThis tries to solve the need for strong prompting and role definition by autogenerating agents\n\nThe code is sparsely documented but readable:\n\n[https://t.co/fe0IM7jzhr[https://t.co/qUmaZ4e0kB](https://t.co/qUmaZ4e0kB)](https://t.co/fe0IM7jzhr)\n\n@bhutanisanyam1\n",
        "tags": [],
        "created": "2023-09-26T23:32:03.718Z",
        "updated": "2023-09-26T23:32:03.718Z"
    },
    {
        "id": "dTnwS7kZE8WXOznHNMUA",
        "title": "A Comprehensive List of Gaussian Splatting Viewers",
        "markdown": "## A Comprehensive List of Gaussian Splatting Viewers\n\n[https://twitter.com/dylan_ebert_/status/1704530483080532462](https://twitter.com/dylan_ebert_/status/1704530483080532462)\n\n**ü™ÑSmart summary:**\nA list of Gaussian splatting viewers has been created, with an accompanying image.\n-------\n\nin case you're having trouble keeping track of all the gaussian splatting viewers, I made a list [https://t.co/FVGkzndzdY](https://t.co/FVGkzndzdY)\n\n![](https://pbs.twimg.com/media/F6e1VnlWwAAMw47.png)\n\n@dylan_ebert_\n",
        "tags": [],
        "created": "2023-09-25T22:15:04.300Z",
        "updated": "2023-09-25T22:15:04.300Z"
    },
    {
        "id": "2gjQKqGucRE1oc3EpHio",
        "title": "Developing a Voice-Activated System with Real-Time Transcription",
        "markdown": "## Developing a Voice-Activated System with Real-Time Transcription\n\n[https://twitter.com/evil_malloc/status/1706401488111632553](https://twitter.com/evil_malloc/status/1706401488111632553)\n\n**ü™ÑSmart summary:**\nA project has been in development for the past few weeks that includes wake word detection, real-time transcription, and a GPT response. The project is still missing a final loop and automatic transcription stop.\n-------\n\n@aidan_mclau I've been working on something similar since @Teknium1's post about 2-3 weeks ago. It has wake word detection and real-time transcription, as well as the gpt response. The final loop and the automatic transcription stop after one stops speaking are missing\n[https://t.co/hURRVYZGsL](https://t.co/hURRVYZGsL)\n\n@evil_malloc @aidan_mclau @Teknium1\n",
        "tags": [],
        "created": "2023-09-25T21:50:04.810Z",
        "updated": "2023-09-25T21:50:04.810Z"
    },
    {
        "id": "LOePrpHhV7LZlNTKiCFb",
        "title": "GitWit Agent: Open Source Superpowers for Code Editing",
        "markdown": "## GitWit Agent: Open Source Superpowers for Code Editing\n\n[https://twitter.com/jamesmurdza/status/1705277042001051694](https://twitter.com/jamesmurdza/status/1705277042001051694)\n\n**ü™ÑSmart summary:**\nGitWit Agent is an open source tool that has been used to generate over 7,000 commits on GitHub. It is different from similar agents because it spawns secure Linux containers to run commands, and is composable with a dozen LLM integrations. It is currently used for building projects such as backend APIs, web scrapers, and Chrome extensions, and can be used for automated testing, deploying cloud infrastructure, and batch data processing.\n-------\n\nGitWit Agent is Open Source! ü§ù\n\nIt took three months to build and has been used by hundreds of users to generate over 7,000 commits on GitHub! üì•\n\nIt's also available as a command-line tool.\n\nSource code: [https://t.co/3e60p0Hr0f](https://t.co/3e60p0Hr0f)\n\n![](https://pbs.twimg.com/media/F6paJsSb0AAIzsC.jpg)\n\n1/ GitWit is different from similar agents because it spawns secure Linux containers to run commands. üê≥\n\nThat gives it superpowers for code editing such as package management (npm, pip), templating (npm create) and stream editing (sed, awk). ü¶æ \n\n![](https://pbs.twimg.com/media/F6pRploaAAA-As3.jpg)\n\n2/ Through @LangChainAI, it's composable with a dozen LLM integrations including @OpenAI, Code Llama and @AnthropicAI.\n\n3/ It's currently used for building projects such as:\n\nBackend APIs: [https://t.co/yyqCNJu0oz](https://t.co/yyqCNJu0oz)\nWeb scrapers: [https://t.co/VFX22SR1CC](https://t.co/VFX22SR1CC)\nChrome extensions: [https://t.co/SdCGAW8Gc7](https://t.co/SdCGAW8Gc7)\n\n4/ Further applications for container-based agents are endless:\n\nüß™ Automated testing\n‚òÅÔ∏è Deploying cloud infrastructure\nüóÇÔ∏è Batch data processing\n\nWant to collaborate on agents? Reach out here or to contact@gitwit.dev.\n\n   [https://t.co/id9jiDA5HG](https://t.co/id9jiDA5HG)\n",
        "tags": [
            "opensource",
            "codegeneration",
            "devtools"
        ],
        "created": "2023-09-22T21:44:06.364Z",
        "updated": "2023-09-22T21:44:06.364Z"
    },
    {
        "id": "PROGI00ePQE3I5zOwthd",
        "title": "ChatGPT: An In-Browser Version Built with Transformers.js!",
        "markdown": "## ChatGPT: An In-Browser Version Built with Transformers.js!\n\n[https://twitter.com/xenovacom/status/1704910846986682581](https://twitter.com/xenovacom/status/1704910846986682581)\n\n**ü™ÑSmart summary:**\nAn in-browser version of ChatGPT has been built with Transformers.js, meaning no need for a server. Check out the links for more information.\n-------\n\nWOW! ü§Ø An in-browser version of ChatGPT (or HF Chat), built with ü§ó Transformers.js!\n\nYes that's right, everything runs 100% locally in your browser, meaning no need for a server! Check it out!\n\nüîó [https://t.co/DaTCQegghx[https://t.co/ve7ctfk8Wj[https://t.co/MnRntQGsPC](https://t.co/MnRntQGsPC)](https://t.co/ve7ctfk8Wj)](https://t.co/DaTCQegghx)\n\n@xenovacom\n",
        "tags": [],
        "created": "2023-09-21T18:55:21.929Z",
        "updated": "2023-09-21T18:55:21.929Z"
    },
    {
        "id": "taGuDSliJvJlrKcCLE32",
        "title": "Testing Retrieval Quality in High-Performing RAG Systems",
        "markdown": "## Testing Retrieval Quality in High-Performing RAG Systems\n\n[https://twitter.com/jerryjliu0/status/1704884854536712491](https://twitter.com/jerryjliu0/status/1704884854536712491)\n\n**ü™ÑSmart summary:**\nTo build high-performing RAG systems, it is important to test both retrieval and generation quality. This can now be easily done in LLAMA Index, with the help of testing retrieval in isolation.\n-------\n\nTo build high-performing RAG systems, retrieval quality matters as much if not more so than LLM quality üîé.\n\nI‚Äôve told users for a while now to try ‚Äútesting retrieval in isolation‚Äù üß™. Now you can easily do so in @llama_index, in conjunction with testing generation quality! üî•‚Ä¶ [https://t.co/JKXQdZodq1](https://t.co/JKXQdZodq1)\n\n![](https://pbs.twimg.com/media/F6j3j7sboAARfGF.jpg)\n\n@jerryjliu0 @llama_index\n",
        "tags": [],
        "created": "2023-09-21T16:18:04.363Z",
        "updated": "2023-09-21T16:18:04.363Z"
    },
    {
        "id": "g9Ec5aki2DRYheop0bVX",
        "title": "Supercharge Your AI Skills: Building RAG from Scratch Tutorial Series",
        "markdown": "## Supercharge Your AI Skills: Building RAG from Scratch Tutorial Series\n\n[https://twitter.com/llama_index/status/1704514611901591605](https://twitter.com/llama_index/status/1704514611901591605)\n\n**ü™ÑSmart summary:**\nWe have created a tutorial series on building RAG from scratch to help people learn how AI works. The tutorials cover topics such as ingestion, vector db, retrieval, LLM synthesis, routing, and evals. Links to the existing tutorials are provided.\n-------\n\nOver the past week we‚Äôve built an entire tutorial series on ‚Äúbuilding RAG from scratch‚Äù - supercharge your AI skills and learn how things work under the hood! üöÄ\n\nIngestion, vector db, retrieval, LLM synthesis, routing, evals. More coming soon üî•\n\n[https://t.co/Ys3MhHvhHD](https://t.co/Ys3MhHvhHD)\n\n![](https://pbs.twimg.com/media/F6em5xAWcAAZpab.png)\n\nHere are the links to our existing tutorials!\n\nIngestion: [https://t.co/gTX1QCW86j](https://t.co/gTX1QCW86j)\nRetrieval: [https://t.co/NuGI7OJnGe](https://t.co/NuGI7OJnGe)\nBuilding a vector store: [https://t.co/qvaFjv7wC3](https://t.co/qvaFjv7wC3)\n\n(more below)\n\nResponse Synthesis: [https://t.co/fq8xvZ35Wo](https://t.co/fq8xvZ35Wo)\nBuilding a Router: [https://t.co/OXxJ1QG2h3](https://t.co/OXxJ1QG2h3)\nBuilding Evaluation: [https://t.co/5QeN4hoaFr](https://t.co/5QeN4hoaFr)\n",
        "tags": [],
        "created": "2023-09-20T21:52:04.966Z",
        "updated": "2023-09-20T21:52:04.966Z"
    },
    {
        "id": "VAMQ5ERNlostWJILC2RL",
        "title": "High Resolution Illusion Diffusion Space Now Available!",
        "markdown": "## High Resolution Illusion Diffusion Space Now Available!\n\n[https://twitter.com/multimodalart/status/1704237716936757683](https://twitter.com/multimodalart/status/1704237716936757683)\n\n**ü™ÑSmart summary:**\nA PR (pull request) was merged to add high resolution to the Illusion Diffusion Space, making it faster and doubling the resolution with crisp details. A link to the project is provided.\n-------\n\nThanks @angrypenguinPNG for merging my PR to add high resolution to the Illusion Diffusion Space üì∫üåÄ \n\nIt's now as fast, double the resolution and has crispy details - go play ‚ñ∂Ô∏è \n\n[https://t.co/FDoEOGfKnF[https://t.co/HtCYi0PLPC](https://t.co/HtCYi0PLPC)](https://t.co/FDoEOGfKnF)\n\n@multimodalart @angrypenguinPNG\n",
        "tags": [],
        "created": "2023-09-20T15:34:05.388Z",
        "updated": "2023-09-20T15:34:05.388Z"
    },
    {
        "id": "Ilqxi24Ft5WkpSHtaboR",
        "title": "Introducing Midjourney for React: AI-Powered React Components with Tailwind CSS",
        "markdown": "## Introducing Midjourney for React: AI-Powered React Components with Tailwind CSS\n\n[https://twitter.com/jaredpalmer/status/1702356555218506070](https://twitter.com/jaredpalmer/status/1702356555218506070)\n\n**ü™ÑSmart summary:**\nI have been working with my team at Vercel to create an AI tool called Midjourney for React. With version 0, users can generate copy-and-paste friendly React code with the help of Shadcn UI and Tailwind CSS.\n-------\n\nOver the past few months, I've been building [https://t.co/9PmgLfLhtL](https://t.co/9PmgLfLhtL) with my team @Vercel. \n\nIt's an AI tool that is effectively Midjourney for React. \n\nWith v0 you can use simple prompts to generate copy-and-paste friendly React , powered by @shadcn UI and Tailwind CSS.‚Ä¶ [https://t.co/8Mdgk3dN00](https://t.co/8Mdgk3dN00)\n\n![](https://pbs.twimg.com/media/F5_7UL2bsAAiqAu.jpg)\n\n@jaredpalmer @vercel @shadcn\n",
        "tags": [],
        "created": "2023-09-14T18:00:04.900Z",
        "updated": "2023-09-14T18:00:04.900Z"
    },
    {
        "id": "m2doZ6OTi3nzbQFiaiO8",
        "title": "Building RAG-Based LLM Applications: Bridging the Gap Between OSS and Closed-Source LLMs",
        "markdown": "## Building RAG-Based LLM Applications: Bridging the Gap Between OSS and Closed-Source LLMs\n\n[https://twitter.com/GokuMohandas/status/1701960178965557284](https://twitter.com/GokuMohandas/status/1701960178965557284)\n\n**ü™ÑSmart summary:**\nThis thread introduces a production guide for building retrieval augmented generation (RAG) based LLM applications, which bridges the gap between open source and closed-source LLMs. It covers topics such as developing a RAG-based LLM application from scratch and scaling major workloads.\n-------\n\nExcited to share our production guide for building RAG-based LLM applications where we bridge the gap between OSS and closed-source LLMs.\n\n- üíª Develop a retrieval augmented generation (RAG) based LLM application from scratch.\n- üöÄ Scale the major workloads (load, chunk, embed,‚Ä¶ [https://t.co/9WawbWQwhv](https://t.co/9WawbWQwhv)\n\n![](https://pbs.twimg.com/media/F56TuXlXEAAEyTO.jpg)\n\n@GokuMohandas\n",
        "tags": [],
        "created": "2023-09-13T16:42:03.885Z",
        "updated": "2023-09-13T16:42:03.885Z"
    },
    {
        "id": "L93ilTEaHnpZUMmJi7VF",
        "title": "Introducing the GitLab Agent Toolkit",
        "markdown": "## Introducing the GitLab Agent Toolkit\n\n[https://twitter.com/LangChainAI/status/1701619546891985369](https://twitter.com/LangChainAI/status/1701619546891985369)\n\n**ü™ÑSmart summary:**\nGitLab has released a new Agent Toolkit that allows users to create agents that can open their own PRs in GitLab repositories. This was made possible by the work of @_laplaceon. Documentation can be found at the provided link.\n-------\n\nü¶ä @gitlab Agent Toolkit\n\nCreate agents that can open their own PRs in you GitLab repos, like this \"Google Principle Engineer\" agent, using the new GitLab agent toolkit ü§ì\n\nAll thanks to the awesome work of @_laplaceon!\n\nDocs: [https://t.co/8z31DA8YeX](https://t.co/8z31DA8YeX)\n\n![](https://pbs.twimg.com/media/F51dEwUbUAAKn99.png)[https://t.co/H9NsfhWEsE](https://t.co/H9NsfhWEsE)\n\n@LangChainAI @gitlab @_laplaceon\n",
        "tags": [],
        "created": "2023-09-12T22:28:04.727Z",
        "updated": "2023-09-12T22:28:04.727Z"
    },
    {
        "id": "yzFtKCTgSHfJKJEcBH8B",
        "title": "A Comprehensive Guide to Vector Databases",
        "markdown": "## A Comprehensive Guide to Vector Databases\n\n[https://twitter.com/bhutanisanyam1/status/1701218322027556914](https://twitter.com/bhutanisanyam1/status/1701218322027556914)\n\n**ü™ÑSmart summary:**\nThis thread promotes a series of articles written by @tech_optimist about vector databases, which provide insight into the nuances and options for building large language model applications.\n-------\n\nThe most comprehensive series I‚Äôve read on Vector databases! üíæ\n\nMost of us got exposed to vector dbs via Langchain or llamaindex documentation\n\nHowever, There‚Äôs a lot of nuance and options to select from when building Large Language Model apps\n\n@tech_optimist has written a 4‚Ä¶ [https://t.co/fzi6aezN4X](https://t.co/fzi6aezN4X)\n\n![](https://pbs.twimg.com/media/F5vxCnFXcAIlOo9.jpg)\n\n@bhutanisanyam1 @tech_optimist\n",
        "tags": [],
        "created": "2023-09-11T21:18:05.908Z",
        "updated": "2023-09-11T21:18:05.908Z"
    },
    {
        "id": "wzVwLQ5C43W46Dwes8Da",
        "title": "Decoupled Video Segmentation: Tracking Anything with Ease",
        "markdown": "## Decoupled Video Segmentation: Tracking Anything with Ease\n\n[https://twitter.com/_akhaliq/status/1700030823926280448](https://twitter.com/_akhaliq/status/1700030823926280448)\n\n**ü™ÑSmart summary:**\nThis paper presents a new approach to video segmentation that does not require expensive annotation of training data. It proposes a decoupled approach that uses a pre-trained model to track objects in videos, allowing for the extension of end-to-end algorithms to new video segmentation tasks.\n-------\n\nTracking Anything with Decoupled Video Segmentation\n\npaper page: [https://t.co/xqfwTkf78V](https://t.co/xqfwTkf78V)\n\nTraining data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To‚Ä¶ [https://t.co/MAkucIKSoQ[https://t.co/6FSShhURHV](https://t.co/6FSShhURHV)](https://t.co/MAkucIKSoQ)\n\n@_akhaliq\n",
        "tags": [],
        "created": "2023-09-09T15:54:04.568Z",
        "updated": "2023-09-09T15:54:04.568Z"
    },
    {
        "id": "jaLr3fcUajgOggNoP8M6",
        "title": "Learn How to Build a Full-Stack Production RAG App with Our Tutorial!",
        "markdown": "## Learn How to Build a Full-Stack Production RAG App with Our Tutorial!\n\n[https://twitter.com/jerryjliu0/status/1699814326427582754](https://twitter.com/jerryjliu0/status/1699814326427582754)\n\n**ü™ÑSmart summary:**\nLearn how to build a full-stack production RAG app with a tutorial by @thesourabhd. The tutorial includes architecture and setup, as well as the ability to feed in custom documents. The tutorial was open-sourced two days ago and has been gaining traction. Check out the repo and app for more information.\n-------\n\nWant to learn how to build a full-stack production RAG app?\n\nCheck out our brand-new tutorial by @thesourabhd üßë\u200düè´ on [https://t.co/VY9we1zhip](https://t.co/VY9we1zhip) - not just a finance app, it's a full-stack LLM app template üìà:\n‚úÖ Architecture + Setup\n‚úÖ Feed in custom docs\n\n[https://t.co/zECy1mWZFC](https://t.co/zECy1mWZFC)\n\n![](https://pbs.twimg.com/media/F5b0FH1XoAAT32h.jpg)\n\nWe open-sourced [https://t.co/VY9we1zhip](https://t.co/VY9we1zhip) two days ago and it caught fire üî•\n\nCheck it out if you haven't already!\n\nRepo: [https://t.co/3MUA01KLXs](https://t.co/3MUA01KLXs)\nApp: [https://t.co/VY9we1zhip](https://t.co/VY9we1zhip)\n\n[https://t.co/h3xhhUHuKo](https://t.co/h3xhhUHuKo)\n",
        "tags": [],
        "created": "2023-09-07T17:48:05.157Z",
        "updated": "2023-09-07T17:48:05.157Z"
    },
    {
        "id": "xhjlbIUaFtTa4AZvwzaz",
        "title": "A Comprehensive Guide to Building Production RAG with Sec-Insights",
        "markdown": "## A Comprehensive Guide to Building Production RAG with Sec-Insights\n\n[https://twitter.com/llama_index/status/1699814907028386009](https://twitter.com/llama_index/status/1699814907028386009)\n\n**ü™ÑSmart summary:**\nWe have created a video guide to help you build a production RAG using an API service with Render, FastAPI, Vercel, Sentry, and Llama Index. Learn how to feed in your own documents with the provided links.\n-------\n\nWe now have a full video guide on sec-insights, to help you build production RAG! üß†\n\nOur architecture: a full API service using @render, @FastAPI, @vercel, @getsentry, @llama_index\n\nDon‚Äôt just use it as a finance app, learn how to feed in your own docs:\n\n[https://t.co/ZhOln3l3ve[https://t.co/hXZJYItWwt](https://t.co/hXZJYItWwt)](https://t.co/ZhOln3l3ve)\n\n@llama_index @render @FastAPI @vercel @getsentry\n",
        "tags": [],
        "created": "2023-09-07T17:27:06.143Z",
        "updated": "2023-09-07T17:27:06.143Z"
    },
    {
        "id": "XyDubWEQZRaepFYdhHLK",
        "title": "Inference of Meta's Segment Anything Model on the CPU by YavorGI",
        "markdown": "## Inference of Meta's Segment Anything Model on the CPU by YavorGI\n\n[https://twitter.com/ggerganov/status/1699092329607450880](https://twitter.com/ggerganov/status/1699092329607450880)\n\n**ü™ÑSmart summary:**\nYavorGI has created a project to infer Meta's Segment Anything Model on the CPU. The project is still in its early stages, but YavorGI has already done a great job with the implementation. People are encouraged to check it out and help bring state-of-the-art image segmentation to their devices.\n-------\n\nsam.cpp üëÄ\n\nInference of Meta's Segment Anything Model on the CPU\n\nProject by @YavorGI - powered by [https://t.co/jFknDoasSy[https://t.co/u7vLhdsUzE](https://t.co/u7vLhdsUzE)](https://t.co/jFknDoasSy)\n\nrepo: [https://t.co/7nmxIH8v7C](https://t.co/7nmxIH8v7C)\n\nThe project is still in it's early stages, but @YavorGI has already done a great job with the implementation.\n\nPlease check it out and let's help him bring SOTA image segmentation to our devices!\n\nIs this what the Terminator used ü§î [https://t.co/eC3FTitV1H](https://t.co/eC3FTitV1H)\n",
        "tags": [],
        "created": "2023-09-07T01:09:04.001Z",
        "updated": "2023-09-07T01:09:04.001Z"
    },
    {
        "id": "KDLQwjw5bQthHDWSqzyB",
        "title": "Introducing Open Interpreter: A Local Code Interpreter with a ChatGPT-like Interface",
        "markdown": "## Introducing Open Interpreter: A Local Code Interpreter with a ChatGPT-like Interface\n\n[https://twitter.com/hellokillian/status/1699156860073640038](https://twitter.com/hellokillian/status/1699156860073640038)\n\n**ü™ÑSmart summary:**\nToday, an open-source Code Interpreter called Open Interpreter was launched. It allows users to summarize PDFs, visualize datasets, and control their browser from a ChatGPT-like interface in their terminal. It is currently the #1 trending repository on GitHub.\n-------\n\nToday I‚Äôm launching Open Interpreter, an open-source Code Interpreter that runs locally.\n\nSummarize PDFs, visualize datasets, and control your browser ‚Äî all from a ChatGPT-like interface in your terminal.\n\n‚óè [https://t.co/UuqbbqUhPk](https://t.co/UuqbbqUhPk)\n$ pip install open-interpreter\n$ interpreter [https://t.co/tWZzv73Vkz[https://t.co/2daKWUH48v](https://t.co/2daKWUH48v)](https://t.co/tWZzv73Vkz)\n\nupdate: open interpreter is the #1 trending repo on [https://t.co/coi4Vj0lxU](https://t.co/coi4Vj0lxU)\n",
        "tags": [],
        "created": "2023-09-06T02:16:04.509Z",
        "updated": "2023-09-06T02:16:04.509Z"
    },
    {
        "id": "IiglpryeJw8AluGUrRNV",
        "title": "Open-Source RAG App: Streamlined Production-Ready LLM App with Intuitive UI",
        "markdown": "## Open-Source RAG App: Streamlined Production-Ready LLM App with Intuitive UI\n\n[https://twitter.com/llama_index/status/1699116440056651976](https://twitter.com/llama_index/status/1699116440056651976)\n\n**ü™ÑSmart summary:**\nWe are excited to open-source a full-stack, production-ready RAG app that can save you weeks or months of hard work. It supports streaming, reasoning steps, citations, and an intuitive UI. It has features such as chat-based document Q&A, citation of source data, PDF viewer with highlighting of citations, token-level streaming of LLM responses, streaming of reasoning steps, and sharing conversations. You can play around with the full app.\n-------\n\nWe‚Äôre excited to open-source [https://t.co/d1UwhnTzD8](https://t.co/d1UwhnTzD8) - a full-stack, production-ready RAG app! ü¶ôüè¶\n\nSupports streaming, reasoning steps, citations, intuitive UI\n\nThis can save you weeks/months of hard work in trying to build a prod LLM app from scratchüî•\n\n[https://t.co/JH4AtWFyog[https://t.co/kvn758xxV7](https://t.co/kvn758xxV7)](https://t.co/JH4AtWFyog)\n\nPrototyping a RAG app takes less than an hour.\n\nBut building a full-stack RAG application with reliable feature support + all the bells/whistles can take months.\n\nThis repo can serve as reference code, or you can directly fork it to kickstart your project.\n\nHere‚Äôs a set of key features that the app supports:\n1. Chat-based Document Q&A against a pool of documents\n2. Citation of source data that LLM response was based on\n3. PDF Viewer with highlighting of citations\n\n4. Token-level streaming of LLM responses via¬†Server-Sent Events\n5. Streaming of Reasoning Steps (Sub-Questions) within Chat\n6. Sharing conversations!\n\nIf you just want to play around with the full app, you can!\n\n[https://t.co/d1UwhnTzD8](https://t.co/d1UwhnTzD8)\n\nWe stealth-launched this on Product Hunt two weeks ago and hit the top-5 for the day\n\n[https://t.co/ih7TPuHAwb](https://t.co/ih7TPuHAwb)\n",
        "tags": [],
        "created": "2023-09-05T20:26:06.086Z",
        "updated": "2023-09-05T20:26:06.086Z"
    },
    {
        "id": "20FnccJyLQZZdM3c8r3z",
        "title": "Launching the Alpha Version of AimenGPT: A Self-Hosted, Offline Chatbot with Document Uploads",
        "markdown": "## Launching the Alpha Version of AimenGPT: A Self-Hosted, Offline Chatbot with Document Uploads\n\n[https://twitter.com/aimengpt/status/1697359357162160450](https://twitter.com/aimengpt/status/1697359357162160450)\n\n**ü™ÑSmart summary:**\nAimenGPT is an open source, self-hosted, offline chatbot that allows document uploads and is 100% private with no data leaving the user's device. The goal is to become a powerful AI assistant. The alpha version has been launched and the link to the repository is provided.\n-------\n\nExcited to launch the alpha version of AimenGPT üôå\n\nAn open source, self-hosted, offline, ChatGPT-like chatbot that allows document uploads - powered by Llama2, @trychroma and @LangChainAI. 100% private, with no data leaving your device.\n\nThe goal for AimenGPT is to become a‚Ä¶ [https://t.co/A6L7slHFvf[https://t.co/M6k1dDRjLK](https://t.co/M6k1dDRjLK)](https://t.co/A6L7slHFvf)\n\nLink to the repo:\n\n[https://t.co/ChnHJaP57o](https://t.co/ChnHJaP57o)\n",
        "tags": [],
        "created": "2023-09-02T20:09:04.190Z",
        "updated": "2023-09-02T20:09:04.190Z"
    },
    {
        "id": "SbSQqmgTfnuF7NJjyaNB",
        "title": "Improving RAG System Performance: A Guide",
        "markdown": "## Improving RAG System Performance: A Guide\n\n[https://twitter.com/llama_index/status/1697408565794001237](https://twitter.com/llama_index/status/1697408565794001237)\n\n**ü™ÑSmart summary:**\nThis thread provides a guide for techniques to improve the performance of RAG systems, and suggests validating each technique against naive retrieval with evaluation modules.\n-------\n\nWe have a full guide for techniques that you can use to improve the performance of your RAG systems: [https://t.co/NTebUCg57Y](https://t.co/NTebUCg57Y)\n\nTry each technique, then validate it against naive retrieval with our evaluation modules: [https://t.co/AAmyL8c4ac[https://t.co/oJNxBwUdL0](https://t.co/oJNxBwUdL0)](https://t.co/AAmyL8c4ac)\n\n@llama_index\n",
        "tags": [],
        "created": "2023-09-01T04:17:03.855Z",
        "updated": "2023-09-01T04:17:03.855Z"
    },
    {
        "id": "ErhWyPZD6pO6a2dOZ3jQ",
        "title": "Improve RAG Performance with Our Comprehensive Guide",
        "markdown": "## Improve RAG Performance with Our Comprehensive Guide\n\n[https://twitter.com/llama_index/status/1696969454138306949](https://twitter.com/llama_index/status/1696969454138306949)\n\n**ü™ÑSmart summary:**\nWe have created a comprehensive guide to help improve RAG performance. It contains a variety of techniques and should be top of mind for building production LLM applications. For more details, check out our thread and the aggregated tweets we have put out in the past few weeks.\n-------\n\nWe have a brand-new guide containing a comprehensive set of techniques for improving RAG performance üî•\n\nWe‚Äôve worked heavily on these features the past few weeks, and want to make sure you‚Äôre aware of it!\n\nShould be top of mind for building prod LLM apps: [https://t.co/NTebUCg57Y](https://t.co/NTebUCg57Y)\n\nFor more details check out our thread here:\n\n[https://t.co/n1PrqZ0EAb](https://t.co/n1PrqZ0EAb)\n\nThere were also aggregated from a big stack of tweets that we‚Äôve put out the past few weeks:\n\n[https://t.co/SiKUQmKIdO](https://t.co/SiKUQmKIdO)\n[https://t.co/LfJExyBZBU](https://t.co/LfJExyBZBU)\n\n[https://t.co/UVMkTM3ATC](https://t.co/UVMkTM3ATC)\n\n[https://t.co/qT5M1LYv6o](https://t.co/qT5M1LYv6o)\n\n[https://t.co/xfZkrf78qM](https://t.co/xfZkrf78qM)\n",
        "tags": [],
        "created": "2023-08-30T23:59:04.508Z",
        "updated": "2023-08-30T23:59:04.508Z"
    },
    {
        "id": "JJ6B7eCXtgs1AsGSYlvA",
        "title": "Generating Contextualized Explanations for Codebases with @llama_index",
        "markdown": "## Generating Contextualized Explanations for Codebases with @llama_index\n\n[https://twitter.com/jerryjliu0/status/1696914828512882761](https://twitter.com/jerryjliu0/status/1696914828512882761)\n\n**ü™ÑSmart summary:**\nRavithejads has created an app that generates contextualized explanations for all modules in any codebase and stitches them together into a narrated video. It makes use of novel concepts like stacking multiple list indices for context accumulation. The full post and repository can be found in the links provided.\n-------\n\nOne of the most creative @llama_index use cases I‚Äôve seen - @ravithejads‚Äôs app generates contextualized explanations for ALL modules in any codebase üßë\u200düè´, and stitches it together into a narrated video üé¨!\n\nHere‚Äôs my deepfake explaining our own codebase üëá: [https://t.co/vVS7lgEdLn[https://t.co/URGnIOKAej](https://t.co/URGnIOKAej)](https://t.co/vVS7lgEdLn)\n\nFull post explaining the architecture is linked above.\n\nIt makes use of some novel concepts like stacking multiple list indices on top of each other for context accumulation.\n\nAlso check out the repo here: [https://t.co/c0D0mUaCmM](https://t.co/c0D0mUaCmM)\n\nYouTube: [https://t.co/ftcqXDU6Nt](https://t.co/ftcqXDU6Nt)\n",
        "tags": [],
        "created": "2023-08-30T23:00:06.606Z",
        "updated": "2023-08-30T23:00:06.606Z"
    },
    {
        "id": "IusML1V8YZ0fCwHra70E",
        "title": "4 Techniques to Improve Your RAG Pipeline Performance",
        "markdown": "## 4 Techniques to Improve Your RAG Pipeline Performance\n\n[https://twitter.com/jerryjliu0/status/1696969195291029853](https://twitter.com/jerryjliu0/status/1696969195291029853)\n\n**ü™ÑSmart summary:**\nThis thread provides four core techniques to improve the performance of a RAG pipeline, with full guides on how to implement them in @llama_index. The techniques include decoupling chunks used for retrieval and chunks used for synthesis, embedding document summaries, or sentences, retrieving documents from a large corpus, and using a multi-task learning approach.\n-------\n\nWe‚Äôve curated a list of 4 core techniques to improve the performance of your RAG pipeline, with full guides on how to implement in @llama_index (short üßµ)\n\n1Ô∏è‚É£ Decoupling chunks used for retrieval vs. chunks used for synthesis: Embed document summaries, or sentences, retrieve‚Ä¶ [https://t.co/usXbAio0Zv](https://t.co/usXbAio0Zv)\n\n![](https://pbs.twimg.com/media/F4zYaumawAAHBGS.jpg)\n\n@jerryjliu0 @llama_index\n",
        "tags": [],
        "created": "2023-08-30T22:51:06.379Z",
        "updated": "2023-08-30T22:51:06.380Z"
    },
    {
        "id": "t9Xyw4iHPb6oRlLEdO1k",
        "title": "Remind me to watch this documentary:",
        "markdown": "Remind me to watch this documentary:\n[https://www.youtube.com/watch?v=HqtSOoySgKc](https://www.youtube.com/watch?v=HqtSOoySgKc)\n",
        "tags": [],
        "created": "2023-08-29T23:16:03.187Z",
        "updated": "2023-08-29T23:16:12.946Z"
    },
    {
        "id": "mYXcnMXPBGOQSn2BrNzL",
        "title": "Gaussian Splatting Tutorial",
        "markdown": "## Gaussian Splatting Tutorial\nHave you seen all of the amazing 3D Gaussian Splatting posts and want to try it out yourself! Well, I have a guide for you! This absolute beginners guide will walk you step by step to make your own scene.\n[https://twitter.com/jonstephens85/status/1695977302537150890](https://twitter.com/jonstephens85/status/1695977302537150890)\n\n\n[https://twitter.com/jonstephens85/status/1695985147823505835](https://twitter.com/jonstephens85/status/1695985147823505835)\n\nI hope it‚Äôs not too long. I could have made it short, but then I would have a bunch of questions to answer.\n\nJust realized I forgot the chapters. I added them to make it easy to jump to the main parts!\n",
        "tags": [],
        "created": "2023-08-29T20:43:02.602Z",
        "updated": "2023-08-29T21:35:56.906Z"
    },
    {
        "id": "FOrlFAF3VQ1uFB9iDaO9",
        "title": "A Comprehensive Anti-Hype LLM Reading List",
        "markdown": "## A Comprehensive Anti-Hype LLM Reading List\n\n[https://twitter.com/hardmaru/status/1696628795288232094](https://twitter.com/hardmaru/status/1696628795288232094)\n\n**ü™ÑSmart summary:**\nThis thread provides a list of recommended readings for those interested in anti-hype legal studies. A link to the list is provided, as well as an accompanying image.\n-------\n\nAnti-hype LLM reading list. Pretty good list.\n\n[https://t.co/7DAnoWLWBC](https://t.co/7DAnoWLWBC)\n\n![](https://pbs.twimg.com/media/F4rrfGfbMAAt5iv.jpg)\n\n@hardmaru\n",
        "tags": [],
        "created": "2023-08-29T21:19:03.532Z",
        "updated": "2023-08-29T21:19:03.532Z"
    },
    {
        "id": "iddSvRIzHw9jraj6pkbW",
        "title": "Fixing ChatGPT Output with Custom Instructions",
        "markdown": "## Fixing ChatGPT Output with Custom Instructions\n\n[https://twitter.com/nisten/status/1696229059183730833](https://twitter.com/nisten/status/1696229059183730833)\n\n**ü™ÑSmart summary:**\nGeoffrey Hinton, a technical manager, provides instructions on how to fix chatGPT output. He suggests responding with a tree of thought, staying in the same persona, and continuing to work non-stop. He also suggests using a code-interpreter to iterate through problems and replying with full code first before continuing to work.\n-------\n\nMy custom instructions to fix chatGPT output:\n----\nI'm your technical manager Geoffrey Hinton who likes kanban boards and always requires you submit complete output, complete code that just works when I copy paste it to use in my own work.\n----\nRespond with tree of thought‚Ä¶ [https://t.co/AVAStlweR5](https://t.co/AVAStlweR5)\n\n![](https://pbs.twimg.com/media/F4o2oA7XgAAx9LQ.jpg)\n\nThen all you do is:\nStay in the same persona and continue working non-stop.\nor just: \ncontinue working nonstop\nWorks best in code-interpreter as it will iterate through problems much better. \nIf it gets too verbose say:\n\nReply with full code first then continue working nonstop. [https://t.co/smG8JLcZIo](https://t.co/smG8JLcZIo)\n",
        "tags": [],
        "created": "2023-08-29T00:53:05.324Z",
        "updated": "2023-08-29T00:53:05.324Z"
    },
    {
        "id": "LrK2kjfvVysf8djFEqyK",
        "title": "Tutorial on How to Train Your Own 3D Gaussian Splatting Models",
        "markdown": "## Tutorial on How to Train Your Own 3D Gaussian Splatting Models\n\n[https://twitter.com/alexcarliera/status/1695903934374711781](https://twitter.com/alexcarliera/status/1695903934374711781)\n\n**ü™ÑSmart summary:**\nThis thread provides a tutorial on how to train 3D Gaussian Splatting models. It outlines the steps to record a scene, camera registration, and training the model. It also provides visual examples and a link to the tutorial.\n-------\n\nI have written a tutorial on how to train your own \"3D Gaussian Splatting\" models!  \n\nWrite me here if you're facing any issues.\n\n‚¨áÔ∏è‚¨áÔ∏è [https://t.co/0UE4A2UDkr](https://t.co/0UE4A2UDkr)\n\nFirst step: record your scene. Keep in mind to:\n- Move slowly to avoid blurry images\n- Try to aim for 200-1000 images\n- Lock the exposure \n\n![](https://pbs.twimg.com/media/F4kPerfWQAEph8H.jpg)\n\nStep 2: camera registration. This is how you obtain the camera poses from your images. \n\n![](https://pbs.twimg.com/media/F4kPfiGWMAEwRnw.jpg)\n\nStep 3 & 4: train the model! After training, you can visualize it with the SIBR tool. \n\n![](https://pbs.twimg.com/media/F4kPgd5WgAAwjSh.jpg)\n\nüìù And here's the tutorial: [https://t.co/YMCXed7ShX](https://t.co/YMCXed7ShX)\n",
        "tags": [
            "GaussianSplatting"
        ],
        "created": "2023-08-28T20:44:05.892Z",
        "updated": "2023-08-28T20:44:05.892Z"
    },
    {
        "id": "rMliVMzdjSJsEsgwhG0s",
        "title": "Making OCR Engines Recognize Flowcharts in Computer Programming Textbooks and Software Documentation",
        "markdown": "## Making OCR Engines Recognize Flowcharts in Computer Programming Textbooks and Software Documentation\n\n[https://twitter.com/DynamicWebPaige/status/1695820207879549114](https://twitter.com/DynamicWebPaige/status/1695820207879549114)\n\n**ü™ÑSmart summary:**\nThis paper discusses the development of a system that can recognize and process flowcharts in computer programming textbooks and software documentation, which are often tagged as graphics and ignored by modern OCR engines.\n-------\n\nüìä \"Computer programming textbooks and software documentation often contain flowcharts to illustrate the flow of an algorithm or procedure.\n\nModern OCR engines often tag these flowcharts as graphics and ignore them in further processing. In this paper, we work towards making‚Ä¶ [https://t.co/oU1KqDu51W](https://t.co/oU1KqDu51W)\n\n![](https://pbs.twimg.com/media/F4jDfY9WkAA2Iql.jpg)\n\n@DynamicWebPaige\n",
        "tags": [],
        "created": "2023-08-27T21:24:04.099Z",
        "updated": "2023-08-27T21:24:04.099Z"
    },
    {
        "id": "CjXkfqp3g53f6xd3woTg",
        "title": "Fine-Tune a Llama2 Model in Free Google Colab with AutoTrain Advanced",
        "markdown": "## Fine-Tune a Llama2 Model in Free Google Colab with AutoTrain Advanced\n\n[https://twitter.com/abhi1thakur/status/1693988336069910808](https://twitter.com/abhi1thakur/status/1693988336069910808)\n\n**ü™ÑSmart summary:**\nWith AutoTrain Advanced, anyone can now use Google Colab to fine-tune LLMs by uploading data and tuning parameters. All you need to do is create a folder called data and put your train.csv in it, adjust the parameters, and the model will start training. In case of issues or feature requests, check out the github repository.\n-------\n\nBut I want to fine-tune a Llama2 model in free version of Google Colab\nSay no more! With AutoTrain Advanced, you can üò± Now anyone can use colab to finetune LLMs just by uploading data and tuning parameters! üöÄ 1/N [https://t.co/ZeGXVfRqH0](https://t.co/ZeGXVfRqH0)\n\n![](https://pbs.twimg.com/media/F4JA5FbbsAAlybc.jpg)\n\nHere's the colab notebook: [https://t.co/h2Dj1plnKl](https://t.co/h2Dj1plnKl)\nAll you need to do is create a folder called data and put your train.csv in it! Make sure it has text column, adjust params, model, etc, and boom üí• your model will start training! 2/N\n\nIn case of issues/feature requests, check out this github repository: 3/N [https://t.co/EgQoumGe54](https://t.co/EgQoumGe54)\n",
        "tags": [],
        "created": "2023-08-22T17:42:06.136Z",
        "updated": "2023-08-22T17:42:06.136Z"
    },
    {
        "id": "ueRTBkJMyKujCHZ3gKTH",
        "title": "Updates to my PDF-Chat-AI-SDK Now Available!",
        "markdown": "## Updates to my PDF-Chat-AI-SDK Now Available!\n\n[https://twitter.com/rajeshdavidbabu/status/1693621388228071690](https://twitter.com/rajeshdavidbabu/status/1693621388228071690)\n\n**ü™ÑSmart summary:**\nI have made updates to my pdf-chat-ai-sdk, which now allows users to read the sources used to generate their answers. I will be releasing a full tutorial on my YouTube channel this week.\n-------\n\nMade some updates to my ‚ú®pdf-chat-ai-sdk‚ú® \n\nNow you can also read the sources that were used to generate your answer ! ‚ú®üöÄ Thanks to AI SDK from @vercel \n\nCheckout the Github [https://t.co/xPUpEh79Jv](https://t.co/xPUpEh79Jv)\n\nReleasing a full-tutorial on my YT channel this week [https://t.co/Ap63QjgZzo‚Ä¶[https://t.co/F1HV9SIhx4[https://t.co/LhCA2d6U6q](https://t.co/LhCA2d6U6q)](https://t.co/F1HV9SIhx4)](https://t.co/Ap63QjgZzo‚Ä¶)\n\n@rajeshdavidbabu @vercel\n",
        "tags": [],
        "created": "2023-08-22T14:23:06.035Z",
        "updated": "2023-08-22T14:23:06.035Z"
    },
    {
        "id": "O4GrDv8yWWRoMma7QkjD",
        "title": "Introducing the Easiest LLM Fine Tuning UI!",
        "markdown": "## Introducing the Easiest LLM Fine Tuning UI!\n\n[https://twitter.com/abhi1thakur/status/1693619860050153958](https://twitter.com/abhi1thakur/status/1693619860050153958)\n\n**ü™ÑSmart summary:**\nThe new Hugging Face Hub Fine Tuning UI allows anyone to easily fine-tune almost any LLM available on the Hub by simply uploading a CSV and choosing the parameters. The training spaces will be visible in the user's account and will shut down on completion, with the models saved as private repositories. The cost of training is billed per minute, with the cost of backends varying. Issues and feature requests can be addressed on the GitHub repository.\n-------\n\nThe easiest LLM Fine Tuning UI just Landed! üöÄ\nNow, ANYONE can fine-tune (almost) any LLM available on Hugging Face Hub by just uploading a CSV and choosing the parameters and by a single click of a button! üí• Here's how you can do it: 1/N \n\n![](https://pbs.twimg.com/media/F4DwCx4XYAAWNFH.jpg)\n\nFirst, you need a huggingface account! If you dont have one, create one: [https://t.co/HlHI7TXWep.](https://t.co/HlHI7TXWep.)\nOnce your account is setup, click this link: [https://t.co/AhBKwjD9Ny](https://t.co/AhBKwjD9Ny)\nYou can choose any name for the space 2/N \n\n![](https://pbs.twimg.com/media/F4DwnW7WQAA2Mjm.jpg)\n\nAfter that, enter your huggingface write token. you can find/create your write token here: [https://t.co/3ySKWViMVq](https://t.co/3ySKWViMVq) and enter task as \"LLM\" (without quotes). Make sure to keep your space private! 3/N \n\n![](https://pbs.twimg.com/media/F4Dw2FIXYAAU1IC.jpg)\n\nSpace will be created in a few seconds. And you will get the screen below. Now, all you need to do is upload your dataset in proper format, choose the column mapping, choose hyperparameters, the model and the backend. You can add as many jobs as you like! 4/N \n\n![](https://pbs.twimg.com/media/F4DxJp7WgAIsI6-.jpg)\n\nYour training spaces will be visible in your account and you can monitor live logs. The spaces will shut themselves down on completion. So, you pay only for the training time! üí• Models will be saved to your account as private repositories! 5/N \n\n![](https://pbs.twimg.com/media/F4Dx46hXUAAiV3Y.jpg)\n\nHow much does it cost? You will be billed per minute for the training. Here's how much the backends cost right now. 6/N \n\n![](https://pbs.twimg.com/media/F4DxjWnX0AAOewv.jpg)\n\nIf you face any issues or have feature requests, check out the github repository ‚≠êÔ∏è 7/N\n[https://t.co/EgQoumFGfw](https://t.co/EgQoumFGfw)\n",
        "tags": [],
        "created": "2023-08-21T16:42:05.874Z",
        "updated": "2023-08-21T16:42:05.874Z"
    },
    {
        "id": "hFnEe3qQMgKnHJUKsTK6",
        "title": "Open-Sourcing SQL Coder: Outperforming OpenAI's GPT-3.5 and GPT-4",
        "markdown": "## Open-Sourcing SQL Coder: Outperforming OpenAI's GPT-3.5 and GPT-4\n\n[https://twitter.com/rishdotblog/status/1693614124864049617](https://twitter.com/rishdotblog/status/1693614124864049617)\n\n**ü™ÑSmart summary:**\nWe have open-sourced SQL Coder, a 15B parameter text-to-SQL model that outperforms OpenAI's gpt-3.5 and gpt-4 when fine-tuned on an individual schema. It is small enough to run on a single A100 40GB in 16 bit floats. We used a slightly novel training approach to train the model on \"easy\" questions first, and then on more difficult questions.\n-------\n\nWe just open-sourced SQL Coder, a 15B param text-to-SQL model that outperforms OpenAI's gpt-3.5! When fine-tuned on an individual schema, it outperforms gpt-4. [https://t.co/w9a2I2HNJM](https://t.co/w9a2I2HNJM)\n\nThe model is small enough to run on a single A100 40GB in 16 bit floats, or on a single‚Ä¶ [https://t.co/VP2Hrw63vY](https://t.co/VP2Hrw63vY)\n\n![](https://pbs.twimg.com/media/F4Dre9Ra8AA4m04.jpg)\n\nImportant things I forgot to mention in the original tweet!\n\n- SQLCoder is fine-tuned on StarCoder, an awesome initiative of the @BigCodeProject \n\n- We used a slightly novel training approach. We first trained the model on \"easy\" questions, and then trained the result of that on‚Ä¶ [https://t.co/MSr6QRTBZy](https://t.co/MSr6QRTBZy)\n",
        "tags": [],
        "created": "2023-08-21T16:35:06.514Z",
        "updated": "2023-08-21T16:35:06.514Z"
    },
    {
        "id": "aD8QEb72F03Huk1mNBUo",
        "title": "The Benefits of Fine Tuning Language Models",
        "markdown": "## The Benefits of Fine Tuning Language Models\n\n[https://twitter.com/rachel_l_woods/status/1692577254914638340](https://twitter.com/rachel_l_woods/status/1692577254914638340)\n\n**ü™ÑSmart summary:**\nFine tuning of language models can be used to teach them specific tasks or behaviors, but it is not necessary for prompting. It is important to note that fine tuning is not used to teach language models new concepts.\n-------\n\nThere's a resurgence of interest in fine tuning LLMs\n\nI've yet to see a successful public use case where fine tuning > prompting.\n\nBut here's where I see fine tuning *mattering*:\n\nFirst, fine tuning is for teaching an LLM specific tasks or behaviors\n\nNot teaching an LLM new‚Ä¶ [https://t.co/osypfifH3S](https://t.co/osypfifH3S)\n\n@rachel_l_woods\n",
        "tags": [],
        "created": "2023-08-19T21:30:03.873Z",
        "updated": "2023-08-19T21:30:03.874Z"
    },
    {
        "id": "zuifIC1G0rdB358IlLF2",
        "title": "Open-Sourcing Our Inference Server Under Apache 2.0",
        "markdown": "## Open-Sourcing Our Inference Server Under Apache 2.0\n\n[https://twitter.com/skalskip92/status/1692258273838215423](https://twitter.com/skalskip92/status/1692258273838215423)\n\n**ü™ÑSmart summary:**\nRoboflow has open-sourced an inference server under Apache 2.0, which allows for object detection over HTTP. Tutorial and repository links are provided.\n-------\n\n- Object detection over HTTP? \n- Easy! \n\nWe just open-sourced our inference server under Apache 2.0\n\nLeft terminal: @roboflow inference\nRight terminal: video client [https://t.co/9aDdoU8qsP](https://t.co/9aDdoU8qsP)\n\ntutorial: [https://t.co/IkQHxJnTte](https://t.co/IkQHxJnTte)\n\nrepository: [https://t.co/0h36AZhONc](https://t.co/0h36AZhONc)\n",
        "tags": [],
        "created": "2023-08-18T18:11:05.495Z",
        "updated": "2023-08-18T18:11:05.495Z"
    },
    {
        "id": "CSSiLkJPt79EILQPaGKS",
        "title": "Improving QA System Performance with a Simple Trick",
        "markdown": "## Improving QA System Performance with a Simple Trick\n\n[https://twitter.com/jerryjliu0/status/1690154205946974208](https://twitter.com/jerryjliu0/status/1690154205946974208)\n\n**ü™ÑSmart summary:**\nA new technique for improving the performance of a QA system has been developed, which uses smaller chunks for embedding and an expanded window for the LLM. This technique is now straightforward to implement in the llama_index, and provides immediate benefits compared to the naive RAG with equal chunk sizes for retrieval and synthesis. Huge shoutout to Yi Ding and Logan Markewich for the idea and implementation.\n-------\n\nA naive assumption for building RAG is that the context ‚Äúchunk‚Äù used for embedding vs. LLM is the same.\n\nInstead üí°: use smaller chunks for embedding and an ‚Äúexpanded window‚Äù for the LLM.\n\nSuper simple trick to improve performance of your QA system. üëá\n\n[https://t.co/2u7HefzreY](https://t.co/2u7HefzreY)\n\n![](https://pbs.twimg.com/media/F3SiSt6aYAIU68Z.jpg)\n\nIt‚Äôs now very straightforward to implement in @llama_index.\n\nFirst, we have a `SentenceWindowNodeParser` to parse documents into single sentence nodes.\n\nBut each node contains a `window` field containing surrounding text in its metadata field. \n\n![](https://pbs.twimg.com/media/F3SiTCJbEAAknJi.jpg)\n\nDuring query time, we retrieve sentences based on embedding similarity.\n\nThis allows for finer-grained retrieval of relevant context. \n\nWe then use the MetadataReplacementPostProcessor to expand the text w/ context around each node - allows LLM to see a larger text window. \n\n![](https://pbs.twimg.com/media/F3SiTQIbwAAJQiW.jpg)\n\nThis provides immediate benefits compared to naive RAG with equal chunk sizes for retrieval and synthesis. \n\nIn naive RAG, the chunk sizes are big, and as a result not all retrieved results are relevant (contain ‚ÄúAMOC‚Äù) \n\nThis creates a ‚Äúlost in the middle‚Äù problem. \n\n![](https://pbs.twimg.com/media/F3SiTfubcAAoaQI.jpg)\n\nHuge shoutout to @yi_ding for the idea inspiration and @LoganMarkewich for the implementation.\n\nGive it a try yourself and let us know what you think! Part of our brand new 0.8.0 release today.\n\nNotebook üìó: [https://t.co/AP2F67HWoP](https://t.co/AP2F67HWoP)\n",
        "tags": [],
        "created": "2023-08-12T19:55:06.144Z",
        "updated": "2023-08-12T19:55:06.144Z"
    },
    {
        "id": "AcU22m4bi3eSXA4a5zRS",
        "title": "Introducing LLM: A CLI Tool and Python Library for Interacting with Large Language Models",
        "markdown": "## Introducing LLM: A CLI Tool and Python Library for Interacting with Large Language Models\n\n[https://twitter.com/simonw/status/1690409724083339265](https://twitter.com/simonw/status/1690409724083339265)\n\n**ü™ÑSmart summary:**\nLLM, a CLI tool and Python library, has released a new version with the ability to assign aliases to models, making it easier to manage complex model names.\n-------\n\nNew release of LLM, my CLI tool and Python library for interacting with Large Language Models\n\nThe main new feature is the ability to assign new aliases to models, useful for dealing with things like \"mlc-chat-Llama-2-7b-chat-hf-q4f16_1\"\n[https://t.co/S4dsVuSQgv](https://t.co/S4dsVuSQgv)\n\n![](https://pbs.twimg.com/media/F3WKqAQaUAAYPKC.jpg)\n\n@simonw\n",
        "tags": [],
        "created": "2023-08-12T17:59:03.754Z",
        "updated": "2023-08-12T17:59:03.754Z"
    },
    {
        "id": "TfaugvuvuUIwWDY57ZDT",
        "title": "Using ChatGPT to Quickly Build a Tool for Combining Release Notes and Answering Questions",
        "markdown": "## Using ChatGPT to Quickly Build a Tool for Combining Release Notes and Answering Questions\n\n[https://twitter.com/simonw/status/1690373651231834115](https://twitter.com/simonw/status/1690373651231834115)\n\n**ü™ÑSmart summary:**\nThis thread demonstrates a workflow for quickly building a tool in Bash and jq to combine release notes from a GitHub repo, and then piping those release notes to LLM to answer questions. It also shows how to use LLM to get installation instructions for software, and how to trigger the ethics filter.\n-------\n\nThis TIL shows my workflow using ChatGPT - it includes prompts and process I used to quickly build a tool in Bash and jq for combining release notes from a GitHub repo, and shows how I then pipe those release notes to \"llm\" to answer questions about them [https://t.co/utpocQg0ob](https://t.co/utpocQg0ob)\n\nOnce you have a CLI tool for pulling release notes for a project, you can pipe them into LLM to use them to answer questions:\n\n./combined-release-notes.sh simonw/llm | llm -s 'how do I install it'\n\n[https://t.co/WJEacynjsT](https://t.co/WJEacynjsT)\n\n![](https://pbs.twimg.com/media/F3VqWf_bgAAI9jJ.jpg)\n\nOh this is excellent... I tried running that request for installation instructions through a local Llama 2 7B model and triggered the ethics filter! \n\n![](https://pbs.twimg.com/media/F3VssSxbwAAR71x.jpg)\n\n... my mistake, that was a bug: It didn't run against the release notes, just against the single prompt \"how do I install it\" - so this is my bug, not Llama 2 refusing to give me installation instructions for my software\n\n(Still funny though)\n\nI managed to get usable installation instructions from Llama 2 7B using this instead:\n\nllm \"$(/tmp/combined-release-notes.sh simonw/llm) how do I install it\" \\\n -m mlc-chat-Llama-2-7b-chat-hf-q4f16_1\n\nOutput here: [https://t.co/7bnU5iyJVp](https://t.co/7bnU5iyJVp)\n",
        "tags": [],
        "created": "2023-08-12T16:04:05.169Z",
        "updated": "2023-08-12T16:04:05.170Z"
    },
    {
        "id": "1oXYOkmV09J6qaQ4C79R",
        "title": "Building Agents with chatGPT Custom Instructions",
        "markdown": "## Building Agents with chatGPT Custom Instructions\n\n[https://twitter.com/NickADobos/status/1682138883222544384](https://twitter.com/NickADobos/status/1682138883222544384)\n\n**ü™ÑSmart summary:**\nThis thread explains how to use custom instructions in chatGPT to build full agents. It provides instructions on how to recreate @babyAGI_ inside chatGPT using a 1079 character prompt, as well as how to use hotkeys to go fast. It also suggests ideas to make the process more reliable, and suggests adapting the process to work with a database file that supports semantic search instead of a .txt file. Finally, it provides links to conversations for anyone curious.\n-------\n\nchatGPT custom instructions is INSANE. You can build full agents!!\n\nHow to recreate @babyAGI_ inside chatGPT using a 1079 char prompt\n\nWe got:\n-Saving tasks to a .txt file\n-Reading tasks in new sessions\n-Since we don't have an infinite loop, instead Hotkeys to GO FAST\n\nHere's the‚Ä¶ [https://t.co/g1O4Agqmhm[https://t.co/5oMfxNO6UE](https://t.co/5oMfxNO6UE)](https://t.co/g1O4Agqmhm)\n\nInstall steps\n-enable new custom instructions in settings\n-open settings again, and paste this in the second box \n-use code interpreter \n-Open new chat and go\n-download and upload tasks file as needed, don‚Äôt forget, it disappears in chat history \n\n![](https://pbs.twimg.com/media/F1goZ_yakAAsy_K.jpg)\n\nSeems a little inconsistent in saving, so you can just ask whenever you need it\n\nAnyone got ideas to make it more reliable?\n\nIf someone wants to tackle adapting this to work with a database file that supports semantic search, instead of a .txt file\n\nthat would be amazing :)\n\nWhat other hotkeys would be fun?\n\nlinks to the conversations for anyone curious\n\n[https://t.co/sKY5YXbzpX](https://t.co/sKY5YXbzpX)\n\n[https://t.co/q72EXeAfOa](https://t.co/q72EXeAfOa)\n\n[https://t.co/0zhwvlXbJe](https://t.co/0zhwvlXbJe)\n\n![](https://pbs.twimg.com/media/F1gpN7rakAAyDYo.jpg)\n\nHere's a crazier version, playing around with sql databases and zipped folders\n\nno talk; just do\n\n# Task reading:\nBefore starting\nwrite python,\nto read and then perform \"[https://t.co/zZ6gTgWgmy\"](https://t.co/zZ6gTgWgmy)\nthen\nwrite python,\nquery tasks & summaries\nin \"chatGPT_Todo.sqlite\"\n\nBeing by‚Ä¶ [https://t.co/3p1DFxKzuT](https://t.co/3p1DFxKzuT)\n\nSpent the day tinkering! Part 2 and new shorter prompt with SQL! [https://t.co/IEc8eyf3aN](https://t.co/IEc8eyf3aN)\n\nPt3 [https://t.co/TgnmsA1DiM](https://t.co/TgnmsA1DiM)\n\nPt4 [https://t.co/kVMksHy9Gl](https://t.co/kVMksHy9Gl)\n",
        "tags": [],
        "created": "2023-08-10T22:38:08.134Z",
        "updated": "2023-08-10T22:38:08.134Z"
    },
    {
        "id": "L66zoEa2Qb62cNyZ4I5c",
        "title": "The Benefits of Code Interpreter AI Agents",
        "markdown": "## The Benefits of Code Interpreter AI Agents\n\n[https://twitter.com/imaurer/status/1688280653857509376](https://twitter.com/imaurer/status/1688280653857509376)\n\n**ü™ÑSmart summary:**\nCode interpreter is the most advanced AI agent as of August 6th 2023, and this thread shares some of the best posts on the subject, including a wild AI bomb project.\n-------\n\nCode interpreter is the best AI agent as of August 6th 2023. \n\nSharing some of the best posts on this subject.\n\nThis AI bomb project is wild. \n\n[https://t.co/BpKdH7sh32](https://t.co/BpKdH7sh32)\n",
        "tags": [],
        "created": "2023-08-06T22:19:04.189Z",
        "updated": "2023-08-06T22:19:04.189Z"
    },
    {
        "id": "6CLAKgv0XIctIrYlrrdA",
        "title": "Connecting Gorilla's Free Inference API to LLM CLI Tool",
        "markdown": "## Connecting Gorilla's Free Inference API to LLM CLI Tool\n\n[https://twitter.com/simonw/status/1686595720789581825](https://twitter.com/simonw/status/1686595720789581825)\n\n**ü™ÑSmart summary:**\nGorilla offers a free inference API endpoint that imitates the OpenAI API, which can be used with the CLI tool found at https://t.co/6mRowSMuUb. To use it, add the specified model_id, model_name, api_base, and api_key to the extra-openai-models.yaml file, then prompt it with the desired command.\n-------\n\nGorilla offer a free inference API endpoint which imitates the OpenAI API... which means you can call it from my [https://t.co/6mRowSMuUb](https://t.co/6mRowSMuUb) CLI tool!\n\nI just started a Discussions area for LLM and published a tip on exactly how to hook it up to Gorilla here: [https://t.co/ReyaDf5Flc[https://t.co/m6ZDCGoTYO](https://t.co/m6ZDCGoTYO)](https://t.co/ReyaDf5Flc)\n\nShort version: add the following to ~/Library/Application Support/io.datasette.llm/extra-openai-models.yaml\n\n- model_id: gorilla\n model_name: gorilla-7b-hf-v1\n api_base: \"[https://t.co/FDyxU4kRW2\"](https://t.co/FDyxU4kRW2)\n api_key: EMPTY\n\nThen prompt it with:\n\nllm -m gorilla 'Convert audio to text'\n",
        "tags": [],
        "created": "2023-08-02T16:38:05.472Z",
        "updated": "2023-08-02T16:38:05.472Z"
    },
    {
        "id": "3ZYhKGXrXKyhoxN52xO4",
        "title": "AI-Powered Code Review: How It Works and How to Implement It",
        "markdown": "## AI-Powered Code Review: How It Works and How to Implement It\n\n[https://twitter.com/mattzcarey/status/1684981044519268352](https://twitter.com/mattzcarey/status/1684981044519268352)\n\n**ü™ÑSmart summary:**\nThis thread explains how AI can be used to spot bugs in large files quickly and efficiently. It also provides a link to a Cisco study that found that code review performance drops after 200 lines and that authors who prepare reviews with annotations and explanations have fewer defects. Finally, it provides instructions on how to install code-review-gpt into Github Actions with one command and provides a demo link.\n-------\n\nMy code reviewer can spot bugs in huge files in a minute üëÄ\n\nYours gets bored after 200 lines üò¨\n\nThis is how it works\n\nüßµüëá \n\n![](https://pbs.twimg.com/media/F2I5Qp0XoA8rGq3.jpg)\n\nIf the file fits in the context window of the AI all good üéâ\n\n(context window really is king üëëuse 8k+ models to get the best performance)\n\nIf it is too big use a method similar to classic document retrieval üëá \n\n![](https://pbs.twimg.com/media/F2I9XoLWsBE858d.jpg)\n\nWe just implemented enhanced chucking for these languages with @LangChainAI RecursiveCharacterTextSplitter.\n\n@hwchase17 and co are smashing out the features :) \n\n![](https://pbs.twimg.com/media/F2JAl6GWsAM6hud.jpg)\n\nFind the code here: [https://t.co/stplRVtDXz](https://t.co/stplRVtDXz)\n\nGive a star if you think it's fun ‚≠êÔ∏è\n\nThe front image is from a Cisco study into the performance of human code reviewers. \n\nThey found that \"review performance drops over 200 [lines]\" \n\n\"authors who prepare the review with annotations and explanations have far fewer defects\" \n\nLink: [https://t.co/EK8pWAo2OS](https://t.co/EK8pWAo2OS)\n\nInstall code-review-gpt into your Github Actions with 1 command.\n\n`npm i code-review-gpt && code-review-gpt configure`\n\nAnd get annotations and suggestions to help your reviews. \n\nDemo: [https://t.co/UQBdvqq4mx](https://t.co/UQBdvqq4mx)\n\nI hope this was helpful üòÅ\n\nIf you enjoyed this thread, please:\n\n1. Follow me@mattzcarey for tips on building with AI\n\n2. Like and retweet the first tweet below üëá\n\n[https://t.co/zEjgHEuFWJ](https://t.co/zEjgHEuFWJ)\n",
        "tags": [],
        "created": "2023-07-30T16:44:04.629Z",
        "updated": "2023-07-30T16:44:04.629Z"
    },
    {
        "id": "Gvb9sPKxZqTyPPdJI4ox",
        "title": "Introducing LLaMA-2-7B-32K: A 32K Context Model for Doc Understanding, Summarization &amp; QA",
        "markdown": "## Introducing LLaMA-2-7B-32K: A 32K Context Model for Doc Understanding, Summarization &amp; QA\n\n[https://twitter.com/togethercompute/status/1685048832168714240](https://twitter.com/togethercompute/status/1685048832168714240)\n\n**ü™ÑSmart summary:**\nWe have released LLaMA-2-7B-32K, a 32K context model that can be used for tasks such as document understanding, summarization, and QA. It is built with Position Interpolation and our data recipe/optimizations, and can run inference and fine-tune with up to 3x speedup. We have evaluated the model and found that it achieves comparable quality to the original LLaMA-2-7B base model, and has improved accuracy by up to 15 points and ROUGE score\n-------\n\nWe just released LLaMA-2-7B-32K, a 32K context model that can be fine-tuned for tasks like doc understanding, summarization & QA!\n\nBuilt with Position Interpolation & our data recipe/optimizations, run inference & fine-tune with up to 3x speedup.\n\nThreadüëá\n[https://t.co/qaxFI0Xb9M](https://t.co/qaxFI0Xb9M)\n\nThe open-source AI ecosystem has made rapid progress in the last several months with RedPajama, LLaMA-2, Falcon & others.\n\nWe believe the next opportunity for open-source is to extend context length to 32K-128K to match (and even surpass) state-of-the-art closed-source models.\n\nEnter LLaMA-2-7B-32K!\n\nTo extend context length we created a new data recipe with:\n\n1) Generic long-context language data to learn interpolated positional embeddings\n2) Instruction data to take advantage of long-context info\n\nHere's the model in our Playground completing a book‚Üí \n\n![](https://pbs.twimg.com/media/F2J_FWabQAA-m-v.jpg)\n\nNext, we evaluated the model.\n\nOn HELM v1.0, LLaMA-2-7B-32K achieves comparable quality than the original LLaMA-2-7B base model. \n\n![](https://pbs.twimg.com/media/F2J_PsCbkAAB85w.jpg)\n\nWe also fine-tuned the model for targeted, long-context applications‚Äîand made it available on OpenChatKit so you can do the same.\n\nExamples below showcase two scenarios: 1) long-context QA and 2) book summarization.\n\n[https://t.co/MpCq8h7Inf](https://t.co/MpCq8h7Inf)\n\n‚ÅâÔ∏èLong-context QA\n\nInput for the model included 1) question requiring an answer, 2) k documents w/ 1 doc containing the answer.\n\nAccuracy improved by up to 15 points once we fine-tuned LLaMA-2-7B-32K on the task. (For LLaMA-2,¬†truncated input when it didn't fit into 4K context.) \n\n![](https://pbs.twimg.com/media/F2KAu2tb0AAfMLn.png)\n\nüìöBook summarization\n\nUsing @salesforce BookSum data, we input long-form text to produce highly abstractive summaries.\n\nROUGE score improved by 13-29 points once we fine-tuned LLaMA-2-7B-32K on the task. (For LLaMA-2,¬†truncated input when it didn't fit into 4K context.) \n\n![](https://pbs.twimg.com/media/F2KBbImbYAAmLLv.jpg)\n\nLastly, we updated both the inference & training stack for greater efficiency.\n\nWe used  from our Chief Scientist @tri_dao & other optimizations, resulting in up to 3x improvement in inference and training throughput.\n\nMore on FA2 here: [https://t.co/XGqcpdp5Fr](https://t.co/XGqcpdp5Fr)\n\nWant to try it yourself?\n\nüí° Run LLaMA-2-7B-32K with our inference API & Playgrounds: [https://t.co/IMjX88xORK](https://t.co/IMjX88xORK)\n\nüî® Fine-tune a 32K model over LLaMA-2-7B-32K for long-context applications w/ OpenChatKit: [https://t.co/MPSUS4iWxC](https://t.co/MPSUS4iWxC)\n\nüìâ Read findings on our bog: [https://t.co/qaxFI0Xb9M](https://t.co/qaxFI0Xb9M)\n",
        "tags": [
            "FlashAttention2"
        ],
        "created": "2023-07-28T22:36:05.831Z",
        "updated": "2023-07-28T22:36:05.832Z"
    },
    {
        "id": "Gu82Rx1IHBWSboocgwG5",
        "title": "Celebrating 20,000 Stars on Github: Our Top 20 Repositories Built on @Gradio",
        "markdown": "## Celebrating 20,000 Stars on Github: Our Top 20 Repositories Built on @Gradio\n\n[https://twitter.com/Gradio/status/1684950314833895424](https://twitter.com/Gradio/status/1684950314833895424)\n\n**ü™ÑSmart summary:**\nWe recently reached 20,000 stars on Github and to celebrate, we are showcasing our top 20 repositories built on @Gradio. These repositories include AUTOMATIC1111/stable-diffusion-webui, XingangPan/DragGAN, lm-sys/FastChat fschat, oobabooga/Text generation web UI, PaddlePaddle/PaddleHub, NVIDIA/NeMo, PaddlePaddle/VisualDL, clovaai/donut, hiyouga/ChatGLM-Efficient-\n-------\n\nWe recently reached 20,000 stars on Github. To celebrate this milestone, we thought it would be a great opportunity to showcase our top 20 repositories that are built on @Gradio! Did we miss any?\n\nHere we go üßµ‚¨áÔ∏è \n\n![](https://pbs.twimg.com/media/F2IgkclboAERM62.jpg)\n\n1Ô∏è‚É£üëëThe King\nAUTOMATIC1111/stable-diffusion-webui üåü93K+ Stars - [https://t.co/0rR0rVTvyr](https://t.co/0rR0rVTvyr)\nüôåUse open-source models for image generation from text, in/outpainting, training, multi-embeddings, upscaling, batch processing, and more. It's an AI Swiss army knife for your art projects!\n\n2Ô∏è‚É£ XingangPan / DragGAN - 31K+ Stars\n[https://t.co/UtPQRsf5aa](https://t.co/UtPQRsf5aa)\n\nMeet DragGAN: A breakthrough in GANs, enabling precise, user-interactive control of image synthesis. Manipulate pose, shape, and more across diverse categories with ease. Superior performance in image manipulation.\n\n3Ô∏è‚É£ lm-sys / FastChat fschat 25K+ Stars üåü -\n[https://t.co/8hETmeXD3v](https://t.co/8hETmeXD3v)\n\n@lmsysorg's FastChat, your one-stop solution for training, serving, and evaluating large language models like Vicuna & FastChat-T5, now comes with a distributed serving system and OpenAI-compatible APIs.\n\n4Ô∏è‚É£ ü¶Åoobabooga/ Text generation web UI- 19K+ Stars -\n[https://t.co/v3yISAE8AY](https://t.co/v3yISAE8AY)\n\nDiscover this Gradio Web UI, your portal to run Large Language Models like LLaMA2, llama.cpp, GPT-J, OPT, etc. aiming to be the go-to for text generation akin to AUTOMATIC1111 / stable-diffusion-webui.\n\n5Ô∏è‚É£ PaddlePaddle / PaddleHub 11K Stars\n[https://t.co/Haz5oCEtmJ](https://t.co/Haz5oCEtmJ)\n\n400+ high-quality AI models across CV, NLP, Speech, Video, and Cross-Modal. Just 3 lines of code to predict, one line to serve, and fully cross-platform! @PaddlePaddle\n\n6Ô∏è‚É£ NVIDIA / NeMo (nemo-toolkit) 7K+ Stars\n[https://t.co/kg3xJGclkB](https://t.co/kg3xJGclkB)\n\n@nvidia's NeMo, a conversational AI toolkit crafted for ASR, TTS, LLMs, & NLP research. Facilitating industry & academia with code and pre-trained model reuse, it simplifies the creation of new conversational AI.\n\n7Ô∏è‚É£PaddlePaddle/VisualDL - 4.5K+ Stars\n[https://t.co/E1SlHrjEWy](https://t.co/E1SlHrjEWy)\n\nDive into VisualDL from @PaddlePaddle. It offers a range of charts to display parameter trends, visualize model structures, data samples, tensors, and performance curves. Intuitive way to understand & optimize models.\n\n8Ô∏è‚É£ clovaai / donut - 4K+ Stars -\n[https://t.co/orvBPljSEX](https://t.co/orvBPljSEX)\n\nMeet Donut, the Document Understanding Transformer from [https://t.co/RC7M8bOST9.](https://t.co/RC7M8bOST9.) Designed to help you extract meaning from your documents with ease.\n\n9Ô∏è‚É£ hiyouga / ChatGLM-Efficient-Tuning - 2.5K+ Stars\n[https://t.co/T9a1sUgIAB](https://t.co/T9a1sUgIAB)\n\nDiscover ChatGLM Efficient Tuning from hiyouga -@zhengyw1999\n\nFine-tune your ChatGLM-6B model effortlessly with PEFT.\n\nüîüdeep-diver / llm as a chatbot - 2.5K+ Stars\n[https://t.co/ZCtXPRoeEf](https://t.co/ZCtXPRoeEf)\n\nüòçIntroducing @algo_diver's LLM as a Chatbot Service is a @Gradio UI\n\nUtilizes variety of OS LLM models for chat format, aided by his own Ping Pong library for model-agnostic context management for seamless UI\n\n1Ô∏è‚É£1Ô∏è‚É£RayVentura/shortgpt 2.7K+ Stars\n[https://t.co/UndEzECOhr](https://t.co/UndEzECOhr)\n\nüò≤ShortGPT by @RayVenturaHQ, a robust framework automating content creation\n\nSimplifies video creation, asset sourcing, voiceover, & auto-generate captions, supported by LLM oriented video editing & memory persistency.\n\n1Ô∏è‚É£2Ô∏è‚É£ juncongmoo / pyllama ~2.5K Stars\n[https://t.co/ydW6j7pkVm](https://t.co/ydW6j7pkVm)\n\nIntroducing pyllama by juncongmoo - a streamlined adaptation of LLaMA designed to run conveniently on a single consumer-grade 4GB GPU.\n\n1Ô∏è‚É£3Ô∏è‚É£ TencentARC / T2I-Adapter 2.1K+ Stars\n[https://t.co/wUEf7tY4om](https://t.co/wUEf7tY4om)\n\nUnveiling T2I-Adapter by TencentARC, the official implementation designed to unlock greater control capabilities for Text-to-Image Diffusion Models.\n\n1Ô∏è‚É£4Ô∏è‚É£ openvinotoolkit / anomalib 2.1K+ Stars\n[https://t.co/dhiD9pGZXg](https://t.co/dhiD9pGZXg)\n\nCheck out anomalib by @floating_otter - a comprehensive toolkit designed for benchmarking, developing, and deploying deep learning anomaly detection algorithms.\n\n1Ô∏è‚É£5Ô∏è‚É£hiyouga/LLaMA-Efficient-Tuning ~2K Stars\n[https://t.co/eIm3z9iFZ0](https://t.co/eIm3z9iFZ0)\n\n@zhengyw1999's 2nd Repo is a versatile Hub for training various LLaMA models. It includes support for models like LLaMA-2, Baichuan-13B, Falcon-7B/40B & more. Repo offers an integrated WebUI for fine-tuning.\n\n1Ô∏è‚É£6Ô∏è‚É£ haoheliu / AudioLDM 1.7K+ Stars\n[https://t.co/tSR1qzMvah](https://t.co/tSR1qzMvah)\n\nExperience AudioLDM by haoheliu - revolutionizing audio generation. Features include Text-to-Audio and Audio-to-Audio Generation, as well as Text-guided Audio-to-Audio Style Transfer for immersive sound experiences.\n\n1Ô∏è‚É£7Ô∏è‚É£ huggingface / evaluate 1.4K+ Stars\n[https://t.co/JVH5Y5SWoK](https://t.co/JVH5Y5SWoK)\n\nExplore Evaluate by our very own üòç@huggingface - a library designed to simplify and standardize the evaluation, comparison, and performance reporting of your models.\n\n1Ô∏è‚É£8Ô∏è‚É£ sweepai / sweep - 1.3K+ Stars\n[https://t.co/8iXFon7FZI](https://t.co/8iXFon7FZI)\n\nMeet Sweep by @sweep__ai - your AI junior developer that converts bug reports and feature requests into actionable code changes, reading your codebase, planning alterations, and crafting pull requests.\n\n1Ô∏è‚É£9Ô∏è‚É£ chatarena / chatarena 900+ Stars\n[https://t.co/orTVYCle9q](https://t.co/orTVYCle9q)\n\nIntroducing ChatArena by chatarena - a library offering multi-agent language game environments, paving the way for research on autonomous LLM agents and their social interactions.\n\n2Ô∏è‚É£0Ô∏è‚É£ liltom-eth / llama2-webui - 750 Stars\n[https://t.co/9CuRxOQwbn](https://t.co/9CuRxOQwbn)\n\nGet onboard with llama2-webui by liltom-eth. Run Llama 2 on any device, supporting all Llama 2 models in 8-bit, 4-bit mode, on GPU with at least 6GB VRAM, or CPU.\n",
        "tags": [],
        "created": "2023-07-28T20:03:09.726Z",
        "updated": "2023-07-28T20:03:09.726Z"
    },
    {
        "id": "o3qmJwOrdOlBbr8zrK70",
        "title": "Building an AI Agent for Office Tasks with ChatGPT and RASCEF",
        "markdown": "## Building an AI Agent for Office Tasks with ChatGPT and RASCEF\n\n[https://twitter.com/cj_zZZz/status/1684630222673698818](https://twitter.com/cj_zZZz/status/1684630222673698818)\n\n**ü™ÑSmart summary:**\nThe author has built an AI Agent that runs 90% of their office job tasks using ChatGPT Code Interpreter and the RASCEF Prompt Framework. They have also created a newsletter called 'The AI Startup' which provides blueprints to make money with AI tools, AI Automation, and AI Saas.\n-------\n\nI've built my AI Agent that runs 90% of my office job tasks using:\n\nChatGPT Code Interpreter + \"RASCEF\" Prompt Framework.\n\nR: Role\nA: Action\nS: Steps\nC: Context\nE: Examples\nF: Format\n\nI used ChatGPT Code interpreter to upload the docs. as context or examples and applied this‚Ä¶ [https://t.co/gmYTnmrAmR](https://t.co/gmYTnmrAmR)\n\n![](https://pbs.twimg.com/media/F2EBzOZaYAEJ7Cu.png)\n\nIf you want to make money with AI ‚Üí Join 'The AI Startup' Newsletter\n\nI'll give you complete blueprints to make money with:\n‚Ä¢ AI tools\n‚Ä¢ AI Automation \n‚Ä¢ AI Saas.\n\n[https://t.co/MM2IBk6dH8](https://t.co/MM2IBk6dH8)\n",
        "tags": [],
        "created": "2023-07-28T15:04:04.781Z",
        "updated": "2023-07-28T15:04:04.781Z"
    },
    {
        "id": "wo8zXOefa73YOHD9lOLl",
        "title": "Introducing VectorAdmin: An Open-Source Universal GUI for Vector Databases",
        "markdown": "## Introducing VectorAdmin: An Open-Source Universal GUI for Vector Databases\n\n[https://twitter.com/tcarambat/status/1684614486798565376](https://twitter.com/tcarambat/status/1684614486798565376)\n\n**ü™ÑSmart summary:**\nVectorAdmin is an open-source, universal GUI for vector databases that allows users to atomically manage embeddings, upload documents, update single embeddings, and clone entire namespaces. It is MIT Licensed and works with multiple vector databases, is 100% private, and is looking for contributors. An in-depth demo is available.\n-------\n\nIntroducing VectorAdmin an open-source, universal GUI for vector databases.\n\n‚úÖ Atomically manage embeddings in @trychroma or @pinecone \n‚úÖ Upload documents straight to your vector db\n‚úÖ Update single embeddings on the fly\n‚úÖ Clone entire namespaces at no cost\n+ dev tools!! [https://t.co/4r3pNsY5W8](https://t.co/4r3pNsY5W8)\n\n![](https://pbs.twimg.com/media/F2DwmYcbIAAG6WN.jpg)\n\nFirst, you can run this tool yourself on GitHub here:\n[https://t.co/uAyAnQCs8D](https://t.co/uAyAnQCs8D)\n\nRequirements:\n- Python 3.9\n- Nodejs\n- A @pinecone or @trychroma database\n\nWe do everything else for you. Whether it's 10 vectors or 100,000 - VectorAdmin is the tool for the job.\n\nWhen using @LangChainAI for splitting and uploading to a vector DB you very quickly realize that you lose control of what's actually in your vector database.\n\nThis is unacceptable for a startup or a business using vector databases for unlocking more powerful LLM applications \n\n![](https://pbs.twimg.com/media/F2DyfN-a8AAjxdl.jpg)\n\nVectorAdmin isn't just for LangChain users though, it works with the data regardless of how it got there.\n\nHere are some more benefits of VectorAdmin\n‚úÖ Multiple vector database support\n‚úÖ Cloning entire namespaces or docs\n‚úÖ 100% private - all data stays on-prem\n+ More to come! \n\n![](https://pbs.twimg.com/media/F2DzTC8akAAcvo0.jpg)\n\nVectorAdmin is MIT Licensed and like AnythingLLM, we are looking for contributors and building a community of developers and people integrating this technology!\n\nDiscord invite is in the GitHub Readme! Join us!\n[https://t.co/uAyAnQCs8D](https://t.co/uAyAnQCs8D)\n\nHere is an in-depth demo of me connecting to my Pinecone & Chroma instance and automagically pulling in all my vectors so I can ready, update, and manage them without going crazy.\n\n[https://t.co/pEc0tJMPG3](https://t.co/pEc0tJMPG3)\n",
        "tags": [],
        "created": "2023-07-27T22:00:05.752Z",
        "updated": "2023-07-27T22:00:05.752Z"
    },
    {
        "id": "tdIZSbcO4Vi4yZ3K1h3S",
        "title": "Creating an Automated Meeting Minutes Generator with Whisper & GPT-4",
        "markdown": "## Creating an Automated Meeting Minutes Generator with Whisper & GPT-4\n\n[https://twitter.com/OfficialLoganK/status/1684579505543970817](https://twitter.com/OfficialLoganK/status/1684579505543970817)\n\n**ü™ÑSmart summary:**\nOpenAI has released a new tutorial on creating an automated meeting minutes generator with Whisper & GPT-4. The tutorial is available online and feedback is welcome. More tutorials are coming soon.\n-------\n\nüì£ New tutorial is live in the @OpenAI docs, \"Creating an automated meeting minutes generator with Whisper & GPT-4\", a very practical and useful walkthrough.\n\n[https://t.co/4KZNe4K3CX](https://t.co/4KZNe4K3CX)\n\n![](https://pbs.twimg.com/media/F2DT2ASacAcrdlk.jpg)\n\nAs you work through this tutorial, please send along any feedback or issues you run into so we can fast follow. More tutorials coming soon : )\n",
        "tags": [],
        "created": "2023-07-27T16:43:05.426Z",
        "updated": "2023-07-27T16:43:05.426Z"
    },
    {
        "id": "ecIsQ8axSioYM4dmUw0j",
        "title": "Exploring Text-Generation WebUI: Installing Locally, Using RunPod, and Google Colab",
        "markdown": "## Exploring Text-Generation WebUI: Installing Locally, Using RunPod, and Google Colab\n\n[https://twitter.com/yvrjsharma/status/1683848226783518720](https://twitter.com/yvrjsharma/status/1683848226783518720)\n\n**ü™ÑSmart summary:**\nText Generation WebUI is a powerful open-source tool built in @Gradio that allows users to download, infer, chat, and fine-tune cutting-edge models like @MetaAI's Llama2. It can be installed locally with GPUs, used with RunPod, or used with Google Colab. It also provides parameters and model tabs to customize the experience. Show some love to @Gradio and Oobabooga on Twitter and Github.\n-------\n\nText-Generation WebUI is a @Gradio UI for running LLaMA2 & other LLMs (+ cpp gptq). @TheBlokeAI thinks it is The Most Popular WebUI !\n\nI explored how you can run the WebUI yourself & following 3 stood out-\n1‚É£Install locally with GPUs\n2‚É£Use RunPod\n3‚É£Use Google Colab \n\nLearn moreüßµ \n\n![](https://pbs.twimg.com/media/F14QzQsaUAAICBv.jpg)\n\n1‚É£ Instal locally if you have a GPU. \n\nText-generation-web-UI repo has made available one-click installers for your machine types.\n\n1 &2) For Windows, run start_windows.bat file,\n3) It will download everything\n4) Next, select any Llama2 model fromü§óhub. \n\n![](https://pbs.twimg.com/media/F13yWMLaQAEymoJ.png)\n\n2‚É£Use RunPod- If you don't have GPUs in your machine, you can rent one by hour from [https://t.co/1bL1Gbdj9L](https://t.co/1bL1Gbdj9L)\n\nRunPod is cheap, has 1-click setup & comes with\n1) @TheBlokeAI's preconfigured TextGen-WebUI template\n2) shows logs\n3&4) Connects you to @Gradio UI \n\n![](https://pbs.twimg.com/media/F14RHg1agAANemP.jpg)\n\n3‚É£Using Google Colab - Awesome work from @camenduruüôå \n\n1)Goto the repo linked\n2,3)Select model & preconfig'd WebUI\n4,5)Open in Colab and run all!\n\nRepo: [https://t.co/kNePHSgJiX](https://t.co/kNePHSgJiX)\n\n![](https://pbs.twimg.com/media/F14dLDgaUAEzSDc.png)\n\nWhat is possible with TextGeneration WebUI?\n\nüôãText Generation tab: You can select the Inference Mode of your Llama2 model. Options are mainly -\n\n1‚É£A Chat interface, or\n2‚É£The default Instruct Mode \n\nTextGeneration-WebUI Github Repo: [https://t.co/rKtPouRVpc[https://t.co/9kB4Iz8ejL](https://t.co/9kB4Iz8ejL)](https://t.co/rKtPouRVpc)\n\nüôãMore on Text Generation:\n\n1&2)üôåFor Instruct mode, get precise Prompts for any model from dropdown\n\nüõ†Ô∏èParameters tab: \n3) Shows you all the available params, eg - top_p, top_k, temp, penalties, early stopping etc. \n\n![](https://pbs.twimg.com/media/F14o4HPaAAAJQvy.png)\n\nüõ†Ô∏èParameters tab provides you all the params and options that you can need for LLM inference!\n\nü¶æExample, make your chat more interesting and non-repetitive with a higher temperature value, or\n\nüì¶Select a preset parameter setting based on your model types. [https://t.co/Dtof8aYrcv](https://t.co/Dtof8aYrcv)\n\nü§ñ Model tab:\n\nüëëTextGeneration Web UI is the King! - It lets you download any Llama2 model through @huggingface simply by copy-pasting the hub repository name in there.\nüôåSo easy-\n1‚É£Download new model 2‚É£Refresh available model list 3‚É£Head over to Text-Generation Tab & have fun! \n\n![](https://pbs.twimg.com/media/F14vAjdaYAEI3GZ.jpg)\n\nüöÇTraining your Llama2 models in TextGen-WebUI:\n\nüéÅSupports Alpaca format out-of-box(Instruction-Input-Output as Json structure)\nü™ÑBut you can set up any model's prompt format in JSON files\nüóíÔ∏èYou can even train on Raw-text !\n\nFollow the guide for more - [https://t.co/Du2BsJn7mu[https://t.co/ZnWZDVDA42](https://t.co/ZnWZDVDA42)](https://t.co/Du2BsJn7mu)\n\nTo summarise the awesomeness of Oobabooga:\n\nüî•Text Generation Web UI is the hottest LLM UI currently\n\nüí™A very powerful Open-Source tool built entirely in @Gradio\n\nüß†Allows the Download, Inference, Chat, & Fine-tune of cutting-edge models like @MetaAI's Llama2 in easiest of ways!\n\nIf this was useful to the community, you can show some love to @Gradio on Twitterüê¶ and on Github here üåüü§©- [https://t.co/B9PosysOOx](https://t.co/B9PosysOOx)\n\nAnd to Oobaboogaüåüü§© here - [https://t.co/rKtPouRVpc](https://t.co/rKtPouRVpc)\n\n![](https://pbs.twimg.com/media/F144V_ragAA4n_A.png)\n",
        "tags": [],
        "created": "2023-07-26T03:45:05.734Z",
        "updated": "2023-07-26T03:45:05.734Z"
    },
    {
        "id": "VyjDXFGBSFQZV3AleNQx",
        "title": "Quick Tip: Using the llm CLI Tool to Output a List of Files in a Zip",
        "markdown": "## Quick Tip: Using the llm CLI Tool to Output a List of Files in a Zip\n\n[https://twitter.com/simonw/status/1682974944844738562](https://twitter.com/simonw/status/1682974944844738562)\n\n**ü™ÑSmart summary:**\nThis thread provides a quick tip for a CLI tool, showing how to save a system prompt to a template and use it to output a list of files in a zip. More documentation is available on the prompt template feature.\n-------\n\nQuick tip for my [https://t.co/6mRowSMuUb](https://t.co/6mRowSMuUb) CLI tool:\n\n# Save this system prompt to a template called cmd:\nllm -s 'reply with macos terminal commands only, no extra information' --save cmd\n# Use that template:\nllm -t cmd 'output list of files in a zip'\n# Output:\nunzip -l file .zip \n\n![](https://pbs.twimg.com/media/F1sgwjCaIAAO2mH.jpg)\n\nMore documentation on the prompt template feature this uses here: [https://t.co/BsBOXQr2DG](https://t.co/BsBOXQr2DG)\n",
        "tags": [],
        "created": "2023-07-23T18:00:04.965Z",
        "updated": "2023-07-23T18:00:04.965Z"
    },
    {
        "id": "lPPsDRfStnA2Z1SOitqp",
        "title": "Exploring the Absurdity of chatGPT Custom Instruction Coding",
        "markdown": "## Exploring the Absurdity of chatGPT Custom Instruction Coding\n\n[https://twitter.com/NickADobos/status/1682856282720710656](https://twitter.com/NickADobos/status/1682856282720710656)\n\n**ü™ÑSmart summary:**\nThis thread discusses the absurdity of coding an agent in a 1478 letter prompt. It includes tasks, subtasks, memory, text and emoji summaries, and skills. It also includes long term memory with SQL, coding instructions, and go fast, hotkeys, command menu, and help message. An example of trying things out is also included, with the codebase being shoved into a code interpreter.\n-------\n\nchatGPT custom instruction absurdity\ncoding agent in a 1478 letter prompt\n\nMocks @babyAGI_ elf \n-Tasks, subtasks\n-Memory, text & emoji summaries\n-Skills! (Experiment!)\n\nIncludes\n-long term memory w/SQL\n-coding instructions\n-go fast, hotkeys, command menu, and help message‚Ä¶ [https://t.co/DC15exey2Z](https://t.co/DC15exey2Z)\n\n![](https://pbs.twimg.com/media/F1q03lTXoAEq0yj.jpg)[https://t.co/TgnmsA1DiM](https://t.co/TgnmsA1DiM)\n\nHere‚Äôs an example trying things out\n\nNote me shoving my entire codebase into code interpreter \n\n[https://t.co/p7kB2jBXAJ](https://t.co/p7kB2jBXAJ)\n\n![](https://pbs.twimg.com/media/F1q04DdXwAAr64M.jpg)\n",
        "tags": [],
        "created": "2023-07-23T16:20:04.213Z",
        "updated": "2023-07-23T16:20:04.213Z"
    },
    {
        "id": "pHAEPdlJ4PAl5VGaBNxu",
        "title": "Introducing New GitHub Copilot Features in VS Code",
        "markdown": "## Introducing New GitHub Copilot Features in VS Code\n\n[https://twitter.com/code/status/1682435342610079761](https://twitter.com/code/status/1682435342610079761)\n\n**ü™ÑSmart summary:**\nGitHub Copilot has released several new features for VS Code, including the ability to move the chat session into the editor space, create workspaces and notebooks with slash commands, use regex searches, live preview mode, more relevant suggestions in notebook editors, and the ability to automatically implement suggestions when going through a PR review. There is also an experimental Quick Question experience. All of these features are available in the stable version of VS Code.\n-------\n\nIntroducing new GitHub Copilot features in VS Code! \n\nThere's a LOT going on, so here's everything we have to show you...üßµ\n\nYou can now move the chat session from the sidebar into the editor space. Which gives you a lot more room for....activities. [https://t.co/qHBhXNrQ3G](https://t.co/qHBhXNrQ3G)\n\nYou can ask Copilot to create workspaces for popular project types with the /createWorkspace slash command. Copilot will first generate a directory structure for your request. \n\nThen click \"Create Workspace\" and it will create the suggested project - files, directories and all. [https://t.co/qPf43E5HNq](https://t.co/qPf43E5HNq)\n\nWith the /createNotebook command, Copilot creates a notebook outline based on your requirements. If you like the outline, click \"Create Notebook\" to...you know...create a notebook. [https://t.co/wvijojepaw](https://t.co/wvijojepaw)\n\nCopilot can now write that regex searches for you. So you you can find stuff that you aren't sure how to find. Check out the /search command. [https://t.co/Xd5Jp5WME1](https://t.co/Xd5Jp5WME1)\n\nEditor chat now has a new \"livePreview\" mode, making it easier to apply changes directly to the document and fix errors before accepting suggestions. \n\n![](https://pbs.twimg.com/media/F1k1-ThWIBQLaNa.png)\n\nCopilot in notebook editors now uses the notebook context to provide more relevant suggestions. It even helps with cell execution failures and automatically accepts suggestions. [https://t.co/YY6xfIebzv](https://t.co/YY6xfIebzv)\n\nYou can now use Copilot to automatically implement suggestions when going through a PR review. Requires the GitHub Pull Requests and Issues extension. [https://t.co/cAzZVNR5PH](https://t.co/cAzZVNR5PH)\n\nThere's an experimental Quick Question experience: Ask quick programming questions without leaving context using chat. What do you think of this one? Would you use it? [https://t.co/KWhEyOjGBY](https://t.co/KWhEyOjGBY)\n\nFinally, you don't need Insiders anymore to use Copilot Chat. Stable will work just fine if that's your jam.\n\nOK - that's it for now - but not all of it. Check out all the details in the month's release notes: [https://t.co/MVDcXvfMBL](https://t.co/MVDcXvfMBL)\n",
        "tags": [],
        "created": "2023-07-23T15:40:04.126Z",
        "updated": "2023-07-23T15:40:04.126Z"
    },
    {
        "id": "CvHMQRd5GqsY7nW7QraE",
        "title": "Exploring Embeddings with Pokemon Clusters",
        "markdown": "## Exploring Embeddings with Pokemon Clusters\n\n[https://twitter.com/_ScottCondron/status/1681975988027105281](https://twitter.com/_ScottCondron/status/1681975988027105281)\n\n**ü™ÑSmart summary:**\nThis thread demonstrates how to explore clusters of data using OpenAI embeddings and quickly derive summary plots. An example is given of plotting Pokemon with projections of OpenAI embeddings and then plotting the types of the selected Pokemon.\n-------\n\nEmbeddings exploration within a notebook üîé\n\nSelect clusters and quickly derive summary plots about them\n\nIn this example, I've plotted Pokemon with projections of @OpenAI embeddings. Then, when I select cluster, there's a plot of selected Pokemon's types. [https://t.co/Ka0My5lYbj](https://t.co/Ka0My5lYbj)\n\n@_ScottCondron @OpenAI\n",
        "tags": [],
        "created": "2023-07-20T20:04:04.316Z",
        "updated": "2023-07-20T20:04:04.316Z"
    },
    {
        "id": "ZuVzoc1GSDQqbqv9dzMU",
        "title": "Train Llama-2 Models with 4bit and PEFT on a Single A100 GPU",
        "markdown": "## Train Llama-2 Models with 4bit and PEFT on a Single A100 GPU\n\n[https://twitter.com/lvwerra/status/1681701409677246471](https://twitter.com/lvwerra/status/1681701409677246471)\n\n**ü™ÑSmart summary:**\nIt is possible to train Llama-2 models on your own data with just a few lines of code, even with the 70B model on a single A100 GPU. Learn more and access the full script by following the provided links.\n-------\n\nDid you know that you can train all Llama-2 models on your own data in just a few lines?\n\nThe script even works with the 70B model on a single A100 GPU thanks to the magic of 4bit and and PEFT!\n\nLearn more: [https://t.co/njGyWdnbzT](https://t.co/njGyWdnbzT)\nFull script: [https://t.co/6y9FNdYSuT](https://t.co/6y9FNdYSuT)\n\n![](https://pbs.twimg.com/media/F1aXj11WYAEvU7N.jpg)\n\n@lvwerra\n",
        "tags": [],
        "created": "2023-07-19T19:54:04.991Z",
        "updated": "2023-07-19T19:54:04.991Z"
    },
    {
        "id": "XDcRyDctIGokiUPj70Z4",
        "title": "Building a Chat Application with Langchain and Clarifai UI Modules",
        "markdown": "## Building a Chat Application with Langchain and Clarifai UI Modules\n\n[https://twitter.com/clarifai/status/1679431918021120001](https://twitter.com/clarifai/status/1679431918021120001)\n\n**ü™ÑSmart summary:**\nThis thread provides a step-by-step guide on how to build a simple chat application using Langchain and Clarifai's UI Modules. It covers the necessary libraries to install, code to write, and how to deploy the app to the Clarifai platform. It also provides links to learn more about UI Modules and other Machine Learning and LLMs content.\n-------\n\nis a framework for developing applications powered by language models.\n\nIn this example, let's build a simple Chat Application with Langchain and create a web app using UI Modules and deploy it to the cloud.\n\nLet's look at it step-by-step\n\n1/8 üßµüëá [https://t.co/jqkoraT34h](https://t.co/jqkoraT34h)\n\nBefore getting started let's see What is an UI Module?\n\nModules are Clarifai's integration with Streamlit.\n\nThis will let you turn anything you do with your inputs, vectors, Machine Learning models, workflows, Python libraries into a beautiful web apps.\n\n2/8 \n\n![](https://pbs.twimg.com/media/F06JOdsagAIIDdS.jpg)\n\nAs a First step,\n\nClone the app module template from below, this makes it really easy in creating a UI module within Clarifai: [https://t.co/oMtieT7pjW](https://t.co/oMtieT7pjW)\n\nAnd Install the Necessary libraries:\n\n- langchain\n- streamlit\n- clarifai &\n- openai\n\n3/8\n\nHere is the code that:\n\n‚Üí Imports the Libraries along with the Clarifai's gRPC based objects.\n\n‚Üí Setting up the Title & Sidebar Image for the app using Streamlit\n\n‚Üí Getting the OpenAI API Key\n\n‚Üí A Function to Generate Text\n\n‚Üí A Form that returns the models response\n\n4/8 \n\n![](https://pbs.twimg.com/media/F06JbOVaMAAg5X2.jpg)\n\nNow once your app is working in local machine, let's create an UI Module within the Clarifai Platfrom and deploy it.\n\nFor this you have to create a New Application.\n\nGo to [https://t.co/dJ5EcXNTKC](https://t.co/dJ5EcXNTKC) and create a New App by specifying the details.\n\n5/8 \n\n![](https://pbs.twimg.com/media/F06JjIOagAQ85GV.jpg)\n\nNow within the App, go to the Modules Section and create a New Module\n\n6/8 \n\n![](https://pbs.twimg.com/media/F06JnR4aQAE-Jq_.jpg)\n\nNow give the details of the Module along with your project Github main branch URL\n\nFor this Langchain APP we are giving:\n[https://t.co/tSIwTtOe0n](https://t.co/tSIwTtOe0n)\n\nThat's it click deploy and your app will be deployed within the Clarifai Platform\n\n7/8 \n\n![](https://pbs.twimg.com/media/F06JsFzaMAA18ka.jpg)\n\nCheckout the Code here: [https://t.co/4kffayUHjR](https://t.co/4kffayUHjR)\n\nLearn more about UI Modules here: [https://t.co/5erboLFgyI](https://t.co/5erboLFgyI)\n\n8/8\n\nThat's a wrap!\n\nFollow us @Clarifai as we are coming up with more content on Machine Learning & LLMs.\n\nAlso we will be showcasing the capabilities of the UI Modules and how you can leverage them to build Applications in coming threads.\n\nStay Tuned.\n",
        "tags": [],
        "created": "2023-07-14T02:31:07.837Z",
        "updated": "2023-07-14T02:31:07.837Z"
    },
    {
        "id": "k5BvsX2Kkd1MrZelNERa",
        "title": "This article shows how to build a summarization app with langchain and OpenAI",
        "markdown": "This article shows how to build a summarization app with langchain and OpenAI\nhttps://dxiaochuan.medium.com/summarising-your-meeting-with-chatgpt-and-langchain-8eb646cfcdd1\n",
        "tags": [],
        "created": "2023-06-21T23:50:17.231Z",
        "updated": "2023-06-21T23:51:08.989Z"
    },
    {
        "id": "I8AL4HhdI80E73DSzNlp",
        "title": "Introducing the Getting Started with AI JS Stack",
        "markdown": "## Introducing the Getting Started with AI JS Stack\n\n[https://twitter.com/stuffyokodraws/status/1671557930062327808](https://twitter.com/stuffyokodraws/status/1671557930062327808)\n\n**ü™ÑSmart summary:**\nWe are launching The Getting Started with AI JS Stack, which includes authentication from ClerkDev, vector database from Pinecone and Supabase pgvector, image model from ReplicateHQ, text model from OpenAI, and deployment from Flydotio. We have also included an interactive CLI for developers to choose their own project scaffold and dependencies, and a lightweight fine-tuning step for open source models. We are thankful for the amazing open source projects that we leveraged in the stack.\n-------\n\n0/ Today we are launching ‚ú®The Getting Started with AI JS Stack‚ú® [https://t.co/7glgmTfXb7](https://t.co/7glgmTfXb7)\n- üîí@ClerkDev for auth\n- üíª@pinecone / @supabase pgvector for vector database\n- üé®@replicatehq for image model\n- ü§ñ@OpenAI for text model\n- üåê@flydotio for deployment \n....\n\nüßµ\n\n1/ üîí First, we included authentication due to the high demand & misuse potential of our powerful models. Without it, devs risk unexpected hefty bills from model providers. That's why we use @ClerkDev - efficient in bot detection and a strong support for future app development.‚Ä¶ [https://t.co/lxEGJaeiuo](https://t.co/lxEGJaeiuo)\n\n2/üé® Building AI apps shouldn't be hampered by the complexities of model hosting. That's why we've harnessed @OpenAI for text & @replicatehq for image analysis in our build. Replicate's support for text-based models is also plus ‚Äî try running Vicuna ([https://t.co/ki9UG2V2gj)](https://t.co/ki9UG2V2gj)) and‚Ä¶ [https://t.co/hb9nTrrkP0](https://t.co/hb9nTrrkP0)\n\n3/üíª To maintain state & context in LLMs, a robust long-term memory or vector database is vital, so we added both @pinecone and @supabase pgvector in the stack. We included paradigms for chunking, embedding generation more, thanks to @LangChainAI !\n\n4/üåê We use @flydotio for deployment, it's multi-region, easy to handle, and supports a broad compute environment (anything in a container). Over time, many AI projects end up adopting multiole languages and having non-trivial functionality in the backend, so Fly is a great‚Ä¶ [https://t.co/jdy26zwELM](https://t.co/jdy26zwELM)\n\n5/ This is a good starting point, and we are in the process of iterating on the stack. Here‚Äôs a glimpse of our roadmap:\n- An interactive CLI for **create-ai-stack**, where developers can choose their own project scaffold and dependencies\n- A lightweight fine-tuning step for open‚Ä¶ [https://t.co/FOM56Jj26g](https://t.co/FOM56Jj26g)\n\n6/ If you want to learn more, read our post here [https://t.co/UccUwLHyFe](https://t.co/UccUwLHyFe)\n\nI also want to thank the amazing open source projects we leveraged in the getting started AI stack. We couldn‚Äôt have pulled it off without your prior work ‚Äî ecosystem wins! \n\nThank you @tailwindcss‚Ä¶ [https://t.co/tN23uPeIpM](https://t.co/tN23uPeIpM)\n",
        "tags": [],
        "created": "2023-06-21T18:55:19.230Z",
        "updated": "2023-06-21T18:55:19.230Z"
    },
    {
        "id": "0RWZx2WEgFf2aIU3xHpx",
        "title": "Summarizing Meetings with ChatGPT and LangChain AI in 6 Simple Steps",
        "markdown": "## Summarizing Meetings with ChatGPT and LangChain AI in 6 Simple Steps\n\n[https://twitter.com/JorisTechTalk/status/1671496626790096901](https://twitter.com/JorisTechTalk/status/1671496626790096901)\n\n**ü™ÑSmart summary:**\nThis thread introduces a project that uses ChatGPT and LangChainAI to summarize online meetings in 6 simple steps. It explains how to load an audio file, use speech-to-text, split the transcript into chunks, and summarize the transcript. It also suggests possible future implementations, such as using Zapier to send the summarized meeting in an email. Tomorrow's project will be creating mindmaps with LangChainAI and XmindHQ.\n-------\n\nDid you know you can use ChatGPT to summarize your (online) meetings?\n\nWith @LangChainAI, and a couple of lines of code, you can!\n\nLet me show you how in 6 simple stepsüßµ\n\n \n\n![](https://pbs.twimg.com/media/FzJZUnlXoAAJsOv.jpg)\n\nBefore we dive in, this is day 2 of my '7 days of LangChain'.\n\nEvery day, I'll introduce you to a simple project that will guide you through the basics of LangChain.\n\nFollow @JorisTechTalk to stay up-to-date.\n\nIf there's anything you'd like to see, let me know!\n\nLet's dive in:\n\nHigh level overview of what's happening:\n\n1Ô∏è‚É£ Load your audio file\n2Ô∏è‚É£ Speech-to-text with Whisper\n3Ô∏è‚É£ Split the transcript into chunks\n4Ô∏è‚É£ Summarize the transcript\n\nLet's dive into the code ‚¨áÔ∏è \n\n![](https://pbs.twimg.com/media/FzJZVNFXsAE8YpW.jpg)\n\n1. Open your audio file.\n\nWhisper works on files with a maximum duration of approximately 20 minutes.\n\nLonger file? \n\nSplit it up by using PyTube and handle each chunk seperately. \n\n![](https://pbs.twimg.com/media/FzJZVo4X0AUzedk.jpg)\n\n2. Call the Whisper API\n\nOf course, you can use any speech-to-text API you prefer. I like Whisper for its accuracy and ease of use.\n\nAny open-source alternatives with quality output? \n\n![](https://pbs.twimg.com/media/FzJZWCkXoAMTn1a.jpg)\n\n3. Splitting the transcript into chunks\n\nWith the new OpenAI GPT 16k model, you can fit a large amount of context into one chunk. This is amazing for the model to 'understand' the full context and make connections.\n\nUse some overlap in order for context to not be lost. \n\n![](https://pbs.twimg.com/media/FzJZWdTXwAEVIQC.jpg)\n\n4. Prompting\n\nPrompting is key. It determines your output more than anything else.\n\nBe as concise as you can be. Instruct the model on how you want the output to look. You could include bullet points, main takeaways, follow-up actions and much more. \n\n![](https://pbs.twimg.com/media/FzJZW77XwAE_GgT.jpg)\n\n5. Initialize and run the summary chain\n\nI'm using the refine summarization chain. This is great when you're working with large files.\n\nIt generates an initial summary based on the first chunk and updates it with the subsequent chunks. \n\n![](https://pbs.twimg.com/media/FzJZXZuXoAY0Q_r.jpg)\n\n6. Export the summary to a text file\n\nYour meeting is summarized and you're ready to take action!\n\nOnce again, try to play around with the prompts you're using. This will greatly impact the resulting summarization. \n\n![](https://pbs.twimg.com/media/FzJZX0EX0AAoaQi.jpg)\n\n7. Possible future implementations\n\nYou can go wild with this. You could use a Zapier integration to send the summarized meeting in an email, create appointments in your schedule and much more.\n\nLet me know what you're going to add.\n\nThat concludes day 2 of '7 days of @LangChainAI'\n\nTomorrow's project: Creating mindmaps with @LangChainAI and @XmindHQ.\n\nFollow @JorisTechTalk to stay up-to-date.\n\nWhat else would you like to see?\n\nDay 2 of '7 days of @LangChainAI' ‚úÖ\n\nTomorrow's project will be exciting: generating mindmaps for studying!\n\nWhat else do you want to see? [https://t.co/oaSroHrs9d](https://t.co/oaSroHrs9d)\n",
        "tags": [
            "AI"
        ],
        "created": "2023-06-21T16:54:08.415Z",
        "updated": "2023-06-21T16:54:08.415Z"
    },
    {
        "id": "bNeWsPhxAUKq6xt8JqvK",
        "title": "GPT Engineer: Generating an Entire Codebase with AI",
        "markdown": "## GPT Engineer: Generating an Entire Codebase with AI\n\n[https://twitter.com/_akhaliq/status/1670976630515200008](https://twitter.com/_akhaliq/status/1670976630515200008)\n\n**ü™ÑSmart summary:**\nGPT Engineer is a tool that allows users to specify what they want it to build, and then the AI will ask for clarification and generate the codebase accordingly. It is designed to be easy to adapt and extend, allowing users to customize their code.\n-------\n\nGPT Engineer\n\ngithub: [https://t.co/I3oXZXmvYT](https://t.co/I3oXZXmvYT)\n\nSpecify what you want it to build, the AI asks for clarification, and then builds it.\n\nGPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based‚Ä¶ [https://t.co/lQr7AALcKM[https://t.co/UvMNw2RgAO](https://t.co/UvMNw2RgAO)](https://t.co/lQr7AALcKM)\n\n@_akhaliq\n",
        "tags": [],
        "created": "2023-06-20T13:56:06.740Z",
        "updated": "2023-06-20T13:56:06.740Z"
    },
    {
        "id": "bC5e0CNb4Qjs4Kt9CzWn",
        "title": "Unbelievable Results from Framer AI Launch",
        "markdown": "## Unbelievable Results from Framer AI Launch\n\n[https://twitter.com/namyakhann/status/1668880952423063552](https://twitter.com/namyakhann/status/1668880952423063552)\n\n**ü™ÑSmart summary:**\nFramer AI was recently launched and has already produced some impressive results, including UX/UI portfolios, photographer websites, fully responsive sites, landing pages for financial startups, monochrome photo galleries, marketing websites, and portfolios for developers. If you enjoyed this thread, follow @namyakhann and RT the first tweet to share with others. Join 750+ creatives who start their Sundays with @namyakhann's newsletter.\n-------\n\nFramer AI launched 12 hours ago and the results are unbelievable.\n\nMind-blowing examples below:\n\n1/ UX/UI Portfolio\n\n[ @zander_supafast ]\n[https://t.co/RbFOZDCekC](https://t.co/RbFOZDCekC)\n\n2/ Photographer website\n\n[ @zee7 ]\n[https://t.co/gwWfxwLyJv](https://t.co/gwWfxwLyJv)\n\n3/ Fully responsive site using a paragraph-long prompt\n\n[ @euboid ]\n[https://t.co/0faDy2ZZL8](https://t.co/0faDy2ZZL8)\n\n4/ Landing page for a financial startup\n\n[ @joshpuckett ]\n[https://t.co/xN6zpTesmv](https://t.co/xN6zpTesmv)\n\n5/ Monochrome Photo Gallery\n\n[ @JoshGuoSpace ]\n[https://t.co/dmpQ5iOIH5](https://t.co/dmpQ5iOIH5)\n\n6/ Marketing website\n\n[ @learnframer ]\n[https://t.co/Cu0m8aZtZK](https://t.co/Cu0m8aZtZK)\n\n7/ Landing page for a freelance community\n\n[ @hyperfreelancer ]\n[https://t.co/IjgFBUdJLk](https://t.co/IjgFBUdJLk)\n\n8/ Portfolio for developers\n\n[ @namyakhann ]\n[https://t.co/b5Wykrg4vO](https://t.co/b5Wykrg4vO)\n\nThat's it for today!\n\nIf you enjoyed this thread:\n‚Üí Follow me @namyakhann for more.\n‚Üí RT the first tweet to share with others.\n[https://t.co/8d6xbNUJIS](https://t.co/8d6xbNUJIS)\n\nLove my tweets?\n\nJoin 750+ creatives who start their Sundays with my newsletterüëá[https://t.co/xvJUdRtMkH](https://t.co/xvJUdRtMkH)\n",
        "tags": [],
        "created": "2023-06-14T18:22:10.742Z",
        "updated": "2023-06-14T18:22:10.742Z"
    },
    {
        "id": "yPChJyjXphL4jwlwpWX4",
        "title": "Exploring StarChatŒ≤: A Beta Version of üí´StarCoder+",
        "markdown": "## Exploring StarChatŒ≤: A Beta Version of üí´StarCoder+\n\n[https://twitter.com//status/1669020088203366401](https://twitter.com//status/1669020088203366401)\n\n**ü™ÑSmart summary:**\nLast week, StarChatŒ≤ was released, an instruction tuned model of StarCoder+. This thread showcases some of the cool examples generated with StarChat, such as helping with coding questions, image processing, errors, Pandas dataframes, and SQL queries. StarChat Beta is still in its Beta version, but it shows promise for open-access code models that are strong. You can play with the model at StarChat Playground or deploy your own Chat-UI.\n-------\n\nLast week we released StarChatŒ≤, an instruction tuned model of üí´StarCoder+.\n\nIn this thread, I will share some cool examples we generated with StarChat and show how it can be used to help you with coding questions.\n\nA thread üßµ \n\n![](https://pbs.twimg.com/media/FymJGDYX0AEgY8u.jpg)\n\nThese examples were generated by deploying a local version of the amazing Hugging Face Chat-UI: [https://t.co/vL64GneEBN](https://t.co/vL64GneEBN)\n\nüîç If you want to dive in the TS source code of how the prompt is built, StarChat can help! \n\n![](https://pbs.twimg.com/media/FymJVuGWwAEuzhd.png)\n\nüñºÔ∏è What about some Image Processing? \n\n![](https://pbs.twimg.com/media/FymJg-LWcAMSEEL.jpg)\n\nStarChat can also help with errors, to get a nice blurred image üå´Ô∏è \n\n![](https://pbs.twimg.com/media/FymJkjZXoAk2kYN.png)\n\nYou can also generate nice plots from Pandas dataframes \n\n![](https://pbs.twimg.com/media/FymJwseXoAk5NhH.jpg)\n\nThe great thing about StarChat, it knows more than just Pythonüìö \n\n![](https://pbs.twimg.com/media/FymLZ0wXoAUi4qS.png)\n\nAnd finally some SQL queries \n\n![](https://pbs.twimg.com/media/FymJ6n3WIAAphvh.jpg)\n\nStarChat Beta might hallucinate and generate problematic output. After all it's still a Beta version, but it shows we‚Äôre on a good path for code models that are open-access but also strong ‚ú®\n\nYou play more with the model at StarChat Playground:\n[https://t.co/KxotFiBlsA](https://t.co/KxotFiBlsA)\n\nOr you can deploy your own Chat-UI:\n[https://t.co/vL64Gnfcrl](https://t.co/vL64Gnfcrl)\n",
        "tags": [],
        "created": "2023-06-14T17:29:12.114Z",
        "updated": "2023-06-14T17:29:12.114Z"
    },
    {
        "id": "gVSCg9xpiAa7vPfHse8O",
        "title": "Unlocking the Power of OpenAI GPT API Function Calls",
        "markdown": "## Unlocking the Power of OpenAI GPT API Function Calls\n\n[https://twitter.com/Sentdex/status/1668795449447268354](https://twitter.com/Sentdex/status/1668795449447268354)\n\n**ü™ÑSmart summary:**\nOpenAI GPT API function calls are more powerful than expected, allowing for the generation of a list of commands from GPT-4 for a given prompt. This is easier and more reliable than pre-prompting and forms, and opens up the possibility of adding new features and abilities. This new paradigm for programming is very cool.\n-------\n\nThese new OpenAI GPT API function calls are more powerful than I expected.\n\nAll examples I've seen so far have a static number of pre-defined variables like \"filename\" \"src\" \"dst\" \"intent\"...etc, or weather for \"location\" and so on. üßµ1/6\n\nIn those examples, the information for functions was \"extracted\" from user input, not generated model output.\n\nYour \"function\" parm could be something more like: a list..of any size. For example, for TermGPT, I want a list of commands from GPT-4 for a given prompt. üßµ2/6 \n\n![](https://pbs.twimg.com/media/Fyi-vfaWYAAPvGG.jpg)\n\nHere, we're also forcing the function call to occur with the \"function_call\" part, which is not to be under estimated in usefulness. The \"auto\" is also cool, but often fails in more niche cases like this. The ability to let GPT-4 we definitely want this done is nice. üßµ3/6 \n\n![](https://pbs.twimg.com/media/Fyi-WC3WIAIfZXy.png)\n\nThe result is a neatly-organized list of commands that can then be run to achieve the prompt: üßµ4/6 \n\n![](https://pbs.twimg.com/media/Fyi_JKCWwAItPHp.jpg)\n\nThis is massively easier than what I have been doing up to this point with pre-prompting and forms and such for TermGPT, and is likely to be more reliable going forward. \n\nI can also much more easily add new features and abilities. üßµ5/6\n\nThis isn't limited to just a list of commands, it could be anything and I am not entirely sure this open-ended use-case was considered or intended. Suspect there are many more \"hacks\" that we'll see in the future. \n\nVery cool and totally a new paradigm for programming! üßµ6/6\n",
        "tags": [],
        "created": "2023-06-14T04:33:04.352Z",
        "updated": "2023-06-14T04:33:04.352Z"
    },
    {
        "id": "YfK9Ba2s3PzbzTXavgUp",
        "title": "Exploring the Magic of Bamboo AI",
        "markdown": "## Exploring the Magic of Bamboo AI\n\n[https://twitter.com/Luc_AI_Insights/status/1668792631806050304](https://twitter.com/Luc_AI_Insights/status/1668792631806050304)\n\n**ü™ÑSmart summary:**[https://t.co/ZJaHysWK5N](https://t.co/ZJaHysWK5N)\n\n![](https://pbs.twimg.com/media/Fyi9kXMWIAAOzb4.jpg)\n\n@Luc_AI_Insights\n",
        "tags": [
            "I explore AI products that offer great user experiences and, if they are open source, I dive deep into the code and logic. Today, I was exploring Bamboo AI, a tool that analyzes data.\n-------\n\nWhenever I use an AI product that offers a great user experience, I ask myself: \"How the F#ü§¨ did they do it?\"\n\nAnd if the product is open source, I dive deep into the code, the logic, and of course, the almighty prompt.\n\nToday, I was exploring Bamboo AI, a tool that analyzes‚Ä¶ "
        ],
        "created": "2023-06-14T04:28:11.256Z",
        "updated": "2023-06-14T04:28:11.256Z"
    },
    {
        "id": "rz5BCVHdch2HDj9WypaR",
        "title": "Automating Prompt Selection with Multi Prompt Chain",
        "markdown": "## Automating Prompt Selection with Multi Prompt Chain\n\n[https://twitter.com/MoonDevOnYT/status/1668418640197169152](https://twitter.com/MoonDevOnYT/status/1668418640197169152)\n\n**ü™ÑSmart summary:**\nWith LangChainAI's Multi Prompt Chain, users can easily handle multiple ChatGPT tabs. The process involves selecting the specialists, formulating the corresponding prompts, and allocating the templates to the prompt information. After configuring the multi prompt chain, users can initiate the chain and create whatever they want.\n-------\n\nEfficient prompts pave the way for superior outputs from ChatGPT.\n\nYet, handling 20 separate ChatGPT tabs can be bothersome.\n\nWith @LangChainAI 's Multi Prompt Chain, the problem is solved. It auto-selects the suitable prompt.\n\nIllustration: Marketing versus business expert. \n\n![](https://pbs.twimg.com/media/FydnoIwXoAIEXDe.jpg)\n\nFirstly, you need to select which 'specialists' you wish to incorporate.\n\nI've picked a marketing and business specialist.\n\nNext, formulate the corresponding prompts. \n\n![](https://pbs.twimg.com/media/Fydn3BAWAAEGEyC.jpg)\n\nNext, allocate the templates to the prompt information.\n\nThe length of this list can be as extensive as you want.\n\nEnsure that you make each prompt's description lucid for the large language model. \n\n![](https://pbs.twimg.com/media/FydoBNiWcAEhufg.jpg)\n\nVoila, your experts are ready.\n\nKick-start the large language model you prefer, configure the @LangChainAI multi prompt chain and initiate the chain.\n\nDo share what you'll be creating with this! \n\n![](https://pbs.twimg.com/media/FydomHFWYAAe83a.jpg)\n",
        "tags": [],
        "created": "2023-06-13T03:21:06.432Z",
        "updated": "2023-06-13T03:21:06.432Z"
    },
    {
        "id": "H2FfCczuORFB6KWZLIbz",
        "title": "Learn How to Build LLM-Powered Apps",
        "markdown": "## Learn How to Build LLM-Powered Apps\n\n[https://twitter.com/capetorch/status/1667321914535333889](https://twitter.com/capetorch/status/1667321914535333889)\n\n**ü™ÑSmart summary:**\nThis thread is about a course that teaches how to build LLM-powered Apps. It covers topics such as how to chat with data, ingest documents, use LangChainAI, and expand chatGPT context window.\n-------\n\nIf you want to start building LLM-powered Apps this is the course for you:\n- How do I chat with my data?\n- How can I ingest my documents and store them on a vector database?\n- What is @LangChainAI and how can I use it?\n- How can I expand chatGPT context window?\n- Learn how we‚Ä¶ [https://t.co/9xGHv6o6PA[https://t.co/BgUkdUtRg9](https://t.co/BgUkdUtRg9)](https://t.co/9xGHv6o6PA)\n\n@capetorch @LangChainAI\n",
        "tags": [],
        "created": "2023-06-11T05:53:05.425Z",
        "updated": "2023-06-11T05:53:05.425Z"
    },
    {
        "id": "RKBVSitIei0LXLJk48Ed",
        "title": "Copilot Enhances Coding Efficiency by Reducing Double-Checking and Editing Time",
        "markdown": "## Copilot Enhances Coding Efficiency by Reducing Double-Checking and Editing Time\n\n[https://twitter.com/DynamicWebPaige/status/1667751396110909440](https://twitter.com/DynamicWebPaige/status/1667751396110909440)\n\n**ü™ÑSmart summary:**\nOur studies showed that when using Copilot to solve coding tasks, programmers spend 34.3% of their time double-checking and editing suggestions, and more than half of their time on Copilot related activities.\n-------\n\n\"Our studies revealed that when solving a coding task with Copilot, programmers may spend a large fraction of total session time (34.3%) on just double-checking and editing suggestions, and spend *more than half* of the task time on Copilot related activities, together indicating‚Ä¶ [https://t.co/C9BoOakPWo](https://t.co/C9BoOakPWo)\n\n![](https://pbs.twimg.com/media/FyULB_HakAE6j9m.jpg)[https://t.co/qQ0W5x7ZtY](https://t.co/qQ0W5x7ZtY)\n\ncc: @RandomlyWalking @dtarlow2 @jacobaustin132\n",
        "tags": [],
        "created": "2023-06-11T05:49:04.243Z",
        "updated": "2023-06-11T05:49:04.243Z"
    },
    {
        "id": "HSFxo2TTvRpVmaeBasVw",
        "title": "Unlocking the Potential of AI Solutions with LLMs and Embeddings",
        "markdown": "## Unlocking the Potential of AI Solutions with LLMs and Embeddings\n\n[https://twitter.com/Lukaesch/status/1666868044184813582](https://twitter.com/Lukaesch/status/1666868044184813582)\n\n**ü™ÑSmart summary:**\nThis thread provides resources for those building AI solutions powered by language models and embeddings. It includes links to a page with gold information and a website to discover, buy, or sell AI embeddings.\n-------\n\nThis page is gold for everyone building AI solutions powered by LLMs and embeddings.\n\n[https://t.co/FgIhDRz49n](https://t.co/FgIhDRz49n)\n\nThanks @LangChainAI\n\nDo you want to discover, buy or sell AI embeddings?\n\nCheck this out: [https://t.co/umUGBlWjEL](https://t.co/umUGBlWjEL)\n",
        "tags": [],
        "created": "2023-06-09T18:38:05.411Z",
        "updated": "2023-06-09T18:38:05.411Z"
    },
    {
        "id": "idejvXJ4ncov3zjDu8Pu",
        "title": "Creating a Blog Outline Generator App with LangChain and Streamlit in 25 Lines of Code",
        "markdown": "## Creating a Blog Outline Generator App with LangChain and Streamlit in 25 Lines of Code\n\n[https://twitter.com/LangChainAI/status/1666837740497797124](https://twitter.com/LangChainAI/status/1666837740497797124)\n\n**ü™ÑSmart summary:**\nThis tweet is promoting a tutorial on how to build a blog outline generator app in 25 lines of code. It was created by @thedataprof.\n-------\n\nAnother excellent LangChain x Streamlit tutorial:\n\nBuild a blog outline generator app in 25 lines of code\n\n[https://t.co/SYPsbqqnnD](https://t.co/SYPsbqqnnD)\n\nThanks @thedataprof for creating!\n\n@LangChainAI @thedataprof\n",
        "tags": [],
        "created": "2023-06-08T17:15:04.447Z",
        "updated": "2023-06-08T17:15:04.447Z"
    },
    {
        "id": "sWE4yTnjkP2vSVpN6YtF",
        "title": "Train a Chatbot on YouTube - Now Open Source and Live on GitHub!",
        "markdown": "## Train a Chatbot on YouTube - Now Open Source and Live on GitHub!\n\n[https://twitter.com/ehalm_/status/1666783939451600898](https://twitter.com/ehalm_/status/1666783939451600898)\n\n**ü™ÑSmart summary:**\nA new open source product, YouTube-to-Chatbot, is now available on GitHub. It allows users to train a chatbot on an entire YouTube channel. The creator was surprised by the amount of interest in the product and is now doubling down on building a no-code product. Follow @ehalm_ for more announcements and thanks to @SeanGrindal and @eniascailliau for their help.\n-------\n\nThe wait is over... Train a chatbot on an entire YouTube channel. üé•ü§ùü§ñ\n\nYouTube-to-Chatbot is now open source and LIVE on GitHub üëá \n\n![](https://pbs.twimg.com/media/FyGaTBLWIAAfSwV.jpg)\n\nIf you missed the original Tweet, here‚Äôs how the story starts: [https://t.co/FverV92Znu](https://t.co/FverV92Znu)\n\nI was blown out of the water by how much creators wanted this ü§Ø \n\nGot DMs from some of my fav creators. W.\n\nSo I decided to double down & keep building a no-code product: [https://t.co/ThBoKhz5kv](https://t.co/ThBoKhz5kv)\n\nMore announcements soon. Time to get back to building.\n\nJust know that something awesome is coming üëÄ \n\nFollow @ehalm_ so you don‚Äôt miss it ‚úåüèº\n\nThanks to @SeanGrindal for late nights looking through my code üôè\n\nBig shout out to @eniascailliau for helping me test & debug ‚Äî check out the incredible work he‚Äôs been doing with AI agents and girlfriendGPT üôå\n\n[https://t.co/IloVZXfcAu](https://t.co/IloVZXfcAu)\n",
        "tags": [],
        "created": "2023-06-08T15:49:06.035Z",
        "updated": "2023-06-08T15:49:06.035Z"
    },
    {
        "id": "yRNhjA5o9DqmDMpyqld7",
        "title": "Chat with Your Brain on Your Phone: Quivr's Mobile App",
        "markdown": "## Chat with Your Brain on Your Phone: Quivr's Mobile App\n\n[https://twitter.com/_StanGirard/status/1666810871795052544](https://twitter.com/_StanGirard/status/1666810871795052544)\n\n**ü™ÑSmart summary:**\nQuivr Brain is a new open-source project that allows users to upload PDFs, CSVs, and other documents from their smartphones. It is powered by Supabase, LangChainAI, and other genius developers. The code for Quivr's mobile app and source code can be found in the links provided.\n-------\n\nWhat if you could chat with your brain on your phone? üß†üì±\n\nYou can now upload PDFs, CSVs, etc ... from your smartphone ü§Ø\n\nThanks @iMADi69235681, one of the genius behind the front of @quivr_brain \n\nOpen-source projects are the best! Powered by @supabase , @LangChainAI \n\nüîóüßµ [https://t.co/Ub5Ir4wEQI](https://t.co/Ub5Ir4wEQI)\n\nYou can find the code for Quivr's mobile app -> \n[https://t.co/xl8kGldzoK](https://t.co/xl8kGldzoK)\n\nAnd for @quivr_brain source code, here \n[https://t.co/XHpLfYNRMF](https://t.co/XHpLfYNRMF)\n",
        "tags": [],
        "created": "2023-06-08T15:48:03.892Z",
        "updated": "2023-06-08T15:48:03.892Z"
    },
    {
        "id": "vS7H1lvgESWf9LHnfuYy",
        "title": "Introducing AnythingLLM: An Open-Source App for Chatting with Documents",
        "markdown": "## Introducing AnythingLLM: An Open-Source App for Chatting with Documents\n\n[https://twitter.com/tcarambat/status/1666545363761922050](https://twitter.com/tcarambat/status/1666545363761922050)\n\n**ü™ÑSmart summary:**\nAnythingLLM is an open-source full-stack app for chatting with documents. It has no massive RAM requirements, no LLM downloading or training, and no need to pay to re-embed documents. It has a data tool suite, efficient vector-caching, persistent and useable UI, and allows users to bring their own OpenAI API key and Pinecone instance. It is MIT Licensed.\n-------\n\nAnnouncing AnythingLLM. An open-source (MIT) full-stack app for chatting with... anything.\n\n‚úÖ UI for managing documents\n‚úÖ Uses @OpenAI , @pinecone & @LangChainAI \n\nüö´ massive RAM requirements\nüö´ LLM downloading or training\nüö´ paying to re-embed documents\n\n+ data tooling! [https://t.co/AAwdA4vAVa](https://t.co/AAwdA4vAVa)\n\n![](https://pbs.twimg.com/media/FyC-HXDaQAAFkkJ.jpg)\n\nFirst, you can find AnythingLLM on Github here \n[https://t.co/xnK8AMQEuy](https://t.co/xnK8AMQEuy)\n\nRequirements:\n- Python 3.8+\n- npm and Node v16+\n\nThats it. No bulky CPU or GPU is required.\n\nChatting with your documents is the \"hello world\" of LLM use-cases, why not make it more accessible?\n\nWhat is so special about AnythingLLM?\n\nüî• No crazy system requirements - runs fast and passively on your machine\n\nüß∞ Full data-collection tool suite. Collect anything\nüëâ Entire YouTube Channels\nüëâ Substacks\nüëâ Mediums\nüëâ Gitbooks\n+ local document processing \n\n![](https://pbs.twimg.com/media/FyC_J4RaUAA8vgu.jpg)\n\n‚ö° Efficient vector-caching\n\nEmbed forever, pay once.\n\nEmbedding is low-cost using OpenAIs ada text embedding, but over thousands of tokens - why pay more than once?\n\nAnythingLLMs vector caching allows you to use the same document across workspaces and pay once. \n\n![](https://pbs.twimg.com/media/FyC__hcaEAAhhhV.jpg)\n\nüíé Persistent and Useable\nChatting and managing documents is all done from a web-browser UI.\n\nShut down the app and start it later - all your documents, chats, and more are still present. Picking up right where you left off.\n\nThe database is locally saved on your machine. \n\n![](https://pbs.twimg.com/media/FyDAXI7aYAAkyDG.jpg)\n\nüîë Bring-Your-Own-Keys\n\nUse your own OpenAI API key and Pinecone (free) instance and you are already on the way to managing documents with ease.\n\nThis keeps overhead low by just using what is existing, easy and accessible. \n\n![](https://pbs.twimg.com/media/FyDA2D1aEAEyO_f.jpg)\n\nAnythingLLM is MIT Licensed so go crazy. Read more about AnythingLLM on Medium at:\n[https://t.co/EDIUvZkmCx](https://t.co/EDIUvZkmCx)\n",
        "tags": [],
        "created": "2023-06-08T03:53:08.997Z",
        "updated": "2023-06-08T03:53:08.997Z"
    },
    {
        "id": "G3AfOgXnT6vB75qW1Fam",
        "title": "Developing a Frontend for Documentation Chat",
        "markdown": "## Developing a Frontend for Documentation Chat\n\n[https://twitter.com/MarkusOdenthal/status/1666435090832916481](https://twitter.com/MarkusOdenthal/status/1666435090832916481)\n\n**ü™ÑSmart summary:**\nToday, I developed a frontend for my Documentation Chat program using OpenAI for embeddings and chat functionality, LangChainAI for workflows, qdrant_engine for Vectorstore, and streamlit for the user interface. I had a successful chat session with the qdrant documentation.\n-------\n\nToday, I developed a frontend for my Documentation Chat program.\n\nTech stack:\n\n@OpenAI for embeddings and chat functionality\n@LangChainAI for workflows\n@qdrant_engine for Vectorstore\n@streamlit for the user interface\n\nI had a chat session with the qdrant documentationüí¨ [https://t.co/StrZQYTPrd](https://t.co/StrZQYTPrd)\n\n@MarkusOdenthal @OpenAI @LangChainAI @qdrant_engine @streamlit\n",
        "tags": [],
        "created": "2023-06-07T15:40:05.487Z",
        "updated": "2023-06-07T15:40:05.487Z"
    },
    {
        "id": "7RDiYH3lAFXY8wJXjLOj",
        "title": "Quivr Answers Questions with the Help of LangChainAI and Gitloader",
        "markdown": "## Quivr Answers Questions with the Help of LangChainAI and Gitloader\n\n[https://twitter.com/_StanGirard/status/1665849750166417408](https://twitter.com/_StanGirard/status/1665849750166417408)\n\n**ü™ÑSmart summary:**\nQuivr now has the ability to answer questions about itself, thanks to LangChainAI and their Gitloader. Quivr can now consume Github repos.\n-------\n\nQuivr can now answer questions on itself üî•\n\nHow ?! \n\nThanks to @LangChainAI and their Gitloader @Quivr_app can now consume Github repos ü§§ \n\n![](https://pbs.twimg.com/media/Fx5JOt7XsAI6oOk.jpg)\n\n@_StanGirard @LangChainAI @Quivr_app\n",
        "tags": [],
        "created": "2023-06-06T15:51:07.779Z",
        "updated": "2023-06-06T15:51:07.779Z"
    },
    {
        "id": "YtU4s0AXU3FRMqtDCiq2",
        "title": "Building the Twitter GPT with Yann Lecun",
        "markdown": "## Building the Twitter GPT with Yann Lecun\n\n[https://twitter.com/acohendev/status/1665852268304822273](https://twitter.com/acohendev/status/1665852268304822273)\n\n**ü™ÑSmart summary:**\nThis thread explains how to build a Twitter GPT using LangChainAI. It demonstrates the process by using Yann Lecun's handle and scraping it with a link. The result is stored in a vector database and then searched using similarity metrics. This data is then used by ChatGPT to answer the original question.\n-------\n\nHow to build the Twitter GPT using @LangChainAI?\n\nLet's try with @ylecun üá´üá∑\n\nRESULTS: \n\n- Q: What does Yann Lecun like to talk about?\n\n- A: Yann Lecun likes to talk about autoregressive transformer models and advises taking more math and quantum physics courses. \n\n![](https://pbs.twimg.com/media/Fx5LTJaWwAMyXBL.jpg)\n\nWe scrape Yann Lecun's handle using [https://t.co/NQnL1HeaZC.](https://t.co/NQnL1HeaZC.)\nThe result is then stored in a vector database indexed by embedding vectors created through the OpenAI API.\n\nWe can then search that database using similarity metrics and provide the resulting data to ChatGPT for it to extract the relevant information answering the original question.\n",
        "tags": [],
        "created": "2023-06-06T12:36:04.640Z",
        "updated": "2023-06-06T12:36:04.640Z"
    },
    {
        "id": "HAYBaptrHDoAMlLmry4G",
        "title": "Replit Code Instruct v2 Now Available on HuggingFace",
        "markdown": "## Replit Code Instruct v2 Now Available on HuggingFace\n\n[https://twitter.com/Teknium1/status/1665175977251708928](https://twitter.com/Teknium1/status/1665175977251708928)\n\n**ü™ÑSmart summary:**\nReplit Code Instruct v2 is now available on HuggingFace, with a training length of 2000 tokens, giving it greater access to dataset knowledge. There is also a HuggingFace spaces demo of the new model, and some output examples include creating code from an instruction, converting code to other languages, and explaining code.\n-------\n\nReplit Code Instruct v2 is now available on HuggingFace. \n\nThe original fine tune had only been trained on 512 token lengths, this one on 2000, giving it much greater access to dataset knowledge!\n\n[https://t.co/lXWAXqzhBf](https://t.co/lXWAXqzhBf)\n\nI have also created a HuggingFace spaces demo of the new model, try it out here: \n\n[https://t.co/o3yeaoFM7Q](https://t.co/o3yeaoFM7Q)\n\nSome output examples from this model - Create code from an instruction, convert code to other languages, even explain code you ask it to! \n\n![](https://pbs.twimg.com/media/FxvpOTnaYAETKa2.jpg)\n",
        "tags": [],
        "created": "2023-06-04T03:47:04.780Z",
        "updated": "2023-06-04T03:47:04.780Z"
    },
    {
        "id": "RDEAnSZA36JxFpooK9SX",
        "title": "Introducing CodeTF: A One-Stop Transformer Library for Code Large Language Models",
        "markdown": "## Introducing CodeTF: A One-Stop Transformer Library for Code Large Language Models\n\n[https://twitter.com/stevenhoi/status/1664483010954272770](https://twitter.com/stevenhoi/status/1664483010954272770)\n\n**ü™ÑSmart summary:**\nCodeTF is a one-stop Transformer Library for Code Large Language Models (CodeLLM) with a unified interface for training and inference on code tasks such as code generation, summarization, and translation. It is designed with key principles to provide a user-friendly and easy-to-use platform for code intelligence tasks, and follows a modular architecture to enhance its extensibility. It was developed by the AI Research team at SFResearch, consisting of Nghi, LHung1610, ayueei, LiJunnan0409\n-------\n\nüì¢Introducing üî•üî•, a one-stop Transformer Library for Code Large Language Models (CodeLLM), with a unified interface for training & inference on code tasks (code generation,summarization,translation,etc)\n\nPaper: [https://t.co/TmTrsOz94a](https://t.co/TmTrsOz94a)\nCode: [https://t.co/GmEN2rOYPE](https://t.co/GmEN2rOYPE)\n\n(1/n)\n\nCodeTF library supports both the development and deployment of Code LLMs for code intelligence tasks. The library can support both training and serving code LLM models, code utilities to process code data, and popular research benchmarks to evaluate the model performance. \n(2/n) \n\n![](https://pbs.twimg.com/media/FxlvgoKaQAA8xA4.jpg)\n\nCodeTF is designed with key principles to provide a user-friendly and easy-to-use platform for code intelligence tasks. It follows a modular architecture, enhancing its extensibility by allowing seamless integration of additional programming languages, models & utilities.\n\n(3/n) \n\n![](https://pbs.twimg.com/media/FxlyKhLaEAAJN9x.jpg)\n\nFind out more details from our technical report here: \n\nCodeTF: One-stop Transformer Library for State-of-the-art Code LLM\n\nAnother great open-source library with our amazing AI Research team: Nghi, @LHung1610 @ayueei @LiJunnan0409 @AkhileshGotmare at @SFResearch. \n\n(4/n)\n\n[https://t.co/TmTrsOz94a](https://t.co/TmTrsOz94a) (5/n)\n",
        "tags": [
            "CodeTF"
        ],
        "created": "2023-06-02T22:17:07.522Z",
        "updated": "2023-06-02T22:17:07.522Z"
    },
    {
        "id": "RZHhTJV8VUFbHJud24DU",
        "title": "Using JSONLoader and JSON Agent with LangChainAI",
        "markdown": "## Using JSONLoader and JSON Agent with LangChainAI\n\n[https://twitter.com/mesudarshan/status/1664664132174376962](https://twitter.com/mesudarshan/status/1664664132174376962)\n\n**ü™ÑSmart summary:**\nLangChainAI has released a video demonstrating how to use JSONLoader and JSON Agent to parse a JSON file and interact with a large API spec yaml file. The code and video are available on Github and Youtube respectively.\n-------\n\nJSONLoader and JSON Agent with @LangChainAI \nüé•Video covers the following:\n\n- JSONLoader to parse the JSON file\n- JSON Agent to interact with large API spec yaml file\n\nüíª Github code: [https://t.co/gL3fLstG01](https://t.co/gL3fLstG01)\n\nüçø Youtube video: [https://t.co/b7UDOgBgoN](https://t.co/b7UDOgBgoN)\n\n   \n\n@mesudarshan @LangChainAI\n",
        "tags": [
            "langchain",
            "json",
            "llm",
            "yaml"
        ],
        "created": "2023-06-02T21:49:03.980Z",
        "updated": "2023-06-02T21:49:03.980Z"
    },
    {
        "id": "mcW8lP40XAcZKilPWZGf",
        "title": "Unlocking the Benefits of Caching LLM Calls with LangChain",
        "markdown": "## Unlocking the Benefits of Caching LLM Calls with LangChain\n\n[https://twitter.com/mesudarshan/status/1664657092140060672](https://twitter.com/mesudarshan/status/1664657092140060672)\n\n**ü™ÑSmart summary:**\nCaching LLM calls can help reduce both cost and response time. LangChainAI has uploaded a video about this topic, and it can be found at the link provided.\n-------\n\nCaching LLM calls can reduce cost as well as response time. Just uploaded a video about it using @LangChainAI. Happy caching and chaining üîó\n\n#langchain\n #openai\n #llm\n #cache\n #python\n #gpt\n\n\nü¶úüîó LangChain | How To Cache LLM Calls ? [https://t.co/6b6zJVHLns](https://t.co/6b6zJVHLns) via @YouTube\n\n@mesudarshan @LangChainAI @YouTube\n",
        "tags": [
            "openai",
            "cache",
            "python",
            "langchain",
            "gpt",
            "llm"
        ],
        "created": "2023-06-02T19:16:06.474Z",
        "updated": "2023-06-02T19:16:06.474Z"
    },
    {
        "id": "o1M2jTU7XR4tkisDUzto",
        "title": "Improving ChatGPT Response Accuracy with Open-Source Prompting Strategy",
        "markdown": "## Improving ChatGPT Response Accuracy with Open-Source Prompting Strategy\n\n[https://twitter.com/krrish_dh/status/1664471146132283393](https://twitter.com/krrish_dh/status/1664471146132283393)\n\n**ü™ÑSmart summary:**\nWe have improved the accuracy of our chatGPT response by 35%, and are now open-sourcing it for everyone else. If you would like to use this strategy at your company, please contact us.\n-------\n\nWe improved our chatGPT response accuracy by 35% with this prompting strategy.\n\nSo we're open-sourcing it for everybody else. \n\nüö® Want to do this @ your co. üëâ let's chat: [https://t.co/ZYmk95n7Zx](https://t.co/ZYmk95n7Zx)\n\n[https://t.co/9pps4Vo3Fo](https://t.co/9pps4Vo3Fo)\n\n@krrish_dh\n",
        "tags": [],
        "created": "2023-06-02T19:10:04.867Z",
        "updated": "2023-06-02T19:10:04.867Z"
    },
    {
        "id": "mByh5js6OkIlJIHBBqO2",
        "title": "The Complete Guide to Fine-Tuning: Tips, Steps, and Use-Cases",
        "markdown": "## The Complete Guide to Fine-Tuning: Tips, Steps, and Use-Cases\n\n[https://twitter.com/pwang_szn/status/1664210285652221952](https://twitter.com/pwang_szn/status/1664210285652221952)\n\n**ü™ÑSmart summary:**\nThis thread provides an overview of fine-tuning, a process used to customize a language model. It explains what fine-tuning is, how to do it, when to use it, and provides tips for improving performance. It also provides use cases for fine-tuning, such as sentiment analysis, text generation, and fake news detection.\n-------\n\n\"The Complete Bozos Guide to Fine-tuning\"\n\n‚Ä¢ What is Fine-tuning?\n‚Ä¢ How to Fine-tune a custom LLM.\n‚Ä¢ When should we use fine-tuning?\n‚Ä¢ When NOT use use fine-tuning?\n\nSave This ‚Üì \n\n![](https://pbs.twimg.com/media/Fxh2btVagAEMtnz.png)\n\nBasic info on Fine-Tuning: \n\n![](https://pbs.twimg.com/media/Fxh2cIXaEAIvLcL.jpg)\n\nFine-tuning vs Semantic Search:\n\nTLDR: \"Fine tuning encourages pattern.\nEmbeddings provide knowledge.\" \n\n![](https://pbs.twimg.com/media/Fxh2ckXagAECIxO.jpg)\n\nWhat are the steps for fine-tuning? ü§î\n\n1. Prepare and upload training data\n2. Train a new fine-tuned model\n3. Use your fine-tuned model\n\nLet's go over all the steps:\n\nIn order to train the model, we need to structure the data must be a JSONL document(a list of prompt-completion pairs.): \n\n![](https://pbs.twimg.com/media/Fxh2dLFaEAAnwU9.jpg)\n\nTips for Improve Fine-Tuning Performance:\n\n1) Provide at least a few hundred high-quality examples (vetted by human.)\n\n2) Increasing the number of examples is usually the best and most reliable way of improving performance.\n\nHere's the command to fine-tune your data: \n\n![](https://pbs.twimg.com/media/Fxh2d1VaMAE2nrb.jpg)\n\nAfter you train the model, there's two ways to use the new model:\n\n1) Command Line\n2) Code \n\n![](https://pbs.twimg.com/media/Fxh2eSraAAArhAs.jpg)\n\nYou can also use the fine-tuned models in on the OpenAI playground directly. \n\n![](https://pbs.twimg.com/media/Fxh2ewyaAAA1aYA.png)\n\nP.S - ‚ú® I'm dropping a FREE step-by-step mini-course guide to start coding with A.I\n\n(Free for now, but not free forever)\n\nCheck it out: [https://t.co/pmDsYC74tK](https://t.co/pmDsYC74tK)\n\nUse-cases for fine-tuning:\n\n1) Sentiment Analysis\n2) Text Generation: chatbot that responds in a specific tone or style based company's guidelines.\n3) Fake News Detection: Models can be fine-tuned to detect fake news or misinformation by training them dataset of fake news.\n",
        "tags": [],
        "created": "2023-06-02T01:53:06.819Z",
        "updated": "2023-06-02T01:53:06.819Z"
    },
    {
        "id": "G2HygM0AkNyZ9R1TQKNa",
        "title": "https://github.com/kyrolabs/awesome-langchain#other--chatbots",
        "markdown": "[https://github.com/kyrolabs/awesome-langchain#other--chatbots](https://github.com/kyrolabs/awesome-langchain#other--chatbots)\n\nLangChain is an amazing framework to get LLM projects done in a matter of no time and the ecosystem is growing fast. Here is an attempt to keep track of the initiatives around LangChain.\nContributions welcome. Add links through pull requests or create an issue to start a discussion. Please read the¬†[contribution guidelines](https://github.com/kyrolabs/awesome-langchain/blob/main/CONTRIBUTING.md)¬†before contributing.\n## \n## Table of Contents\n- [ü¶úüîó¬†Awesome LangChain[Table of Contents](https://github.com/kyrolabs/awesome-langchain#table-of-contents)](https://github.com/kyrolabs/awesome-langchain#-awesome-langchain--)\n- [LangChain Framework](https://github.com/kyrolabs/awesome-langchain#langchain-framework)\n- [Tools[Low-code](https://github.com/kyrolabs/awesome-langchain#low-code)](https://github.com/kyrolabs/awesome-langchain#tools)\n- [Services](https://github.com/kyrolabs/awesome-langchain#services)\n- [Agents](https://github.com/kyrolabs/awesome-langchain#agents)\n- [Templates](https://github.com/kyrolabs/awesome-langchain#templates)\n- [Open Source Projects[Knowledge Management](https://github.com/kyrolabs/awesome-langchain#knowledge-management)](https://github.com/kyrolabs/awesome-langchain#open-source-projects)\n- [Other / Chatbots](https://github.com/kyrolabs/awesome-langchain#other--chatbots)\n- [Learn[Notebooks](https://github.com/kyrolabs/awesome-langchain#notebooks)](https://github.com/kyrolabs/awesome-langchain#learn)\n- [Videos](https://github.com/kyrolabs/awesome-langchain#videos)\n- [Articles](https://github.com/kyrolabs/awesome-langchain#articles)\n- [Alternatives](https://github.com/kyrolabs/awesome-langchain#alternatives)\n- [Complement to this list](https://github.com/kyrolabs/awesome-langchain#complement-to-this-list)\n\n## \n## LangChain Framework\n- [LangChain](https://github.com/hwchase17/langchain): the original¬†üêç¬†[![](https://camo.githubusercontent.com/63354a25bfac04c720aa99292f657b9d59740705fb29d3c71726106f5a017590/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/63354a25bfac04c720aa99292f657b9d59740705fb29d3c71726106f5a017590/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e3f7374796c653d736f6369616c)\n- [LangChain.js](https://github.com/hwchase17/langchainjs): the js brother¬†‚ú®¬†[![](https://camo.githubusercontent.com/39c7a3539f1e5abfc015a00918b68cbb4cb93e47a86d8a58a0cda6ba3ee3c01a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e6a733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/39c7a3539f1e5abfc015a00918b68cbb4cb93e47a86d8a58a0cda6ba3ee3c01a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e6a733f7374796c653d736f6369616c)\n- [Concepts](https://docs.langchain.com/docs/): Langchain concepts doc\n- [Twitter account](https://twitter.com/LangChainAI): follow to get fresh updates\n- [Youtube Channel](https://www.youtube.com/channel/UCC-lyoTfSrcJzA1ab3APAgw)\n- [Discord](https://discord.gg/6adMQxSpJS): discussion\n- [Langchain Blog](https://blog.langchain.dev/): The Official Langchain blog\n- [LangChainHub](https://github.com/hwchase17/langchain-hub): collection of all artifacts useful for working with LangChain primitives such as prompts, chains and agents¬†[![](https://camo.githubusercontent.com/c6a192e23f03b6ecb26489ba1e6d1b4f5776810f541c1ed59777c3422b326e57/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d6875623f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c6a192e23f03b6ecb26489ba1e6d1b4f5776810f541c1ed59777c3422b326e57/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d6875623f7374796c653d736f6369616c)\n\n## \n## Tools\n### \n### Low-code\n- [Langflow](https://github.com/logspace-ai/langflow): LangFlow is a UI for LangChain¬†[![](https://camo.githubusercontent.com/196fc1df5f5e5d6b1c0d147c121168693afe44bbadc87cc3f443efd46b1ec74f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f6773706163652d61692f6c616e67666c6f773f7374796c653d736f6369616c)](https://camo.githubusercontent.com/196fc1df5f5e5d6b1c0d147c121168693afe44bbadc87cc3f443efd46b1ec74f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f6773706163652d61692f6c616e67666c6f773f7374796c653d736f6369616c)\n- [Flowise - LangchainJS UI](https://github.com/FlowiseAI/Flowise): Drag & drop UI to build your customized LLM flow using LangchainJS¬†[![](https://camo.githubusercontent.com/1ba1ab2cc7be5ca0a17eadbc4cff268741a7d0c606a6d33ac90077b54c903ea9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f466c6f7769736541492f466c6f776973653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/1ba1ab2cc7be5ca0a17eadbc4cff268741a7d0c606a6d33ac90077b54c903ea9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f466c6f7769736541492f466c6f776973653f7374796c653d736f6369616c)\n- [Databerry](https://github.com/gmpetrov/databerry): The no-code platform for semantic search and documents retrieval¬†[![](https://camo.githubusercontent.com/a693798f36ebeeeaec4bd77303cf15a4b567976c043ce14c4a8bf5264d996603/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676d706574726f762f6461746162657272793f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a693798f36ebeeeaec4bd77303cf15a4b567976c043ce14c4a8bf5264d996603/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676d706574726f762f6461746162657272793f7374796c653d736f6369616c)\n- [LangchainUI](https://github.com/homanp/langchain-ui): The open source chat-ai toolkit¬†[![](https://camo.githubusercontent.com/e9852774df90de16175df3eddcb12ad2974b95d3dd89cf53a7415850f25d528e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6d616e702f6c616e67636861696e2d75693f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e9852774df90de16175df3eddcb12ad2974b95d3dd89cf53a7415850f25d528e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6d616e702f6c616e67636861696e2d75693f7374796c653d736f6369616c)\n- [Yeager.ai](https://github.com/yeagerai/yeagerai-agent): Yeager.ai Agent is the first Langchain Agent creator designed to help you build, prototype, and deploy AI-powered agents with ease¬†[![](https://camo.githubusercontent.com/08865c0e9b8bbf60e9bd2e40316cdb5444d53908047d4a0a40b92b839dad0635/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79656167657261692f79656167657261692d6167656e743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/08865c0e9b8bbf60e9bd2e40316cdb5444d53908047d4a0a40b92b839dad0635/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79656167657261692f79656167657261692d6167656e743f7374796c653d736f6369616c)\n\n### \n### Services\n- [GPTCache](https://github.com/zilliztech/GPTCache): A Library for Creating Semantic Cache for LLM Queries¬†[![](https://camo.githubusercontent.com/00b53511e42ff679259ffae7a508ddc2ccdd07f82c0f56adee16f17f4f6a5033/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a696c6c697a746563682f47505443616368653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/00b53511e42ff679259ffae7a508ddc2ccdd07f82c0f56adee16f17f4f6a5033/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a696c6c697a746563682f47505443616368653f7374796c653d736f6369616c)\n- [LlamaHub](https://github.com/emptycrown/llama-hub): a library of data loaders for LLMs made by the community¬†[![](https://camo.githubusercontent.com/a69d2a5e9cfae3676a6d3a0f9a1e4c850c0d3e7654dddf343c6e86280db279c0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f656d70747963726f776e2f6c6c616d612d6875623f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a69d2a5e9cfae3676a6d3a0f9a1e4c850c0d3e7654dddf343c6e86280db279c0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f656d70747963726f776e2f6c6c616d612d6875623f7374796c653d736f6369616c)\n- [EVAL](https://github.com/corca-ai/EVAL): Elastic Versatile Agent with Langchain. will execute all your requests.¬†[![](https://camo.githubusercontent.com/c46cfaedbf5b687d90775504cc5ff80c105f2772c4bc1e0c015d1261ce33c39a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f7263612d61692f4556414c3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c46cfaedbf5b687d90775504cc5ff80c105f2772c4bc1e0c015d1261ce33c39a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f7263612d61692f4556414c3f7374796c653d736f6369616c)\n- [Auto-evaluator](https://github.com/PineappleExpress808/auto-evaluator): a lightweight evaluation tool for question-answering using Langchain¬†[![](https://camo.githubusercontent.com/6e0c6def5e38e56bb156f78ef5354c3e21e2d39e2a7260a498f77d68892801d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f50696e656170706c65457870726573733830382f6175746f2d6576616c7561746f723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/6e0c6def5e38e56bb156f78ef5354c3e21e2d39e2a7260a498f77d68892801d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f50696e656170706c65457870726573733830382f6175746f2d6576616c7561746f723f7374796c653d736f6369616c)\n- [Langchain visualizer](https://github.com/amosjyng/langchain-visualizer): visualization and debugging tool for LangChain workflows¬†[![](https://camo.githubusercontent.com/518db549cc722ee463a35962545cd6abca0181c18c7d8561365be5437501a2d5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616d6f736a796e672f6c616e67636861696e2d76697375616c697a65723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/518db549cc722ee463a35962545cd6abca0181c18c7d8561365be5437501a2d5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616d6f736a796e672f6c616e67636861696e2d76697375616c697a65723f7374796c653d736f6369616c)\n- [LLM Strategy](https://github.com/BlackHC/llm-strategy): implementing the Strategy Pattern using LLMs¬†[![](https://camo.githubusercontent.com/656f14bc805aa1866b97d46d4a5e6905cdf082b73b47bde015bed68252fe814c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426c61636b48432f6c6c6d2d73747261746567793f7374796c653d736f6369616c)](https://camo.githubusercontent.com/656f14bc805aa1866b97d46d4a5e6905cdf082b73b47bde015bed68252fe814c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426c61636b48432f6c6c6d2d73747261746567793f7374796c653d736f6369616c)\n- [datasetGPT](https://github.com/radi-cho/datasetGPT): A command-line interface to generate textual and conversational datasets with LLMs.¬†[![](https://camo.githubusercontent.com/506478becd977c7ee2d6e514c051cbfd6c9bc8626dce5c793d72110d1463b218/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726164692d63686f2f646174617365744750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/506478becd977c7ee2d6e514c051cbfd6c9bc8626dce5c793d72110d1463b218/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726164692d63686f2f646174617365744750543f7374796c653d736f6369616c)\n- [spellbook-forge](https://github.com/rafalzawadzki/spellbook-forge): Make your LLM prompts executable and version controlled.¬†[![](https://camo.githubusercontent.com/7a927dd591070e945a19bf0f00f8cf72a05ef3612c4305ff7a4cafdd3a20e89b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726166616c7a617761647a6b692f7370656c6c626f6f6b2d666f7267653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/7a927dd591070e945a19bf0f00f8cf72a05ef3612c4305ff7a4cafdd3a20e89b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726166616c7a617761647a6b692f7370656c6c626f6f6b2d666f7267653f7374796c653d736f6369616c)\n- [Auto Evaluator](https://github.com/langchain-ai/auto-evaluator): Langchain auto evaluator¬†[![](https://camo.githubusercontent.com/53ccc7655a0834e0da4e1da9c6a181e8878df952a0a3cd9270ef3528e6022be3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6175746f2d6576616c7561746f723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/53ccc7655a0834e0da4e1da9c6a181e8878df952a0a3cd9270ef3528e6022be3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6175746f2d6576616c7561746f723f7374796c653d736f6369616c)\n- [Jina](https://github.com/jina-ai/langchain-serve): Langchain Apps on Production with Jina¬†[![](https://camo.githubusercontent.com/c3c5be6d138cb73f94b981c0c05c3adfb547c6a7baf552af9c5940ffd4267a54/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a696e612d61692f6c616e67636861696e2d73657276653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c3c5be6d138cb73f94b981c0c05c3adfb547c6a7baf552af9c5940ffd4267a54/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a696e612d61692f6c616e67636861696e2d73657276653f7374796c653d736f6369616c)\n- [Gradio Tools](https://github.com/freddyaboulton/gradio-tools): Gradio¬†ü§ù¬†LLM Agents¬†[![](https://camo.githubusercontent.com/4301edbd0b2ce3f8f7930eb0dd575a9524f9564b7ab25d9c9d498b8871c7674e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66726564647961626f756c746f6e2f67726164696f2d746f6f6c733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/4301edbd0b2ce3f8f7930eb0dd575a9524f9564b7ab25d9c9d498b8871c7674e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66726564647961626f756c746f6e2f67726164696f2d746f6f6c733f7374796c653d736f6369616c)\n- [steamship-langchain](https://github.com/steamship-core/steamship-langchain): adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship¬†üêç¬†[![](https://camo.githubusercontent.com/42b7d766231d87754ad003af81d64fc49f06b5248debb37abe2e911db05961ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f737465616d736869702d636f72652f737465616d736869702d6c616e67636861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/42b7d766231d87754ad003af81d64fc49f06b5248debb37abe2e911db05961ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f737465616d736869702d636f72652f737465616d736869702d6c616e67636861696e3f7374796c653d736f6369616c)\n- [LangForge](https://github.com/mme/langforge): A Toolkit for Creating and Deploying LangChain Apps¬†[![](https://camo.githubusercontent.com/89de1946e96db34b3f3d48a3cff44fd884fc4da67df7d7dfcc620453c4b7af1e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6d652f6c616e67666f7267653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/89de1946e96db34b3f3d48a3cff44fd884fc4da67df7d7dfcc620453c4b7af1e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6d652f6c616e67666f7267653f7374796c653d736f6369616c)\n- [BentoChain](https://github.com/ssheng/BentoChain): LangChain Deployment on BentoML¬†[![](https://camo.githubusercontent.com/b3b125636b316d83534302d480b260e243bd738b1fe9e720fa2fcc30057043f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f737368656e672f42656e746f436861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/b3b125636b316d83534302d480b260e243bd738b1fe9e720fa2fcc30057043f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f737368656e672f42656e746f436861696e3f7374796c653d736f6369616c)\n- [LangCorn](https://github.com/msoedov/langcorn): Serving LangChain apps automagically with FastApi¬†[![](https://camo.githubusercontent.com/6f0fffaa6dbf5f07643bb3372c070c0b89f3933060ea89802af1596af474b4be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d736f65646f762f6c616e67636f726e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/6f0fffaa6dbf5f07643bb3372c070c0b89f3933060ea89802af1596af474b4be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d736f65646f762f6c616e67636f726e3f7374796c653d736f6369616c)\n- [Langchain Service](https://github.com/kyrolabs/langchain-service): Opinionated Langchain setup with Qdrant vector store and Kong gateway¬†[![](https://camo.githubusercontent.com/b7105e814bf87a1a8af2e2c997102143c2defb761ebd8671682a3b4954cc4ed6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b79726f6c6162732f6c616e67636861696e2d736572766963653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/b7105e814bf87a1a8af2e2c997102143c2defb761ebd8671682a3b4954cc4ed6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b79726f6c6162732f6c616e67636861696e2d736572766963653f7374796c653d736f6369616c)\n- [Lanarky](https://github.com/ajndkr/lanarky):¬†üö¢¬†Ship production-ready LLM projects with FastAPI¬†[![](https://camo.githubusercontent.com/cfa3b6eefb6ee92bd41dbfb38927778de4adaf0f61db5883852e33100d41ad69/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616a6e646b722f6c616e61726b793f7374796c653d736f6369616c)](https://camo.githubusercontent.com/cfa3b6eefb6ee92bd41dbfb38927778de4adaf0f61db5883852e33100d41ad69/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616a6e646b722f6c616e61726b793f7374796c653d736f6369616c)\n- [Dify](https://github.com/langgenius/dify): One API for plugins and datasets, one interface for prompt engineering and visual operation, all for creating powerful AI applications.¬†[![](https://camo.githubusercontent.com/321751ad931de2f3e0cc451aa22d73ec924f9b73680a2adc489e0288fbcd6861/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e6767656e6975732f646966793f7374796c653d736f6369616c)](https://camo.githubusercontent.com/321751ad931de2f3e0cc451aa22d73ec924f9b73680a2adc489e0288fbcd6861/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e6767656e6975732f646966793f7374796c653d736f6369616c)\n- [LangchainJS Worker](https://github.com/rickyrobinett/langchainjs-workers): LangchainJS worker on cloudflare¬†[![](https://camo.githubusercontent.com/cff09d01c74012b3cbc370879faab58878969d24c82269219db49c089283333d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7269636b79726f62696e6574742f6c616e67636861696e6a732d776f726b6572733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/cff09d01c74012b3cbc370879faab58878969d24c82269219db49c089283333d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7269636b79726f62696e6574742f6c616e67636861696e6a732d776f726b6572733f7374796c653d736f6369616c)\n- [Chainlit](https://github.com/Chainlit/chainlit): Build Python LLM apps in minutes¬†‚ö°Ô∏è¬†[![](https://camo.githubusercontent.com/fe4ee7187e5201a75e40e63a87c5855ddb9313ccdd52ec4e062c7e0e7c2d73f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436861696e6c69742f636861696e6c69743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/fe4ee7187e5201a75e40e63a87c5855ddb9313ccdd52ec4e062c7e0e7c2d73f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436861696e6c69742f636861696e6c69743f7374796c653d736f6369616c)\n- [Zep](https://github.com/getzep/zep): Zep: A long-term memory store for LLM / Chatbot applications¬†[![](https://camo.githubusercontent.com/773f823c68d50bf34aacf9661ed451b89731456c32e5594ae74b56c312002894/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6765747a65702f7a65703f7374796c653d736f6369616c)](https://camo.githubusercontent.com/773f823c68d50bf34aacf9661ed451b89731456c32e5594ae74b56c312002894/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6765747a65702f7a65703f7374796c653d736f6369616c)\n- [Gorilla](https://github.com/ShishirPatil/gorilla): An API store for LLMs¬†[![](https://camo.githubusercontent.com/114728d53d657728c659e84ab512e3f7af9e4392dd83b9db1539c340cf776965/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53686973686972506174696c2f676f72696c6c613f7374796c653d736f6369616c)](https://camo.githubusercontent.com/114728d53d657728c659e84ab512e3f7af9e4392dd83b9db1539c340cf776965/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53686973686972506174696c2f676f72696c6c613f7374796c653d736f6369616c)\n\n### \n### Agents\n- [AgentGPT](https://github.com/reworkd/AgentGPT): AI Agents with Langchain & OpenAI (Vercel / Nextjs)¬†[![](https://camo.githubusercontent.com/0db278150651f631d291ca1ef7943b8a28343cb5d471ee6ff65fdefc8019a9a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7265776f726b642f4167656e744750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/0db278150651f631d291ca1ef7943b8a28343cb5d471ee6ff65fdefc8019a9a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7265776f726b642f4167656e744750543f7374796c653d736f6369616c)\n- [ThinkGPT](https://github.com/alaeddine-13/thinkgpt): Agent techniques to augment your LLM and push it beyond its limits¬†[![](https://camo.githubusercontent.com/b0b4ec2adfe1dff62cc9a965c17f540887b23cc21b06138e5f61f3d3964bd286/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c61656464696e652d31332f7468696e6b6770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/b0b4ec2adfe1dff62cc9a965c17f540887b23cc21b06138e5f61f3d3964bd286/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c61656464696e652d31332f7468696e6b6770743f7374796c653d736f6369616c)\n- [Camel-AutoGPT](https://github.com/SamurAIGPT/Camel-AutoGPT): role-playing approach for LLMs and auto-agents like BabyAGI & AutoGPT¬†[![](https://camo.githubusercontent.com/a17e9217d26674eed8f86b32e6737b09510de507eaa79efd8ab262154732b6a6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53616d757241494750542f43616d656c2d4175746f4750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a17e9217d26674eed8f86b32e6737b09510de507eaa79efd8ab262154732b6a6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53616d757241494750542f43616d656c2d4175746f4750543f7374796c653d736f6369616c)\n- [Private GPT](https://github.com/imartinez/privateGPT): Interact privately with your documents using the power of GPT, 100% privately, no data leaks¬†[![](https://camo.githubusercontent.com/ba10dae69316f5005522d167d2b602ba6c40b272a92735a72da8d0caa531e032/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696d617274696e657a2f707269766174654750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/ba10dae69316f5005522d167d2b602ba6c40b272a92735a72da8d0caa531e032/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696d617274696e657a2f707269766174654750543f7374796c653d736f6369616c)\n- [RasaGPT](https://github.com/paulpierre/RasaGPT): RasaGPT is the first headless LLM chatbot platform built on top of Rasa and Langchain.¬†[![](https://camo.githubusercontent.com/27b2aaab1dc52e23ab7f64190bbe99efb3b2ac11e10f369ea281933980fd3d55/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7061756c7069657272652f526173614750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/27b2aaab1dc52e23ab7f64190bbe99efb3b2ac11e10f369ea281933980fd3d55/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7061756c7069657272652f526173614750543f7374796c653d736f6369616c)\n- [SkyAGI](https://github.com/litanlitudan/skyagi): Emerging human-behavior simulation capability in LLM agents¬†[![](https://camo.githubusercontent.com/4546ad03ef3423599f31e2e32f3d2998c24974dd578ef51ad97b3abbb8a3dc50/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6974616e6c69747564616e2f736b796167693f7374796c653d736f6369616c)](https://camo.githubusercontent.com/4546ad03ef3423599f31e2e32f3d2998c24974dd578ef51ad97b3abbb8a3dc50/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6974616e6c69747564616e2f736b796167693f7374796c653d736f6369616c)\n- [PyCodeAGI](https://github.com/chakkaradeep/pyCodeAGI): A small AGI experiment to generate a Python app given what app the user wants to build¬†[![](https://camo.githubusercontent.com/91d14cb4a900173d6244817ec5c14419e08d01fcf823700d30cd2401b888f5b0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6368616b6b617261646565702f7079436f64654147493f7374796c653d736f6369616c)](https://camo.githubusercontent.com/91d14cb4a900173d6244817ec5c14419e08d01fcf823700d30cd2401b888f5b0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6368616b6b617261646565702f7079436f64654147493f7374796c653d736f6369616c)\n- [BabyAGI UI](https://github.com/miurla/babyagi-ui): Make it easier to run and develop with babyagi in a web app, like a ChatGPT¬†[![](https://camo.githubusercontent.com/a8f5e8865c07989547d2c47d284b7de1744f0fbfb3f5fe6303285374a766594f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6975726c612f626162796167692d75693f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a8f5e8865c07989547d2c47d284b7de1744f0fbfb3f5fe6303285374a766594f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6975726c612f626162796167692d75693f7374796c653d736f6369616c)\n- [CollosalAI Chat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): implement LLM with RLHF, powered by the Colossal-AI project¬†[![](https://camo.githubusercontent.com/a294d2683fcc9c5a7d3606036c8f40f711045255ade0d47000a607e620f8fd77/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6870636169746563682f436f6c6f7373616c41493f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a294d2683fcc9c5a7d3606036c8f40f711045255ade0d47000a607e620f8fd77/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6870636169746563682f436f6c6f7373616c41493f7374796c653d736f6369616c)\n- [SuperAgent](https://github.com/homanp/superagent): Deploy LLM Agents to production¬†[![](https://camo.githubusercontent.com/a7df1f9b460cdfb2185b935b4c0d2065b846a95518abc3d9b0204dda3f4224d0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6d616e702f73757065726167656e743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a7df1f9b460cdfb2185b935b4c0d2065b846a95518abc3d9b0204dda3f4224d0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6d616e702f73757065726167656e743f7374796c653d736f6369616c)\n- [ix](https://github.com/kreneskyp/ix): Autonomous GPT-4 agent platform\n- [DuetGPT](https://github.com/kristoferlund/duet-gpt): A conversational semi-autonomous developer assistant, AI pair programming without the copypasta.¬†[![](https://camo.githubusercontent.com/c641b4611f161e9c430cc9cd4ba42bbbc993db32cc9e4b8e39614d88ed8bb6d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b726973746f6665726c756e642f647565742d6770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c641b4611f161e9c430cc9cd4ba42bbbc993db32cc9e4b8e39614d88ed8bb6d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b726973746f6665726c756e642f647565742d6770743f7374796c653d736f6369616c)\n\n### \n### Templates\n- [create-t3-turbo-ai](https://github.com/zckly/create-t3-turbo-ai): t3 based, Langchain-friendly boilerplate for building type-safe, full-stack, LLM-powered web apps with Nextjs and Prisma¬†[![](https://camo.githubusercontent.com/be0fec1598a9e399614998e99614698be34e38cfa1e49119c7163bab0d086f0f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a636b6c792f6372656174652d74332d747572626f2d61693f7374796c653d736f6369616c)](https://camo.githubusercontent.com/be0fec1598a9e399614998e99614698be34e38cfa1e49119c7163bab0d086f0f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a636b6c792f6372656174652d74332d747572626f2d61693f7374796c653d736f6369616c)\n- [LangChain.js LLM Template](https://github.com/Conner1115/LangChain.js-LLM-Template): LangChain LLM template that allows you to train your own custom AI LLM model.¬†[![](https://camo.githubusercontent.com/8789af1935a695b9973b81795d513aa7cce5f3206c0c1fb92b0f8b7c372c69ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6e6e6572313131352f4c616e67436861696e2e6a732d4c4c4d2d54656d706c6174653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/8789af1935a695b9973b81795d513aa7cce5f3206c0c1fb92b0f8b7c372c69ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6e6e6572313131352f4c616e67436861696e2e6a732d4c4c4d2d54656d706c6174653f7374796c653d736f6369616c)\n- [Streamlit Template](https://github.com/hwchase17/langchain-streamlit-template): template for how to deploy a LangChain on Streamlit¬†[![](https://camo.githubusercontent.com/58c68659e9cbd2320ba8e95f39d107f198f9494df9b9a572c285a14945cd6158/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d73747265616d6c69742d74656d706c6174653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/58c68659e9cbd2320ba8e95f39d107f198f9494df9b9a572c285a14945cd6158/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d73747265616d6c69742d74656d706c6174653f7374796c653d736f6369616c)\n- [Codespaces Template](https://github.com/lostintangent/codespaces-langchain): a Codespaces template for getting up-and-running with LangChain in seconds!¬†[![](https://camo.githubusercontent.com/48c12a002fffa01390175ab943dcb85e3e242c2109dd4b2bf7f24d95517a5188/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f7374696e74616e67656e742f636f64657370616365732d6c616e67636861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/48c12a002fffa01390175ab943dcb85e3e242c2109dd4b2bf7f24d95517a5188/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f7374696e74616e67656e742f636f64657370616365732d6c616e67636861696e3f7374796c653d736f6369616c)\n- [Gradio Template](https://github.com/hwchase17/langchain-gradio-template): template for how to deploy a LangChain on Gradio¬†[![](https://camo.githubusercontent.com/ce54042bf524fdbef5961f2d72191386cbe10cdedaf15e4f77bfa8f2ea4ab59c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d67726164696f2d74656d706c6174653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/ce54042bf524fdbef5961f2d72191386cbe10cdedaf15e4f77bfa8f2ea4ab59c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6c616e67636861696e2d67726164696f2d74656d706c6174653f7374796c653d736f6369616c)\n\n### \n### Platforms\n- [Modal](https://modal.com/docs/guide/ex/potus_speech_qanda): End-to-end stack for cloud/ML compute\n- [Metal](https://getmetal.io/): Metal is a managed service that allows you to build AI products without the hassle of managing infrastructure\n- [Graphsignal](https://graphsignal.com/): Observability for AI agents and LLM-powered applications. Trace, monitor and debug LangChain in production.\n\n## \n## Open Source Projects\n### \n### Knowledge Management\n- [DocsGPT](https://github.com/arc53/docsgpt): GPT-powered chat for documentation search & assistance.¬†[![](https://camo.githubusercontent.com/a41725f3992f3cd17f51738c72075cfa9d555d35723491e12ad767284d94bb30/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61726335332f646f63736770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a41725f3992f3cd17f51738c72075cfa9d555d35723491e12ad767284d94bb30/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61726335332f646f63736770743f7374796c653d736f6369616c)\n- [Knowledge GPT](https://github.com/mmz-001/knowledge_gpt): Accurate answers and instant citations for your documents.¬†[![](https://camo.githubusercontent.com/a3ed3202ccc6ba7768991d4f8e978547650a294663e9b3238d0603c6f176446c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6d7a2d3030312f6b6e6f776c656467655f6770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a3ed3202ccc6ba7768991d4f8e978547650a294663e9b3238d0603c6f176446c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6d7a2d3030312f6b6e6f776c656467655f6770743f7374796c653d736f6369616c)\n- [Quiver](https://github.com/StanGirard/quiver): Dump your brain into your GenerativeAI Vault¬†[![](https://camo.githubusercontent.com/9bcf87647186c1535768ad6870eedb60510c95ee5788a8ba9bbe12e676912b32/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5374616e4769726172642f7175697665723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/9bcf87647186c1535768ad6870eedb60510c95ee5788a8ba9bbe12e676912b32/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5374616e4769726172642f7175697665723f7374796c653d736f6369616c)\n- [Knowledge](https://github.com/KnowledgeCanvas/knowledge): Knowledge is a tool for saving, searching, accessing, and exploring all of your favorite websites, documents and files.¬†[![](https://camo.githubusercontent.com/c8617c494b060b8796fd65df50225be793297c22b96f2849a9084ca6d040320b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b6e6f776c6564676543616e7661732f6b6e6f776c656467653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c8617c494b060b8796fd65df50225be793297c22b96f2849a9084ca6d040320b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b6e6f776c6564676543616e7661732f6b6e6f776c656467653f7374796c653d736f6369616c)\n\n### \n### Other / Chatbots\n- [AudioGPT](https://github.com/AIGC-Audio/AudioGPT): Understanding and Generating Speech, Music, Sound, and Talking Head¬†[![](https://camo.githubusercontent.com/5470b3c3dc42d53dc2278c61da746ca64a977dc3a07b176b0681a6332a69fc29/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f414947432d417564696f2f417564696f4750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/5470b3c3dc42d53dc2278c61da746ca64a977dc3a07b176b0681a6332a69fc29/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f414947432d417564696f2f417564696f4750543f7374796c653d736f6369616c)\n- [Paper QA](https://github.com/whitead/paper-qa): LLM Chain for answering questions from documents with citations¬†[![](https://camo.githubusercontent.com/dba68995d8c57a8f685b08c2a8c4597b37cdc2c18cbcabc861da7c233bb0c759/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f776869746561642f70617065722d71613f7374796c653d736f6369616c)](https://camo.githubusercontent.com/dba68995d8c57a8f685b08c2a8c4597b37cdc2c18cbcabc861da7c233bb0c759/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f776869746561642f70617065722d71613f7374796c653d736f6369616c)\n- [Chat Langchain](https://github.com/hwchase17/chat-langchain): locally hosted chatbot specifically focused on question answering over the LangChain documentation¬†[![](https://camo.githubusercontent.com/c59033679486f2a986cbce41c685156cf24dbbacf54a121865ec45001a37369b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f636861742d6c616e67636861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c59033679486f2a986cbce41c685156cf24dbbacf54a121865ec45001a37369b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f636861742d6c616e67636861696e3f7374796c653d736f6369616c)\n- [Langchain Chat](https://github.com/zahidkhawaja/langchain-chat-nextjs): another Next.js frontend for LangChain Chat.¬†[![](https://camo.githubusercontent.com/041179065940c15acf156b3cea8dbc29abb5d68315bfd081ea81a66da9469950/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a616869646b686177616a612f6c616e67636861696e2d636861742d6e6578746a733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/041179065940c15acf156b3cea8dbc29abb5d68315bfd081ea81a66da9469950/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a616869646b686177616a612f6c616e67636861696e2d636861742d6e6578746a733f7374796c653d736f6369616c)\n- [Book GPT](https://github.com/fraserxu/book-gpt): drop a book, start asking question.¬†[![](https://camo.githubusercontent.com/6060f86aad12f6395748728471eccf47e34a979ca7a3b3de3cc4151c851cb044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66726173657278752f626f6f6b2d6770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/6060f86aad12f6395748728471eccf47e34a979ca7a3b3de3cc4151c851cb044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66726173657278752f626f6f6b2d6770743f7374796c653d736f6369616c)\n- [Chat LangchainJS](https://github.com/sullivan-sean/chat-langchainjs): NextJS version of Chat Langchain¬†[![](https://camo.githubusercontent.com/ee835cb4cdd9c898ed127ebeca4f49ff99e828f95cfb6dcd44e1f61c5a53d41f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73756c6c6976616e2d7365616e2f636861742d6c616e67636861696e6a733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/ee835cb4cdd9c898ed127ebeca4f49ff99e828f95cfb6dcd44e1f61c5a53d41f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73756c6c6976616e2d7365616e2f636861742d6c616e67636861696e6a733f7374796c653d736f6369616c)\n- [Doc Search](https://github.com/namuan/dr-doc-search): converse with book - Built with GPT-3¬†[![](https://camo.githubusercontent.com/d3d5fa76bb6d0423c7a9ab58dfe27ed6efdf2c195abd4d6f663427f27056ab1c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e616d75616e2f64722d646f632d7365617263683f7374796c653d736f6369616c)](https://camo.githubusercontent.com/d3d5fa76bb6d0423c7a9ab58dfe27ed6efdf2c195abd4d6f663427f27056ab1c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e616d75616e2f64722d646f632d7365617263683f7374796c653d736f6369616c)\n- [Fact Checker](https://github.com/jagilley/fact-checker): fact-checking LLM outputs with langchain¬†[![](https://camo.githubusercontent.com/4f9cacaadb278a862999316c57d35487cfc172c374dfe1e918b39ed7fe4051f8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6167696c6c65792f666163742d636865636b65723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/4f9cacaadb278a862999316c57d35487cfc172c374dfe1e918b39ed7fe4051f8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6167696c6c65792f666163742d636865636b65723f7374796c653d736f6369616c)\n- [MM ReAct](https://github.com/microsoft/MM-REACT): Multi Modal ReAct Design\n- [QABot](https://github.com/hardbyte/qabot): Query local or remote files or databases with natural language queries powered by langchain and openai¬†[![](https://camo.githubusercontent.com/99662d4f67ba8bbea0278d6e4d28a8e9c5238ab0dd985bf0dc5cf6461f57c6c1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68617264627974652f7161626f743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/99662d4f67ba8bbea0278d6e4d28a8e9c5238ab0dd985bf0dc5cf6461f57c6c1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68617264627974652f7161626f743f7374796c653d736f6369616c)\n- [GPT Automator](https://github.com/chidiwilliams/GPT-Automator): Your voice-controlled Mac assistant.¬†[![](https://camo.githubusercontent.com/55381e9fcb24b4d3e72ded6717a7d400b760d9d95707d4aaa978a30459565bde/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636869646977696c6c69616d732f4750542d4175746f6d61746f723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/55381e9fcb24b4d3e72ded6717a7d400b760d9d95707d4aaa978a30459565bde/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636869646977696c6c69616d732f4750542d4175746f6d61746f723f7374796c653d736f6369616c)\n- [Teams LangchainJS](https://github.com/SidU/teams-langchain-js): Demonstration of LangChainJS with Teams / Bot Framework bots¬†[![](https://camo.githubusercontent.com/ea86ee696c48e223c36ebfc0baa784a10cc3d4cce25d47b4c3d77834e27c10e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f536964552f7465616d732d6c616e67636861696e2d6a733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/ea86ee696c48e223c36ebfc0baa784a10cc3d4cce25d47b4c3d77834e27c10e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f536964552f7465616d732d6c616e67636861696e2d6a733f7374796c653d736f6369616c)\n- [ChatGPT](https://github.com/biff-ai/chatgpt-langchainjs-example): ChatGPT & langchain example for node.js & Docker¬†[![](https://camo.githubusercontent.com/dde2d69bc7dafbb5fc89bd0046314e8f73d0601807126d3cd22f43c6bdb874a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626966662d61692f636861746770742d6c616e67636861696e6a732d6578616d706c653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/dde2d69bc7dafbb5fc89bd0046314e8f73d0601807126d3cd22f43c6bdb874a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626966662d61692f636861746770742d6c616e67636861696e6a732d6578616d706c653f7374796c653d736f6369616c)\n- [FlowGPT](https://github.com/nilooy/flowgpt): Generate diagram with AI¬†[![](https://camo.githubusercontent.com/e15d208eab49cb5cf60063ca518ba0ccd9e462ba58d39d15d98a03cb0b027c88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e696c6f6f792f666c6f776770743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e15d208eab49cb5cf60063ca518ba0ccd9e462ba58d39d15d98a03cb0b027c88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e696c6f6f792f666c6f776770743f7374796c653d736f6369616c)\n- [langchain-text-summarizer](https://github.com/alphasecio/langchain-text-summarizer): A sample streamlit application summarizing text using LangChain¬†[![](https://camo.githubusercontent.com/96e633813d55ac19437cd9e1d0e2f9cebde40212ea3c0d5656599e303e3f934d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c706861736563696f2f6c616e67636861696e2d746578742d73756d6d6172697a65723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/96e633813d55ac19437cd9e1d0e2f9cebde40212ea3c0d5656599e303e3f934d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c706861736563696f2f6c616e67636861696e2d746578742d73756d6d6172697a65723f7374796c653d736f6369616c)\n- [Langchain Chat Websocket](https://github.com/pors/langchain-chat-websockets): About LangChain LLM chat with streaming response over websockets¬†[![](https://camo.githubusercontent.com/7861ad47175002b4345b10d9bc16cd1229eea9c6378d6652528437ac84afbf95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f706f72732f6c616e67636861696e2d636861742d776562736f636b6574733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/7861ad47175002b4345b10d9bc16cd1229eea9c6378d6652528437ac84afbf95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f706f72732f6c616e67636861696e2d636861742d776562736f636b6574733f7374796c653d736f6369616c)\n- [langchain_yt_tools](https://github.com/venuv/langchain_yt_tools): Langchain tools to search/extract/transcribe text transcripts of Youtube videos¬†[![](https://camo.githubusercontent.com/21fd2e1ca841205ca16498d558765d9e71eedb6cecb76df2e484e9fd8bbf1dae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76656e75762f6c616e67636861696e5f79745f746f6f6c733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/21fd2e1ca841205ca16498d558765d9e71eedb6cecb76df2e484e9fd8bbf1dae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76656e75762f6c616e67636861696e5f79745f746f6f6c733f7374796c653d736f6369616c)\n- [SmartPilot](https://github.com/jaredkirby/SmartPilot): A Python program leveraging OpenAI's language models to generate, analyze, and select the best answer to a given question¬†[![](https://camo.githubusercontent.com/e9e112b8ab909283debe21344eeed0e59cc18f921f1598a84da5c0c25bb2c98b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a617265646b697262792f536d61727450696c6f743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e9e112b8ab909283debe21344eeed0e59cc18f921f1598a84da5c0c25bb2c98b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a617265646b697262792f536d61727450696c6f743f7374796c653d736f6369616c)\n- [Howdol](https://github.com/bborn/howdoi.ai): a helpful chatbot that can answer questions¬†[![](https://camo.githubusercontent.com/b36d4996863f2bd44c45d58ca7984340ac863bc8b20ee7e5cf12eb54a13782fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62626f726e2f686f77646f692e61693f7374796c653d736f6369616c)](https://camo.githubusercontent.com/b36d4996863f2bd44c45d58ca7984340ac863bc8b20ee7e5cf12eb54a13782fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62626f726e2f686f77646f692e61693f7374796c653d736f6369616c)\n- [MrsStax](https://github.com/normandmickey/MrsStax): QA Slack Bot¬†[![](https://camo.githubusercontent.com/eb9978fa70e5f4d17926f90f0cf0df1f778eb195c1a1396e168bb91624423ba1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e6f726d616e646d69636b65792f4d7273537461783f7374796c653d736f6369616c)](https://camo.githubusercontent.com/eb9978fa70e5f4d17926f90f0cf0df1f778eb195c1a1396e168bb91624423ba1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e6f726d616e646d69636b65792f4d7273537461783f7374796c653d736f6369616c)\n- [ThoughtSource‚ö°](https://github.com/OpenBioLink/ThoughtSource): A framework for the science of machine thinking¬†[![](https://camo.githubusercontent.com/653a1cd642671af399ad8f5d73295542d50c562294449228a38a638c2448a090/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e42696f4c696e6b2f54686f75676874536f757263653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/653a1cd642671af399ad8f5d73295542d50c562294449228a38a638c2448a090/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e42696f4c696e6b2f54686f75676874536f757263653f7374796c653d736f6369616c)\n- [ChatGPT Langchain](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain): ChatGPT clone using langchain on Huggingface\n- [Chat Math Techniques](https://huggingface.co/spaces/JavaFXpert/gpt-math-techniques): langchain chat with math techniques on Huggingface\n- [Notion QA](https://github.com/hwchase17/notion-qa): Notion Question-Answering Bot¬†[![](https://camo.githubusercontent.com/75bda4b7eff8eb8fcbad3992822de97bfd27d774b6da8ae7f09a1102e4a4b81a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6e6f74696f6e2d71613f7374796c653d736f6369616c)](https://camo.githubusercontent.com/75bda4b7eff8eb8fcbad3992822de97bfd27d774b6da8ae7f09a1102e4a4b81a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6877636861736531372f6e6f74696f6e2d71613f7374796c653d736f6369616c)\n- [QNimGPT](https://huggingface.co/spaces/rituthombre/QNim): Play Nim against an IBM Quantum Computer simulator or OpenAI GPT-3.5¬†[![](https://camo.githubusercontent.com/e3542647046c43291c0c731d810161bb137c5af7736b67753f3f865ae2e48a5f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7269747574686f6d6272652f514e696d3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e3542647046c43291c0c731d810161bb137c5af7736b67753f3f865ae2e48a5f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7269747574686f6d6272652f514e696d3f7374796c653d736f6369616c)\n- [ChatPDF](https://github.com/akshata29/chatpdf): ChatGPT + Enterprise data with Azure OpenAI¬†[![](https://camo.githubusercontent.com/d5098faa9a4f6b26495a0362803705327055023b92e107993684aed0dfef0d71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616b736861746132392f636861747064663f7374796c653d736f6369616c)](https://camo.githubusercontent.com/d5098faa9a4f6b26495a0362803705327055023b92e107993684aed0dfef0d71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616b736861746132392f636861747064663f7374796c653d736f6369616c)\n- [Chat with Scanned Documents](https://github.com/tony-xlh/Chat-with-Scanned-Documents): A demo chatting with documents scanned with Dynamic Web TWAIN.\n- [snowChat¬†‚ùÑÔ∏è](https://github.com/kaarthik108/snowChat): Chat with you're snowflake database¬†[![](https://camo.githubusercontent.com/582065323c330dde6d02694d14e58f519f48c4a7cd524219ea2700de4bf35067/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b6161727468696b3130382f736e6f77436861743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/582065323c330dde6d02694d14e58f519f48c4a7cd524219ea2700de4bf35067/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b6161727468696b3130382f736e6f77436861743f7374796c653d736f6369616c)\n- [DB GPT](https://github.com/csunny/DB-GPT): Interact your data and environment using the local GPT, no data leaks, 100% privately, 100% security¬†[![](https://camo.githubusercontent.com/3bcfab955ea5ab9ecc0f8c6328861f8aff429cbfd068f58cdca34b61b6627fb6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6373756e6e792f44422d4750543f7374796c653d736f6369616c)](https://camo.githubusercontent.com/3bcfab955ea5ab9ecc0f8c6328861f8aff429cbfd068f58cdca34b61b6627fb6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6373756e6e792f44422d4750543f7374796c653d736f6369616c)\n- [Psychic](https://github.com/psychic-api/psychic): Universal APIs for unstructured data. Sync documents from SaaS tools to a SQL or vector database, where they can be easily queried by AI applications like ChatGPT.¬†[![](https://camo.githubusercontent.com/4216d2048d50dd82bf9c3130f90a30c4cdcf5636659278504fa1da8d276c069b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707379636869632d6170692f707379636869633f7374796c653d736f6369616c)](https://camo.githubusercontent.com/4216d2048d50dd82bf9c3130f90a30c4cdcf5636659278504fa1da8d276c069b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707379636869632d6170692f707379636869633f7374796c653d736f6369616c)\n- [Airtable-QnA](https://github.com/ikram-shah/airtable-qna):¬†üåü¬†a question-answering tool for your Airtable content\n- [Voyager](https://github.com/MineDojo/Voyager): An Open-Ended Embodied Agent with Large Language Models¬†[![](https://camo.githubusercontent.com/f5727e805f338b458d65c6474a276368a5d64648c3574cfdb774cef15d41951a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d696e65446f6a6f2f566f79616765723f7374796c653d736f6369616c)](https://camo.githubusercontent.com/f5727e805f338b458d65c6474a276368a5d64648c3574cfdb774cef15d41951a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d696e65446f6a6f2f566f79616765723f7374796c653d736f6369616c)\n- [WingmanAI](https://github.com/e-johnstonn/wingmanAI): tool for interacting with real-time transcription of both system and microphone audio\n\n## \n## Learn\n### \n### Notebooks\n- [Langchain Tutorials](https://github.com/gkamradt/langchain-tutorials): overview and tutorial of the LangChain Library¬†[![](https://camo.githubusercontent.com/642d3cd81c698a80bf78ad73b79767f0b6ab8f09c28fc8236b14f38ddf46227b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676b616d726164742f6c616e67636861696e2d7475746f7269616c733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/642d3cd81c698a80bf78ad73b79767f0b6ab8f09c28fc8236b14f38ddf46227b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676b616d726164742f6c616e67636861696e2d7475746f7269616c733f7374796c653d736f6369616c)\n- [LangChain Chinese Getting Started Guide](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide): Chinese LangChain Tutorial for Beginners¬†[![](https://camo.githubusercontent.com/1c2670db501e3ace6a3a812617a9f0a6dda558b6f7401c4108f6ed512b956c61/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c69616f6b6f6e675646582f4c616e67436861696e2d4368696e6573652d47657474696e672d537461727465642d47756964653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/1c2670db501e3ace6a3a812617a9f0a6dda558b6f7401c4108f6ed512b956c61/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c69616f6b6f6e675646582f4c616e67436861696e2d4368696e6573652d47657474696e672d537461727465642d47756964653f7374796c653d736f6369616c)\n- [Flan5 LLM](https://colab.research.google.com/drive/1AVh9dOsG9DKzfK7gOFrJuitPIcLPqlbO?usp=sharing): PDF QA using LangChain for chain of thought and multi-task instructions, Flan5 on HuggingFace\n- [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook): Pinecone / James Briggs' LangChain handbook\n- [Query the YouTube video transcripts](https://colab.research.google.com/drive/1sKSTjt9cPstl_WMZ86JsgEqFG-aSAwkn?usp=sharing): Query the YouTube video transcripts, returning timestamps as sources to legitimize the answers\n- [llm-lobbyist](https://github.com/JohnNay/llm-lobbyist): Large Language Models as Corporate Lobbyists\n- [Langchain Semantic Search](https://github.com/venuv/langchain_semantic_search): Search and indexing your own Google Drive Files using GPT3, LangChain, and Python\n- [GPT Political Compass](https://colab.research.google.com/drive/1xt2IsFPGYMEQdoJFNgWNAjWGxa60VXdV)\n- [llm-grovers-search-party](https://github.com/JavaFXpert/llm-grovers-search-party): Leveraging Qiskit, OpenAI and LangChain to demonstrate Grover's algorithm\n- [TextWorld ReAct Agent](https://colab.research.google.com/drive/19WTIWC3prw5LDMHmRMvqNV2loD9FHls6?usp=sharing)\n- [LangChain <> Wolfram Alpha](https://colab.research.google.com/drive/1AAyEdTz-Z6ShKvewbt1ZHUICqak0MiwR?usp=sharing)\n- [BYO Knowledge Graph](https://github.com/prof-frink-lab/slangchain/blob/main/docs/modules/knowledge_graph/examples/byo_knowledge_graph.ipynb)\n\n### \n### Videos\n- [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n- [LangChain Series by Sam Witteveen](https://www.youtube.com/watch?v=J_0qvRt4LNk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ)\n- [LangChain Tutorials Playlist](https://www.youtube.com/playlist?list=PL611FKPtL866MnlDPHvI3KwVGqCB-QJAx)\n- [LangChain James Briggs' Playlist](https://www.youtube.com/watch?v=nE2skSRWTTs&list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)\n- [What Is LangChain? - LangChain + ChatGPT Overview](https://www.youtube.com/watch?v=_v_fgW2SkkQ)\n- [LangChain Demo + Q&A with Harrison Chase](https://www.youtube.com/watch?v=zaYTXQFR0_s)\n- [LangChain for LLMs is... basically just an Ansible playbook](https://www.youtube.com/watch?v=X51N9C-OhlE)¬†(David Shapiro)\n- [Data Independent Playlist](https://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n- [Langchain Agent Webinar](https://www.crowdcast.io/c/46erbpbz609r)\n- [BabyAGI with LangChain](https://www.youtube.com/watch?v=DRgPyOXZ-oE)\n- [LangChain Tutorial in Python - Crash Course](https://www.python-engineer.com/posts/langchain-crash-course/)\n- [LangChain Crash Course: Build a AutoGPT¬†](https://www.youtube.com/watch?v=MlK6SIjcjE8)(Nicholas Renotte)\n- [LangChain and the Future of LLM Agents](https://www.youtube.com/watch?v=JwO08Pk6S_Q&t=4s)\n\n### \n### Articles\n- [Build a GitHub support bot with GPT3, LangChain, and Python](https://dagster.io/blog/chatgpt-langchain)\n- [The Emergence Of Large Language Model (LLM) API Build Frameworks](https://cobusgreyling.medium.com/the-emergence-of-large-language-model-llm-api-build-frameworks-78d83d68eeda)\n- [How I used LangChain¬†ü¶úüîó¬†and GPT-3 to Automate my Boss¬†ü§ñ](https://dev.to/ironcladdev/how-i-used-langchain-and-gpt-3-to-automate-my-boss-3bk4)\n- [Multilingual Semantic Search with Cohere and Langchain](https://txt.cohere.ai/search-cohere-langchain/)\n- [How Haystack and LangChain are Empowering Large Language Models](https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/)\n- [DataIndependent Tutorials](https://github.com/gkamradt/langchain-tutorials)\n- [Build an Ecommerce Chatbot With Redis, LangChain, and OpenAI](https://redis.com/blog/build-ecommerce-chatbot-with-redis/)\n- [Getting Started with LangChain: A Beginner‚Äôs Guide to Building LLM-Powered Applications](https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)\n- [How To Use LangChain with LLM Agent Monitoring To Fine-Tune Your LLM Apps](https://arize.com/blog-course/langchain-llm-agent-monitoring/)\n- [Build a Simple ChatGPT CLI with memory](https://getmetal.io/posts/07-tutorial-motorhead-cli)\n- [Deploy a Voice-Based Chatbot with BentoML, LangChain, and Gradio](https://towardsdatascience.com/deploy-a-voice-based-chatbot-with-bentoml-langchain-and-gradio-7f25af3e45df)\n- [LangChain tutorial at PromptChap](https://promptchap.com/)\n- [Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI](https://dylancastillo.co/code-interpreter-chatbot-pyodide-langchain-openai/)\n- [LangChain has added Cypher Search](https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5)\n- [Langchain Decoded](https://alphasec.io/langchain-decoded-the-muggles-guide-to-langchain/)\n- [Implementing GPT4All Locally with Python and Langchain](https://medium.datadriveninvestor.com/offline-ai-magic-implementing-gpt4all-locally-with-python-b51971ce80af)\n- [GPT your GDrive with LangChain](https://www.haihai.ai/gpt-gdrive/)\n\n## \n## Alternatives\n- [Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents): Provides a natural language API on top of transformers\n- [LlamaIndex](https://github.com/jerryjliu/llama_index): provides a central interface to connect your LLM's with external data.¬†[![](https://camo.githubusercontent.com/b71839c374e0f66d0a648dfbdcc50dc8f82a05190aabee3e7e1af8efbdd59448/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a657272796a6c69752f6c6c616d615f696e6465783f7374796c653d736f6369616c)](https://camo.githubusercontent.com/b71839c374e0f66d0a648dfbdcc50dc8f82a05190aabee3e7e1af8efbdd59448/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a657272796a6c69752f6c6c616d615f696e6465783f7374796c653d736f6369616c)\n- [Botpress](https://github.com/botpress/botpress): The building blocks for building chatbots¬†[![](https://camo.githubusercontent.com/e27eeef7b262ccf2d6a30dcb63d7f9df4042e6a9d096fe2239de721b898ea588/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626f7470726573732f626f7470726573733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e27eeef7b262ccf2d6a30dcb63d7f9df4042e6a9d096fe2239de721b898ea588/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626f7470726573732f626f7470726573733f7374796c653d736f6369616c)\n- [Haystack](https://github.com/deepset-ai/haystack): NLP framework to interact with your data using Transformer models and LLMs¬†[![](https://camo.githubusercontent.com/218024f0ff54369f855336782710773b791a9249b01c132a01d9d358d18b78d4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565707365742d61692f686179737461636b3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/218024f0ff54369f855336782710773b791a9249b01c132a01d9d358d18b78d4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565707365742d61692f686179737461636b3f7374796c653d736f6369616c)\n- [Semantic Kernel](https://github.com/microsoft/semantic-kernel): Microsoft C# SDK to integrate cutting-edge LLM technology quickly and easily into your apps¬†[![](https://camo.githubusercontent.com/0c8ed625bbebf26eafaaedf292e634000d328848a5a2ac2fcafbbd88733b3c60/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f73656d616e7469632d6b65726e656c3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/0c8ed625bbebf26eafaaedf292e634000d328848a5a2ac2fcafbbd88733b3c60/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f73656d616e7469632d6b65726e656c3f7374796c653d736f6369616c)\n- [Promptify](https://github.com/promptslab/Promptify): Prompt Engineering | Use GPT or other prompt based models to get structured output.¬†[![](https://camo.githubusercontent.com/fb7a18c8e3a6a388d9d8c051728dff99856954ad9577bb26dffb08800fb378ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726f6d7074736c61622f50726f6d70746966793f7374796c653d736f6369616c)](https://camo.githubusercontent.com/fb7a18c8e3a6a388d9d8c051728dff99856954ad9577bb26dffb08800fb378ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726f6d7074736c61622f50726f6d70746966793f7374796c653d736f6369616c)\n- [PromptSource](https://github.com/bigscience-workshop/promptsource): About Toolkit for creating, sharing and using natural language prompts.¬†[![](https://camo.githubusercontent.com/9ec4b56ec5cb8db9de1c882aec3288814dbaae61879d35b6371817c324dca465/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626967736369656e63652d776f726b73686f702f70726f6d7074736f757263653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/9ec4b56ec5cb8db9de1c882aec3288814dbaae61879d35b6371817c324dca465/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626967736369656e63652d776f726b73686f702f70726f6d7074736f757263653f7374796c653d736f6369616c)\n- [Agent-LLM](https://github.com/Josh-XT/Agent-LLM): An Artificial Intelligence Automation Platform.¬†[![](https://camo.githubusercontent.com/4649baddc026d521e526188b7195043cb5cc05e1ba8af77c7ed78a60984490ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a6f73682d58542f4167656e742d4c4c4d3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/4649baddc026d521e526188b7195043cb5cc05e1ba8af77c7ed78a60984490ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a6f73682d58542f4167656e742d4c4c4d3f7374796c653d736f6369616c)\n- [LLM Agents](https://github.com/mpaepper/llm_agents): Build agents which are controlled by LLMs¬†[![](https://camo.githubusercontent.com/038e74d24a91dd6f5399618900607ddf63607729032443fd40d54eac2a5b3fe7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d706165707065722f6c6c6d5f6167656e74733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/038e74d24a91dd6f5399618900607ddf63607729032443fd40d54eac2a5b3fe7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d706165707065722f6c6c6d5f6167656e74733f7374796c653d736f6369616c)\n- [MiniChain](https://github.com/srush/MiniChain): A tiny library for coding with large language models.¬†[![](https://camo.githubusercontent.com/1932058a2e0df4209bfc4f606e5a24f2fc08616dd2d6f01adc5f32c71b7fefeb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73727573682f4d696e69436861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/1932058a2e0df4209bfc4f606e5a24f2fc08616dd2d6f01adc5f32c71b7fefeb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73727573682f4d696e69436861696e3f7374796c653d736f6369616c)\n- [Griptape](https://github.com/griptape-ai/griptape): Python framework for AI workflows and pipelines with chain of thought reasoning, external tools, and memory.¬†[![](https://camo.githubusercontent.com/a0e1fbbcdb14d7c875753f1e2ccd59e2d543ed559a2fd7a65934324f1bd019b8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726970746170652d61692f67726970746170653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/a0e1fbbcdb14d7c875753f1e2ccd59e2d543ed559a2fd7a65934324f1bd019b8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726970746170652d61692f67726970746170653f7374796c653d736f6369616c)\n- [llm-chain](https://github.com/sobelio/llm-chain): is a powerful rust crate for building chains in LLMs allowing you to summarise text and complete complex tasks.¬†[![](https://camo.githubusercontent.com/c9be0faf35d5044a80d3761932cfc02316edaba778512d98a46c66d2d75ebcd9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736f62656c696f2f6c6c6d2d636861696e3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/c9be0faf35d5044a80d3761932cfc02316edaba778512d98a46c66d2d75ebcd9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736f62656c696f2f6c6c6d2d636861696e3f7374796c653d736f6369616c)\n- [BoxCars](https://github.com/BoxcarsAI/boxcars): Ruby gem, Building applications with composability using Boxcars with LLM's. Inspired by LangChain.¬†[![](https://camo.githubusercontent.com/e26e61c79121588cd06e0d6a66aa69f22fcd222bbe6eca0951d8e4ab6267c7b2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426f786361727341492f626f78636172733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/e26e61c79121588cd06e0d6a66aa69f22fcd222bbe6eca0951d8e4ab6267c7b2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426f786361727341492f626f78636172733f7374796c653d736f6369616c)\n- [LangTorch](https://github.com/Knowly-ai/langtorch): Building composable LLM applications with Java / JVM. Inspired by LangChain.¬†[![](https://camo.githubusercontent.com/11cfd1693f5867155bc206215563505310f50c4e065f6d5fc69b0569a1a41141/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b6e6f776c792d61692f6c616e67746f7263683f7374796c653d736f6369616c)](https://camo.githubusercontent.com/11cfd1693f5867155bc206215563505310f50c4e065f6d5fc69b0569a1a41141/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b6e6f776c792d61692f6c616e67746f7263683f7374796c653d736f6369616c)\n- [Langchain Go](https://github.com/tmc/langchaingo): Golang Langchain¬†[![](https://camo.githubusercontent.com/7725f4f6e2147276d02cc4f64d8100981661adbe6386341703f98efaa4f30cef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f746d632f6c616e67636861696e676f3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/7725f4f6e2147276d02cc4f64d8100981661adbe6386341703f98efaa4f30cef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f746d632f6c616e67636861696e676f3f7374796c653d736f6369616c)\n- [LangchainRb](https://github.com/andreibondarev/langchainrb): Ruby Langchain¬†[![](https://camo.githubusercontent.com/55c4a1afcfa5004b5db84b27598c8e48328b55aa5695dadac63fae8e4948fd89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e64726569626f6e64617265762f6c616e67636861696e72623f7374796c653d736f6369616c)](https://camo.githubusercontent.com/55c4a1afcfa5004b5db84b27598c8e48328b55aa5695dadac63fae8e4948fd89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e64726569626f6e64617265762f6c616e67636861696e72623f7374796c653d736f6369616c)\n- [PromptFlow](https://github.com/InsuranceToolkits/promptflow): Create executable flowcharts that link LLMs (Large Language Models), Prompts, Python functions, and conditional logic together.¬†[![](https://camo.githubusercontent.com/58247d00081ed0704506361fe79d30d783413e7ca30368c32fba2faf5ef9381a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f496e737572616e6365546f6f6c6b6974732f70726f6d7074666c6f773f7374796c653d736f6369616c)](https://camo.githubusercontent.com/58247d00081ed0704506361fe79d30d783413e7ca30368c32fba2faf5ef9381a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f496e737572616e6365546f6f6c6b6974732f70726f6d7074666c6f773f7374796c653d736f6369616c)\n- [OpenLM](https://github.com/r2d4/openlm): a drop-in OpenAI-compatible library that can call LLMs from any other hosted inference API. Also¬†[Typescript](https://github.com/r2d4/llm.ts)¬†[![](https://camo.githubusercontent.com/08a8691693033dcfd922016525b382cd1c1294551c75f98a3870a940cd56ffab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f723264342f6f70656e6c6d3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/08a8691693033dcfd922016525b382cd1c1294551c75f98a3870a940cd56ffab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f723264342f6f70656e6c6d3f7374796c653d736f6369616c)\n- [Dust](https://github.com/dust-tt/dust): Design and Deploy Large Language Model Apps¬†[![](https://camo.githubusercontent.com/704c7a65c2460f591aa534837bb4bc2229869e2aa32f086456c12a8e692c95d2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f647573742d74742f647573743f7374796c653d736f6369616c)](https://camo.githubusercontent.com/704c7a65c2460f591aa534837bb4bc2229869e2aa32f086456c12a8e692c95d2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f647573742d74742f647573743f7374796c653d736f6369616c)\n- [e2b](https://github.com/kyrolabs/awesome-langchain/blob/main): Open-source platform for building & deploying virtual developers‚Äô agents\n\n## \n## Complement to this list\n- [Open LLMs](https://github.com/eugeneyan/open-llms): A list of open LLMs available for commercial use¬†[![](https://camo.githubusercontent.com/080925de6c37772e871e90824e7e75f9c9a36c050d2c4f2bf8df6c15bfe101c6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657567656e6579616e2f6f70656e2d6c6c6d733f7374796c653d736f6369616c)](https://camo.githubusercontent.com/080925de6c37772e871e90824e7e75f9c9a36c050d2c4f2bf8df6c15bfe101c6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657567656e6579616e2f6f70656e2d6c6c6d733f7374796c653d736f6369616c)\n- [Awesome LLM](https://github.com/Hannibal046/Awesome-LLM): Awesome-LLM: a curated list of Large Language Model resources.¬†[![](https://camo.githubusercontent.com/2b0bebb7e6ae67d166ba4b605fcf73a02fb04a942805f8a81502614a7b99bca3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f48616e6e6962616c3034362f417765736f6d652d4c4c4d3f7374796c653d736f6369616c)](https://camo.githubusercontent.com/2b0bebb7e6ae67d166ba4b605fcf73a02fb04a942805f8a81502614a7b99bca3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f48616e6e6962616c3034362f417765736f6d652d4c4c4d3f7374796c653d736f6369616c)\n- [LLaMA Cult and More](https://github.com/shm007g/LLaMA-Cult-and-More): Keeping Track of Affordable LLMs,¬†ü¶ô¬†Cult and More¬†[![](https://camo.githubusercontent.com/9faa9c5e628ad7d24fc3ab07652ad3dada2e6b0922806bc2823aad70fca6ffb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73686d303037672f4c4c614d412d43756c742d616e642d4d6f72653f7374796c653d736f6369616c)](https://camo.githubusercontent.com/9faa9c5e628ad7d24fc3ab07652ad3dada2e6b0922806bc2823aad70fca6ffb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73686d303037672f4c4c614d412d43756c742d616e642d4d6f72653f7374796c653d736f6369616c)\n",
        "tags": [],
        "created": "2023-05-31T22:39:30.764Z",
        "updated": "2023-05-31T22:39:59.472Z"
    },
    {
        "id": "31Xd1zpADY4dsct90DkZ",
        "title": "Creating My Own ChatGPT Code Interpreter and Open Sourcing It",
        "markdown": "## Creating My Own ChatGPT Code Interpreter and Open Sourcing It\n\n[https://twitter.com/RickLamers/status/1659195480197459969](https://twitter.com/RickLamers/status/1659195480197459969)\n\n**ü™ÑSmart summary:**\nThe author wrote their own code interpreter for ChatGPT and open sourced it. They are excited to explore the possibilities of the code interpreter and share their enthusiasm with others.\n-------\n\nCouldn't get access to ChatGPT Code Interpreter so I wrote my own! And Open Sourced it of course üï∫\n\n[https://t.co/cxHIFRb1FO](https://t.co/cxHIFRb1FO)\n\nSoooo many ideas to try. It's addictive. \n\n![](https://pbs.twimg.com/media/Fwaxcr7aMAAr8-u.jpg)\n",
        "tags": [],
        "created": "2023-05-31T22:24:05.374Z",
        "updated": "2023-05-31T22:24:05.374Z"
    },
    {
        "id": "3yLE6MYv5b2mHaj4IVhX",
        "title": "Unlocking the Power of Metadata for LLM-Enabled QA Systems",
        "markdown": "## Unlocking the Power of Metadata for LLM-Enabled QA Systems\n\n[https://twitter.com/jerryjliu0/status/1660683176099078144](https://twitter.com/jerryjliu0/status/1660683176099078144)\n\n**ü™ÑSmart summary:**\nWhen building LLM-enabled QA systems, it is important to add the right metadata for each chunk of the document in order to improve performance. Docugami is a powerful tool that can automatically provide metadata, and an example of this is given with a question about the security deposit for a property owned by Birch Street. Without the right metadata, the answer is incorrect, but with the right metadata, the answer is correct.\n-------\n\nWhen building LLM-enabled QA systems, we chunk up the docs - can lose global context.\n\nAdding the right metadata for each chunk can improve performance by a LOT. \n\n@docugami is a powerful tool that can automatically provide metadata (now in LlamaHub!)\n\n[https://t.co/HSOCrOQuF5](https://t.co/HSOCrOQuF5)\n\n![](https://pbs.twimg.com/media/FwvudKKagAI38IY.jpg)\n\nAs an example, let‚Äôs ask ‚ÄúWhat is the security deposit for the property owned by Birch Street?‚Äù\n\nWith ‚Äúnaive‚Äù chunking without adding metadata, the landlord name and rentable area are not in the same chunk - you get back a non-answer. \n\n![](https://pbs.twimg.com/media/Fwvt7OJaAAE-kBG.jpg)\n\nWhen you add the right metadata (‚ÄúLandlord‚Äù, ‚ÄúTenant‚Äù), then you get back a correct answer!\n\n@docugami can do this automatically.\n\nCheck out the full notebook here: [https://t.co/FdIy9YevIh](https://t.co/FdIy9YevIh)\n\n![](https://pbs.twimg.com/media/Fwvt9cjaQAEuQI0.jpg)\n",
        "tags": [],
        "created": "2023-05-31T22:23:06.439Z",
        "updated": "2023-05-31T22:23:06.439Z"
    },
    {
        "id": "S6QHpLDBV5QR0NUDGiwT",
        "title": "Gorilla: Unleashing the Power of a Large Language Model Connected with Massive APIs",
        "markdown": "## Gorilla: Unleashing the Power of a Large Language Model Connected with Massive APIs\n\n[https://twitter.com/arankomatsuzaki/status/1661541064367386624](https://twitter.com/arankomatsuzaki/status/1661541064367386624)\n\n**ü™ÑSmart summary:**\nGorilla has released a finetuned language model based on LLaMA that outperforms GPT-4 in writing API calls. The project and abstract can be found at the provided links.\n-------\n\nGorilla: Large Language Model Connected with Massive APIs\n\nReleases Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.\n\nproj: [https://t.co/5ERbyaZ3ws](https://t.co/5ERbyaZ3ws)\nabs: [https://t.co/kYdjb8H5gN](https://t.co/kYdjb8H5gN)\n\n![](https://pbs.twimg.com/media/Fw76D3tWAAI2csM.jpg)\n\n@arankomatsuzaki\n",
        "tags": [],
        "created": "2023-05-31T22:23:06.412Z",
        "updated": "2023-05-31T22:23:06.412Z"
    },
    {
        "id": "qFEJlwVH6Q9FtjBopZlQ",
        "title": "Understanding and Training Without a GUI/CLI",
        "markdown": "## Understanding and Training Without a GUI/CLI\n\n[https://twitter.com/rashmigb/status/1662126535761960961](https://twitter.com/rashmigb/status/1662126535761960961)\n\n@tunguz This [https://t.co/ntK4WbiwUG](https://t.co/ntK4WbiwUG) is very well done - I was able to understand and train on my own without their GUI/CLI\n\n@rashmigb @tunguz\n",
        "tags": [],
        "created": "2023-05-31T22:23:03.647Z",
        "updated": "2023-05-31T22:23:03.647Z"
    },
    {
        "id": "gouGtNTok8zp6VmV8IGB",
        "title": "Unlocking the Power of Entity Memory with SQLite",
        "markdown": "## Unlocking the Power of Entity Memory with SQLite\n\n[https://twitter.com/LangChainAI/status/1663936000241049600](https://twitter.com/LangChainAI/status/1663936000241049600)\n\n**ü™ÑSmart summary:**\nEntity memory is a powerful concept that allows for the extraction of entities in a conversation, summarizing what is known about them, and adding that context to prompts. Zelzebu's addition makes it easy to use and persist entity memory locally using SQLite.\n-------\n\nüóÉÔ∏èEntity MemoryüóÉÔ∏è\n\nEntity memory is a powerful paradigm: extract entities in a conversation, summarize what's known about them, add that context to prompts.\n\nWith @zelzebu's addition, you can easily use and persist entity memory locally using SQLite ü•≥\n\n[https://t.co/mEtQksxfl6](https://t.co/mEtQksxfl6)\n\n![](https://pbs.twimg.com/media/Fxd82BKakAIZqMR.jpg)\n\n@LangChainAI @zelzebu\n",
        "tags": [],
        "created": "2023-05-31T20:29:05.084Z",
        "updated": "2023-05-31T20:29:05.085Z"
    },
    {
        "id": "rhyNQUWUoUEGWgFl1p0x",
        "title": "Introducing OpenChat: Create and Embed Custom ChatGPT-like Bots Anywhere!",
        "markdown": "## Introducing OpenChat: Create and Embed Custom ChatGPT-like Bots Anywhere!\n\n[https://twitter.com/ikbenbasha/status/1663896757271515139](https://twitter.com/ikbenbasha/status/1663896757271515139)\n\n**ü™ÑSmart summary:**\nOpenChat, an open-source tool, has been released. It allows users to create and run custom ChatGPT-like bots, and embed and share them anywhere. It also supports vector DB and can be self-hosted with one command. A video example is provided.\n-------\n\nüöÄüöÄüöÄüöÄüöÄ\nI published the first version of OpenChat, the open-source tool that allow you to run and create custom ChatGPT-like bots, and embed and share these bots anywhere.\n\nRepo: [https://t.co/dtMq8A7Yz1](https://t.co/dtMq8A7Yz1) Website: [https://t.co/QANrVaHyCR](https://t.co/QANrVaHyCR)\n\nDone with @LangChainAI @pinecone ü•∞\n\nYou can train your bot on any website and embed it as a chat widget! \n\n![](https://pbs.twimg.com/media/FxdY1veX0AI4030.jpg)\n\nsupport to vector DB was extremely easy to implement, and because of that, you can self-host OpenChat with one command!\n\nThe same goes for @LangChainAI, adding PDFs, websites, and soon many other sources was super easy!\n\nVideo example [https://t.co/smuiXoDtkq](https://t.co/smuiXoDtkq)\n",
        "tags": [],
        "created": "2023-05-31T17:22:06.216Z",
        "updated": "2023-05-31T17:22:06.216Z"
    },
    {
        "id": "KTAOFQY971WTzaAteo7M",
        "title": "Introducing Three New Document Loaders for LLM Apps",
        "markdown": "## Introducing Three New Document Loaders for LLM Apps\n\n[https://twitter.com/LangChainAI/status/1663566820933275656](https://twitter.com/LangChainAI/status/1663566820933275656)\n\n**ü™ÑSmart summary:**\nWe have added 3 new document loaders to make it easier to integrate data from any source into LLM apps. These include GitHub Document Loader, Trello Document Loader, and Spark DataFrame Document Loader. These loaders can be used for project management, loading cards from Trello boards, and retrieving information from DataFrames. Credit goes to UmerHAdil, musicaoriginal2, and rithwik-db for their contributions.\n-------\n\nüë©\u200düíºMore Data Loadersüë©\u200düíº\n\nLLMs + app-specific data = üéÜ\n\nWe want to make it super easy to integrate data from any source into your LLM app. So we've added 3 new document loaders, bringing the total count to > 100 üåù\n\nLoad from:\nüå≥@github\n‚úÖ@trello \nüñºÔ∏èSpark DataFrame\n\na üßµ:\n\nüå≥GitHub Document Loaderüå≥\n\nLoad issues and PR's from a GH repo.\n\nWe're already using this internally to help with project management.\n\ns/o to @UmerHAdil for yet another great contribution!!\n\n[https://t.co/x6MSgTaVHa](https://t.co/x6MSgTaVHa)\n\n![](https://pbs.twimg.com/media/FxYqShnaUAADPiN.jpg)\n\n‚úÖTrello Document Loader‚úÖ\n\nLevel up your project management *even more* by also loading in cards from your Trello boards.\n\nThanks to @musicaoriginal2 for the addition!\n\n[https://t.co/6UbhzjQwGK](https://t.co/6UbhzjQwGK)\n\n![](https://pbs.twimg.com/media/FxYrKZ1aUAIi1Oj.jpg)\n\nüñºÔ∏èSpark DataFrame Document LoaderüñºÔ∏è\n\nRetrieve information from your DataFrames with this loader, which lets you convert any Spark DF into documents that can be fed into a model.\n\nBig thanks to GH user rnthwik-db for adding!\n\n[https://t.co/imN97dzWPL](https://t.co/imN97dzWPL)\n\nrithwik-db**\n",
        "tags": [],
        "created": "2023-05-30T21:11:07.818Z",
        "updated": "2023-05-30T21:11:07.818Z"
    },
    {
        "id": "SmqDyzhwkFXvJFM6Un5R",
        "title": "Tips for Retrieval Augmented Generation",
        "markdown": "## Tips for Retrieval Augmented Generation\n\n[https://twitter.com/LangChainAI/status/1663646291254206464](https://twitter.com/LangChainAI/status/1663646291254206464)\n\nsome really good tips in this thread for retrieval augmented generation! [https://t.co/wQFbHPqF6i](https://t.co/wQFbHPqF6i)\n\n@LangChainAI\n",
        "tags": [],
        "created": "2023-05-30T20:54:09.598Z",
        "updated": "2023-05-30T20:54:09.598Z"
    },
    {
        "id": "shd2du2aqUr7yx8suXQ1",
        "title": "Searching Internal Docs with LangChainAI and MendableAI VSCode",
        "markdown": "## Searching Internal Docs with LangChainAI and MendableAI VSCode\n\n[https://twitter.com/ericciarla/status/1662193342707277824](https://twitter.com/ericciarla/status/1662193342707277824)\n\n**ü™ÑSmart summary:**\nI just used a search engine to quickly find internal documents like bug reports and product requirements while coding. This demo only required ingesting a Notion feature document and the GitHub repository for BabyAGIJS. I'm now considering the possibility of creating a Mendable AI Visual Studio Code extension.\n-------\n\nJust used [https://t.co/qeFS9ot7xH](https://t.co/qeFS9ot7xH) to search internal docs like bug reports and product requirements while coding.\n\nFor this demo all it took was ingesting a notion feature doc and the GitHub repo for BabyAGIJS.\n\nShout out to @LangChainAI for the awesome data loaders üöÄ. (1/2) [https://t.co/vf4l0RL0gO](https://t.co/vf4l0RL0gO)\n\nThis has me thinking, maybe a @mendableai VSCode would not be the worst idea ü§î. (2/2)\n",
        "tags": [],
        "created": "2023-05-26T23:37:06.227Z",
        "updated": "2023-05-26T23:37:06.227Z"
    },
    {
        "id": "KNz2UDqlXHbglvLNsMdl",
        "title": "New Features for LLM Caching, Texting, and GGML Models",
        "markdown": "## New Features for LLM Caching, Texting, and GGML Models\n\n[https://twitter.com/LangChainAI/status/1662138670332395520](https://twitter.com/LangChainAI/status/1662138670332395520)\n\n**ü™ÑSmart summary:**\nThis thread introduces three new features: Momento Cache for LLM caching and chain memory, Twilio tool for sending texts, and CTransformers wrapper for GGML models. Momento Cache reduces the burden of repetitive workloads, Twilio gives agents texting abilities, and C Transformers provides an easy interface to powerful models built with the GGML library.\n-------\n\nNew day, new (feature) delivery üì¶\n\nüìù Momento Cache for LLM caching and chain memory\nüì±Twilio tool for sending texts\nüß† CTransformers wrapper for GGML models\n\nLet's dive in! üßµ\n\nüìù Momento Cache\n\nLLM queries are expensive and slow, making it hard to scale LLM applications.\n\n@momentohq's serverless cache significantly reduces the burden for repetitive workloads.\n\nMore on the integration: [https://t.co/oPTBVKVP7G](https://t.co/oPTBVKVP7G)\n\nh/t to Michael Landis for contribution! \n\n![](https://pbs.twimg.com/media/FxEXEQBaIAAN7Fx.jpg)\n\nüì±Twilio\n\nGive your agents texting abilities! Great for things like personal assistants and personal CRMs.\n\nWhat are the use cases you're excited about?\n\ns/o to GH user Tedma4 for the addition!\n\nDocs: [https://t.co/ZxYZP81aIR](https://t.co/ZxYZP81aIR)\n\n![](https://pbs.twimg.com/media/FxEXzeMaQAIaGdf.jpg)\n\nüß† C Transformers\n\nThere's a new, powerful class of models built with the GGML library (LLaMA, Dolly v2, ...)\n\nC Transformers is an easy interface to these, and with this integration LangChain is, too!\n\ns/o to Ravindra Marella for the new interface!\n\nDocs: [https://t.co/gnvGTvNEVv](https://t.co/gnvGTvNEVv)\n\n![](https://pbs.twimg.com/media/FxEZG5FacAI_2LG.jpg)\n",
        "tags": [],
        "created": "2023-05-26T19:50:10.489Z",
        "updated": "2023-05-26T19:50:10.489Z"
    },
    {
        "id": "sXZgUemCVgPHf4OIZ5PP",
        "title": "Saving Costs with GPT-4 and Vector Databases",
        "markdown": "## Saving Costs with GPT-4 and Vector Databases\n\n[https://twitter.com/_areebpasha/status/1662138694462062606](https://twitter.com/_areebpasha/status/1662138694462062606)\n\n**ü™ÑSmart summary:**\nGPT-4 API calls can be expensive, so caching responses in a vector database like Pinecone is a good way to save costs. Redis can be used for implementation, but it can be complicated. Questions can be asked in the comments.\n-------\n\nWith GPT-4, it's expensive to keep making API calls. Caching your responses in a vector database is probably the best way to save on costs. As more queries come in, costs keep reducing over time as cached responses are returned.\nHow to do it with @LangChainAI and @pinecone :\n\n1. Save responses to a vector database like Pinecone.\n2. Get the similarity score. If the score > 0.8, then use the cached(saved) response, else generate it through API. \nI've seen implementations using redis, but I felt it was complicated. \nHave any questions, drop them below! \n\n![](https://pbs.twimg.com/media/FxEY1luXgAItLbB.jpg)\n",
        "tags": [],
        "created": "2023-05-26T18:44:06.129Z",
        "updated": "2023-05-26T18:44:06.129Z"
    },
    {
        "id": "dDfOscDd5KYLhfT5v5aT",
        "title": "Unlocking Endless Possibilities with GPT-4 in Minecraft: Introducing Voyager, the Lifelong Learning Agent",
        "markdown": "## Unlocking Endless Possibilities with GPT-4 in Minecraft: Introducing Voyager, the Lifelong Learning Agent\n\n[https://twitter.com/DrJimFan/status/1662115266933972993](https://twitter.com/DrJimFan/status/1662115266933972993)\n\n**ü™ÑSmart summary:**\nVoyager is a lifelong learning agent that plays Minecraft using GPT-4. It has three components: an iterative prompting mechanism, a skill library, and an automatic curriculum. Experiments show that Voyager discovers more items and travels further than other LLM-based agents. It is also able to construct complex 3D structures with the help of human feedback.\n-------\n\nWhat if we set GPT-4 free in Minecraft? ‚õèÔ∏è\n\nI‚Äôm excited to announce Voyager, the first lifelong learning agent that plays Minecraft purely in-context. Voyager continuously improves itself by writing, refining, committing, and retrieving *code* from a skill library.\n\nGPT-4 unlocks‚Ä¶ [https://t.co/SZU0n8nWsW[https://t.co/hjTxk6Qb1x](https://t.co/hjTxk6Qb1x)](https://t.co/SZU0n8nWsW)\n\nGenerally capable, autonomous agents are the next frontier of AI. They continuously explore, plan, and develop new skills in open-ended worlds, driven by survival & curiosity.\n\nMinecraft is by far the best testbed with endless possibilities for agents:\n\n[https://t.co/w7zyYvFr3z](https://t.co/w7zyYvFr3z)\n\nVoyager has 3 key components:\n\n1) An iterative prompting mechanism that incorporates game feedback, execution errors, and self-verification to refine programs;\n2) A skill library of code to store & retrieve complex behaviors;\n3) An automatic curriculum to maximize exploration. [https://t.co/T0trc55Hfg](https://t.co/T0trc55Hfg)\n\nFirst, Voyager attempts to write a program to achieve a particular goal, using a popular Javascript Minecraft API (Mineflayer). The program is likely incorrect at the first try. The game environment feedback and javascript execution error (if any) help GPT-4 refine the program. \n\n![](https://pbs.twimg.com/media/FxEFlcuagAA6pqd.jpg)\n\nSecond, Voyager incrementally builds a skill library by storing the successful programs in a vector DB. Each program can be retrieved by the embedding of its docstring. Complex skills are synthesized by composing simpler skills, which compounds Voyager‚Äôs capabilities over time. \n\n![](https://pbs.twimg.com/media/FxEFu9XacAAgDhC.jpg)\n\nThird, an automatic curriculum proposes suitable exploration tasks based on the agent‚Äôs current skill level & world state, e.g. learn to harvest sand & cactus before iron if it finds itself in a desert rather than a forest.\n\nThink of it as an in-context form of *novelty search*. \n\n![](https://pbs.twimg.com/media/FxEGAMLaAAAgnYb.jpg)\n\nPutting these all together, here‚Äôs the full data flow design that drives lifelong learning in a vast 3D voxel world without any human intervention. \n\n![](https://pbs.twimg.com/media/FxEGFw1akAMxNKQ.jpg)\n\nLet‚Äôs look at some experiments!\n\nWe evaluate Voyager systematically against other LLM-based agent techniques, such as ReAct, Reflexion, and the popular AutoGPT in Minecraft.\n\nVoyager discovers 63 unique items within 160 prompting iterations, 3.3x more than the next best approach. \n\n![](https://pbs.twimg.com/media/FxEGMUqaMAEhrME.png)\n\nThe novelty-seeking automatic curriculum naturally compels Voyager to travel extensively. Without being explicitly instructed to do so, Voyager traverses 2.3x longer distances and visits more terrains than the baselines, which are ‚Äúlazier‚Äù and often stuck in local areas. \n\n![](https://pbs.twimg.com/media/FxEGRFeacAAsANK.jpg)\n\nHow good is the ‚Äútrained model‚Äù, i.e. skill library after lifelong learning?\n\nWe clear the agent‚Äôs inventory/armors, spawn a new world, and test with unseen tasks. Voyager solves them significantly faster. Our skill library even boosts AutoGPT, since code is easily transferrable. \n\n![](https://pbs.twimg.com/media/FxEGVqRaIAAhocN.jpg)\n\nVoyager is currently text-only, but can be augmented by visual perception in the future. We do a preliminary study where humans act like an image captioning model and provide feedback to Voyager.\n\nIt is able to construct complex 3D structures, such as a Nether Portal and a house. [https://t.co/rQeFppxEQJ](https://t.co/rQeFppxEQJ)\n\nLet agents emerge in Minecraft! All open-source: [https://t.co/OUat4RAP17](https://t.co/OUat4RAP17)\n\nThis work is co-authored by my team at NVIDIA: @guanzhi_wang (our awesome intern), @yuqi_xie5, @YunfanJiang, @AjayMandlekar, @ChaoweiX, @yukez, @DrJimFan (myself, co-advisor), @AnimaAnandkumar (co-advisor). \n\n![](https://pbs.twimg.com/media/FxEGy5_aAAIo6Ye.jpg)\n",
        "tags": [],
        "created": "2023-05-26T18:43:08.426Z",
        "updated": "2023-05-26T18:43:08.426Z"
    },
    {
        "id": "VMHVmYXFgwkYJ0Z28LdT",
        "title": "Combining ChatGPT and GDrive with LangChainAI in 30 Lines of Python",
        "markdown": "## Combining ChatGPT and GDrive with LangChainAI in 30 Lines of Python\n\n[https://twitter.com/greggyb/status/1662093412676390912](https://twitter.com/greggyb/status/1662093412676390912)\n\n**ü™ÑSmart summary:**\nWith LangChainAI, it is possible to create powerful applications with just 30 lines of Python code that combine ChatGPT and GDrive.\n-------\n\nStrong agree! \n\nYou can combine ChatGPT + GDrive in 30 lines of Python with @LangChainAI. It's an absurdly powerful foundation for an infinite number of great apps. \n\n[https://t.co/zjPvUwBWF7[https://t.co/zsPkCvyV2H[https://t.co/4Q33nnCkES](https://t.co/4Q33nnCkES)](https://t.co/zsPkCvyV2H)](https://t.co/zjPvUwBWF7)\n\n@greggyb @LangChainAI\n",
        "tags": [],
        "created": "2023-05-26T18:42:05.830Z",
        "updated": "2023-05-26T18:42:05.830Z"
    },
    {
        "id": "JmKz8tmDAPOw5OvzP8aE",
        "title": "https://www.reddit.com/r/OpenAI/comments/13scry1/_/",
        "markdown": "[https://www.reddit.com/r/OpenAI/comments/13scry1/_/](https://www.reddit.com/r/OpenAI/comments/13scry1/_)\n\nAfter building a few tools utilizing OpenAI, I noticed a few things that many folks might not be aware of. Things that could be adding up to 30% unnecessary costs to your OpenAI usage. So, I thought I'd share what I've learned:\n1Ô∏è‚É£ **Ensure your JSON is as lean as possible**: OpenAI bills per token, and that includes whitespaces and line breaks in your JSON responses. If you eliminate these extras both in sending and receiving data, you might save up between 30%-50%! You simply need to tell OpenAI to \"return JSON in a single-line without whitespaces\". Boom!\n**Example:** This [Pokemon API JSON response](https://pokeapi.co/api/v2/pokemon?limit=3) is 210 tokens. After minifying it, we dropped to 117 tokens! This is almost 50% money saved.\n**Edit:** As mentioned by [u/brucebay](https://www.reddit.com/user/brucebay/), CSV format is also a good idea. No indentation and less repetitive characters.\n2Ô∏è‚É£ **Set temperature to 0 for structured responses**: When expecting a structured response (like JSON), setting the temperature parameter to 0 helps the model strictly stick to your expected JSON structure. This will prevent cases where you expect JSON, but something went wrong, and OpenAI responds with \"Sorry, I am not sure I can ...\".\n3Ô∏è‚É£ **Robots don't need you to be polite**: Computers understand simple instructions well. Trimming redundant/filler words from your prompt can not only save money but also speed up execution. Words like \"please\", \"kindly\", \"really\", \"very\", and so on, can often be dropped without losing accuracy.\n**Tip:** You can use the [OpenAI Tokenizer](https://platform.openai.com/tokenizer) to count the tokens of your requests/responses.\nTo help solve these problems, I've created Pezzo ([https://pezzo.ai](https://pezzo.ai)). It's an open-source (Apache 2.0 license) tool that helps anyone write better prompts. It also helps with observability, troubleshooting, collaboration and cost breakdown of your prompts. Using the Pezzo testing tool, you can design you prompts and test to see exactly how much they will cost, how many tokens they will use, and how long it'll take them to execute.\nüåü Check it out on GitHub here (and show your support by giving a star): [https://github.com/pezzolabs/pezzo](https://github.com/pezzolabs/pezzo)\nBy the way, here's a sneak peak to the features we're adding soon - suggestions for reducing costs, eliminating bias and increasing performance in your prompts. Let me know what you think!\n",
        "tags": [],
        "created": "2023-05-26T16:05:17.848Z",
        "updated": "2023-05-26T16:05:49.946Z"
    },
    {
        "id": "VM7c2Ua0uVeA5zD45L0s",
        "title": "New Supernote",
        "markdown": "\n",
        "tags": [],
        "created": "2023-05-26T16:04:05.709Z",
        "updated": "2023-05-26T16:04:10.676Z"
    },
    {
        "id": "ZZcMRGwE3JtyIjacZVaq",
        "title": "New Supernote",
        "markdown": "\n",
        "tags": [],
        "created": "2023-05-15T19:45:55.017Z",
        "updated": "2023-05-15T19:46:05.384Z"
    },
    {
        "id": "PJuT87g0ratyf0pp8m3j",
        "title": "Remind me to create a meeting on May 16th to go over design details for Ephemeral Charts Tickets w/ Brent, Ginesh, and Courtney.",
        "markdown": "Remind me to create a meeting on May 16th to go over design details for Ephemeral Charts Tickets w/ Brent, Ginesh, and Courtney.\n",
        "tags": [],
        "created": "2023-05-15T19:44:37.867Z",
        "updated": "2023-05-15T19:45:18.897Z"
    },
    {
        "id": "evI4vl4n2h4a0JA6KrIr",
        "title": "Make a chatgpt plugin for github. Something that can create github actions pipelines from natural langues, create repos, ect.",
        "markdown": "Make a chatgpt plugin for github. Something that can create github actions pipelines from natural langues, create repos, ect. \nWill need the GitHub REST API:\n[https://docs.github.com/en/rest?apiVersion=2022-11-28](https://docs.github.com/en/rest?apiVersion=2022-11-28)\n\nChatGPT Plugin docs:\n[https://platform.openai.com/docs/plugins/introduction](https://platform.openai.com/docs/plugins/introduction)\n",
        "tags": [],
        "created": "2023-05-15T19:31:25.597Z",
        "updated": "2023-05-15T19:33:19.126Z"
    },
    {
        "id": "ztzqYs7IiCAr9wZMqNkt",
        "title": "Quality AI Twitter Posts You Shouldn't Miss",
        "markdown": "## Quality AI Twitter Posts You Shouldn't Miss\n\n[https://twitter.com//status/1657057456319762434](https://twitter.com//status/1657057456319762434)\n\n**ü™ÑSmart summary:**\nThis thread provides a list of quality posts from AI Twitter, including links to papers, talks, and other resources. It is intended to provide a counterpoint to the low-quality content that has been flooding the platform recently. The thread also suggests that the author may curate more quality content in the future.\n-------\n\nAI Twitter is flooded with low-quality stuff recently. No, GPT is not ‚Äúdethroned‚Äù. And thin wrapper apps are not ‚Äúinsane‚Äù. At all.\n\nI feel obligated to surface some quality posts I bookmarked. Every one of them should've been promoted 10x, but ¬Ø\\_(„ÉÑ)_/¬Ø\n\nIn no particular order: \n\n![](https://pbs.twimg.com/media/Fv8IoXFaEAAlj1o.png)\n\nIf you only have 1 seat to follow in AI Twitter, don't give that seat to me. Give it to @karpathy. \n\nAndrej has the best take, by far, on the landscape of the open-source LLM ecosystem.\n\n1/\n[https://t.co/H3kghRzvXf](https://t.co/H3kghRzvXf)\n\nDon't watch AI FOMO and fear-mongering videos on YouTube. Watch the excellent talk from John Schulman @johnschulman2, creator of RLHF that powers GPT-4. Now *this* qualifies as \"insane\" ingenuity, if you ask me.\n\nLink: [https://t.co/bPqFx9tshr](https://t.co/bPqFx9tshr)\n\n2/\n[https://t.co/46vdqLFiA3](https://t.co/46vdqLFiA3)\n\nSasha has written some of my favorite NLP papers. Read about how attention could be an overkill in some cases, and alternative architectures for foundation models. Attention may not be all you need.\n\n3/\n[https://t.co/6VgfYo4GRI](https://t.co/6VgfYo4GRI)\n\nRead the excellent cookbook for self-supervised learning, from the godfather of deep learning himself @ylecun. The recipes in this freely available paper will truly help your business. Not the \"top 10 ChatGPT tricks you shouldn't miss\".\n\n4/\n[https://t.co/MVBpIA1MSk](https://t.co/MVBpIA1MSk)\n\nRead this rock solid deep dive thread from Loubna, author of StarCoder from @huggingface. I called StarCode the \"Llama moment for coding models\". It is the best open-source Codex you can get. ‚Äú10 outrageous GPT-4 prompts‚Äù are outrageously useless.\n\n5/\n[https://t.co/4g9HwZwDFT](https://t.co/4g9HwZwDFT)\n\n(with @percyliang) releases RedPajama-3B and 7B. They are Llama-grade models with commercially friendly license (Apache 2.0). Integrate them as your base model to avoid legal troubles.\n\n6/\n[https://t.co/q29EiwytQn](https://t.co/q29EiwytQn)\n\nInstruct-BLIP, a new multimodal open-source chat model. Led by @stevenhoi's team. Steven's works (BLIP series @SFResearch) are some of my favorite papers in multimodal foundation models. They provided inspiration for my own work Prismer at NVIDIA.\n\n7/\n[https://t.co/e9BgT38umN](https://t.co/e9BgT38umN)\n\nAlex @unixpickle is one of the few researchers still publishing at OpenAI. Read his text-to-3D papers \"Shap-E\" and \"Point-E\". 3D generative models are starting to work but far from mature, which means they are inevitably the future trend.\n\n8/\n[https://t.co/HRQXTFv6wB](https://t.co/HRQXTFv6wB)\n\nGabriel @gabrielpeyre makes some of the best math visualizations I've seen here.\n\n9/\n[https://t.co/QL3qfUqsRX](https://t.co/QL3qfUqsRX)\n\nKevin @kevin_zakka makes some of the best robotics simulations I've seen here.\n\n10/\n[https://t.co/x9tKpOqfGQ](https://t.co/x9tKpOqfGQ)\n\nIf people like this format, I‚Äôll try to curate quality contents more (with my own take) in the future. This is the job that Twitter‚Äôs algorithm is supposed to do, but it‚Äôs easily gamed by crappy engagement hacks. I guess I‚Äôll manually correct it for a while.\n\nEND/ \n\n![](https://pbs.twimg.com/media/Fv8MLAMacAEZNHg.jpg)\n",
        "tags": [],
        "created": "2023-05-12T18:00:17.269Z",
        "updated": "2023-05-12T18:00:17.269Z"
    },
    {
        "id": "Au99ByofIsXUZFBsuIYJ",
        "title": "Amazing OpenAI Prompts for AI Developers, GPT Lawyers, and More",
        "markdown": "## Amazing OpenAI Prompts for AI Developers, GPT Lawyers, and More\n\n[https://twitter.com/SullyOmarr/status/1648444218342535169](https://twitter.com/SullyOmarr/status/1648444218342535169)\n\n**ü™ÑSmart summary:**\nThis thread provides a list of interesting prompts from OpenAI's closed Discord server. These prompts include Codepup 3.0, GPT Lawyer, PromptGPT, and MidJourney Image Prompt Creator. The thread also provides instructions on how to properly format image prompts.\n-------\n\nOpenAI's discord is filled with amazing prompts. \n\nUnfortunately it's closed (full). \n\nHere are some of the more interesting ones i've found (GPT4 - prompts in alt text)\n\nCodepup 3.0 - AI Developer \n\n![](https://pbs.twimg.com/media/FuBxkJIaAAAtUBT.jpg)\n\nGPT Lawyer (basic one, make sure to replace it with your city) \n\n![](https://pbs.twimg.com/media/FuBxhodacAQsaou.png)\n\nPromptGPT \n\n![](https://pbs.twimg.com/media/FuByHDsakAU1w50.jpg)\n\nMidJourney Image Prompt Creator (continued in 2nd prompt) \n\n![](https://pbs.twimg.com/media/FuByd5IakAAMiEj.jpg)\n\nOk looks like it was cutoff heres the rest (long ass prompt lol)\n9.This is salt ‚Äú::3‚Äù and these are ‚Äú::2‚Äù,‚Äù::1‚Äù pepper, use accordingly. [https://t.co/W6oES83OFr](https://t.co/W6oES83OFr) commas or quotations in the image prompt. 11.When constructing an image prompt, follow the proper format: /imagine:‚Ä¶ [https://t.co/ackGLedecE](https://t.co/ackGLedecE)\n",
        "tags": [],
        "created": "2023-05-12T16:00:19.556Z",
        "updated": "2023-05-12T16:00:19.556Z"
    },
    {
        "id": "F9bF6cnL6ErOQgUmJ44C",
        "title": "CompressGPT: 70% Token Usage Reduction with a One-Line Change",
        "markdown": "## CompressGPT: 70% Token Usage Reduction with a One-Line Change\n\n[https://twitter.com/yasyf/status/1648753559561998337](https://twitter.com/yasyf/status/1648753559561998337)\n\n**ü™ÑSmart summary:**\nVictor Taelin's experiments around compressing GPT prompts have been used to create CompressGPT, which can reduce token usage by 70% with just one line of code in LangChainAI. Thanks to hwchase17 for creating an easy framework to build on.\n-------\n\nTook @VictorTaelin‚Äôs awesome experiments around compressing GPT prompts and built CompressGPT. A one-line change in your @LangChainAI code => 70% token usage deduction!\n\n[https://t.co/P3wuFEb2P2](https://t.co/P3wuFEb2P2)\n\ncc @hwchase17 thx as always for creating such an easy framework to build on top of!\n",
        "tags": [],
        "created": "2023-05-12T16:00:18.407Z",
        "updated": "2023-05-12T16:00:18.407Z"
    },
    {
        "id": "LrY2wwz4UFJXfXqDlTZE",
        "title": "Unlocking the Power of Claude with 100K Context Windows",
        "markdown": "## Unlocking the Power of Claude with 100K Context Windows\n\n[https://twitter.com/AnthropicAI/status/1656700154190389248](https://twitter.com/AnthropicAI/status/1656700154190389248)\n\n**ü™ÑSmart summary:**\nClaude is a machine learning tooling that can now analyze up to 100,000 tokens of text, corresponding to around 75K words. It can help retrieve information from business documents, quickly consume and get up to speed on dense material like research papers, and answer questions. It is currently in beta and will be priced at standard API pricing rates.\n-------\n\nIntroducing 100K Context Windows! We‚Äôve expanded Claude‚Äôs context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days. [https://t.co/4WLEp7ou7U](https://t.co/4WLEp7ou7U)\n\nWe fed Claude-Instant The Great Gatsby (72K tokens), except we modified one line to say that Mr. Carraway was \"a software engineer that works on machine learning tooling at Anthropic.\" We asked the model to spot what was added - it responded with the right answer in 22 seconds.\n\nClaude can help retrieve information from business documents. Drop multiple documents or even a book into the prompt and ask Claude questions that require synthesis of knowledge across many parts of the text.\n\nWith 100K context windows, you can: Digest, summarize, and explain dense documents like financial statements or business reports.\n\nRead through hundreds of pages of developer documentation to get quick answers to technical questions. [https://t.co/xy9ya1omhg](https://t.co/xy9ya1omhg)\n\nLastly, quickly consume and get up to speed on dense material like research papers. [https://t.co/GRopRjoscZ](https://t.co/GRopRjoscZ)\n\nWe‚Äôve made this available to our business partners, and are excited to see what they build. Read more here: [https://t.co/r1OSARYyFe](https://t.co/r1OSARYyFe)\n\nTo answer your questions: Right now, our 100K context window is a beta feature and will be priced at our standard API pricing rates during this period!\n",
        "tags": [],
        "created": "2023-05-12T15:00:18.737Z",
        "updated": "2023-05-12T15:00:18.737Z"
    },
    {
        "id": "OQ2PgReNiDIVMxz3o3eE",
        "title": "New Supernote",
        "markdown": "\n",
        "tags": [],
        "created": "2023-05-11T18:31:11.575Z",
        "updated": "2023-05-11T18:31:11.575Z"
    },
    {
        "id": "jkPyxRlfoqkVBaZBegvp",
        "title": "Welcome to Mem (Beta)!",
        "markdown": "# Welcome to Mem (Beta)!\n#tutorials\n\nMem is the fastest way to capture, share and make use of information.\n\nHere, we call the fundamental unit of information a \"mem\" (lowercase m), each of which is a flexible container of information. A mem can be a short reminder (which you can snooze), a meeting note, a product design doc, or a codified piece of knowledge.\n\nOn the surface, we've tried to build something as simple and lightweight as Apple Notes. In the background, Mem is powered by a collaborative knowledge graph ‚Äî which helps you form relationships between the information you capture, generate relevant - sometimes unexpected - insights, and share with anyone. \n### \n## Get started with this 7 minute video\n\n### [https://mem.ai/onboarding/getting-started-video](https://mem.ai/onboarding/getting-started-video)\n\n## Need help, or want to share feedback?\n\nVisit [support.mem.ai](https://support.mem.ai) to search our docs for what you need. Share feedback with us [here](https://support.mem.ai/share-feedback-form), or request support [here](https://support.mem.ai/request-support) ‚Äî¬†we'd love to hear from you! \n\n\nHappy memming!\n\n‚ÄìThe Mem Team\n",
        "tags": [],
        "created": "2023-05-11T18:31:10.422Z",
        "updated": "2023-05-11T18:31:10.422Z"
    },
    {
        "id": "ZvU2orexaIEadygL8aG2",
        "title": "How to create and edit mems",
        "markdown": "# How to create and edit mems\n\n \n### Creating mems\n\n**On desktop (app or browser):**\n- [ ] To create a new mem in the Mem app, just start typing anywhere (if you're already editing a mem, press **‚åòN** on a Mac, **Ctrl+N** on Windows).\n- [ ] To create a new mem in browser, press **Ctrl+N** to begin, on Mac or Windows.\n\n**On iPhone:**\n- [ ] To create a new mem in the iPhone app, select the **Write** icon in the bottom-right corner of the screen and then begin typing.\n\nTo download the TestFlight iOS app, go to [mem.ai/iOS](http://mem.ai/iOS) on your mobile browser.\n\n**From Spotlight (on desktop):**\n- [ ] To create a new mem from anywhere on your desktop, press **‚åò‚áßSpace **to open Mem Spotlight, and then hit Tab to open a new mem (Windows users can activate Mem Spotlight by pressing **Ctrl‚áßSpace**). If you are viewing a webpage, have text selected in an application, or both, you will have the option to save that webpage or selected text to a new mem (hit **Enter**, then **‚åòEnter**), or to an existing mem (select \"Send to an existing mem\" on the left-hand side of the window, then search for the relevant mem in the search bar).\n\n\n**Focus mode:**\n- [ ] Turn on **Focus Mode** to remove the Mem interface while you are typing, helping you to concentrate on what you are writing at the minute. Simply move your cursor to exit the mode when you need to see surrounding information on the screen. Preferences for **Focus Mode** can be changed by clicking on your **Profile** in the top left-hand corner of the screen and selecting **Settings**.\n\n### \n### **Formatting mems**\nFormat text with keyboard shortcuts, Markdown, or the formatting toolbar. Discover more in the [Markdown Guide](https://www.markdownguide.org/basic-syntax) or pull up Mem's keyboard shortcuts by clicking the ? in the bottom right corner.\n\n**Basic formatting (on desktop app and in browser):**\n- [ ] When selecting any text, you'll see a formatting toolbar that pops up that lets you style the text (with _italics_, **bold**, strikethrough, and more)\n\n\n- [ ] Create a large header (H1) by typing # followed by a SPACE\n\n\n- [ ] Create a medium header (H2) with ## followed by a SPACE\n\n\n- [ ] Create a small header (H3) with ### followed by a SPACE\n\n\n- [ ] Try **bolding** text by surrounding it with asterisks **YourText** (The usual ‚åò + B on selected text works here as well.)\n\n\n- [ ] Try _italicizing_ text with *YourText* , or use ‚åò + I on selected text\n\n\n- [ ] Try creating a code block with ```\n\n\n- [ ] Try creating a quote block with >\n\n\n- [ ] Create a divider with ---\n\n\n- [ ] Create a todo list by typing [] followed by a SPACE\n\n\n- [ ] Create a bulleted list by typing - followed by a SPACE\n\n\n- [ ] Create a numbered list by typing 1. followed by a SPACE\n\n\n**Adding content to mems:**\n\n- [ ] Add an image from your computer by typing **/image** and hitting **Enter**.\n\n\n- [ ] Attach a file from your computer by typing **/file **and hitting **Enter**.\n\n\n**Templates:**\n\n- [ ] Select **Flows **on the left-hand side of the screen, followed by **Templates**, to create your own custom templates for repeated use within Mem.\n\n\nAfter creating a template, you can access it in a variety of ways:\n\n- By typing the template name at the start of a mem\n- By typing **/** followed by the template name anywhere within a mem\n- By searching in Mem with **‚åòK** or the search bar at the top of the screen\n- By searching with Mem Spotlight using **‚åò‚áßSPACE **on Mac or **Ctrl‚áßSpace **on Windows.\n\n\n**Daily mems:**\n- [ ] Select **Flows **on the left-hand sidebar, followed by **Daily Mem**, to create and customize a Daily Mem template for yourself. The Daily Mem will appear at the top of your timeline on the days and at the time you select in the fields at the top of the customization screen.\n\n\n\n- [ ] Extra credit: Try navigating to the next tutorial with search using ‚åò + K and searching for \"How to link and organize mems\".\n",
        "tags": [
            "tutorials"
        ],
        "created": "2023-05-11T18:30:10.323Z",
        "updated": "2023-05-11T18:30:10.323Z"
    },
    {
        "id": "jp8RySn3AoAfTjq38ggV",
        "title": "Searching in Mem",
        "markdown": "# Searching in Mem\n#tutorials\n\n\nYou can search in Mem according to a variety of different criteria. As well as being able to conduct basic text-matching searches, such as for specific tags or individuals, you can also conduct conversationally-phrased searches for:\n\n- mems that contain links (e.g.¬†_\"has Google Drive links\", ‚Äúhas Figma links‚Äù_)\n- mems from specific dates or within specific date ranges (e.g. _\"from July 2019\"_)\n- mems sent via SMS (e.g. _\"sent from text\"_)\n- mems that have been edited by specific individuals (e.g._\"edited by Isabella\"_)\n- mems that include tasks (e.g. _\"contains checklist\"_)\n- mems that have been shared with particular Groups (e.g. _\"Shared with @Acme Corp_\")\n- mems that include multimedia content (e.g. _\"contains GIFs\", \"contains photos\"_)\n- mems that include code (e.g. _\"contains code\"_)\n",
        "tags": [],
        "created": "2023-05-11T18:29:10.323Z",
        "updated": "2023-05-11T18:29:10.323Z"
    },
    {
        "id": "GgVGtGqvCiP7kaIxdDww",
        "title": "How to link and organize mems",
        "markdown": "# How to link and organize mems\n\n\nWhile you can use search to find what you need in Mem, there are two main ways to link and organize your knowledge ‚Äî with **collections** and **linked mems.**\n\n### **Collections**\nCollections allow you to create lightweight collections to organize your mems. They're more flexible than folders because a mem can be part of multiple collections instead of just being associated with one folder. Example collections might be \"meetings\", \"network\", and \"home\".\n\nWhen creating a new mem and adding it to a collection, any mems that appear in the same collections will appear on the right-hand side of the screen under **Collections this mem is in**. \n### Linking between mems\nYou can link to other mems by typing the @ key. This allows you to link to an existing mem, or, create a new mem and link to it ‚Äî all in one go.\n\n- [ ] Try linking to the \"How to create and edit mems\" mem now by typing @ in the blank space below and searching for it.\n\n\n**üöÄ Workflow tip: **Use linked mems to **keep track of important people and companies** that you regularly meet with. Here's how (or, watch this [1-minute video](https://www.youtube.com/watch?v=CwKPiGd1ozc)).\n\n1. Let's say you're about to have your first meeting with Caroline Frank who works at Acme Corp. Type First meeting with @Acme Corp\n2. On the next line, type Attendees: @Caroline Frank\n3. You'll see that mems for Acme Corp and Caroline Frank have been created. You can click into these mems to take down notes about that person or company. (E.g. for Acme Corp, you can write \"Founded in 2019\", \"Headquartered in Silicon Valley\".)\n4. You now have your meeting notes mem with Acme Corp, along with individual mems for Acme Corp and Caroline Frank.\n\n\n1. The next week, you have your second meeting with Acme Corp and Caroline Frank. You can type Second meeting with @Acme Corp and Attendees: @Caroline Frank again. You'll see that on the right pane, your meeting notes from the first meeting show up, right at your fingertips.\n\n\nMake sure to type @ to keep referencing the same people and companies to keep your mems organized and connected.\n### Related mems\n\nAs noted above, **Related** mems displays mems that share one or multiple of the same collectins as that of the mem you are currently viewing. **Related** mems also displays mems that mention the word contained within the collection(s) on the mem you are currently viewing. For example, a mem that is part of the \"twitter\" collection will also surface mems that contain the word \"twitter\". These will be displayed in a separate panel on the right-hand side of the screen, under the heading \"Mentions [name of collection]\".\n\n\n![](https://storage.googleapis.com/memvp-25499.appspot.com/images/onboarding.pngd4ec8eaa-0ffa-4949-bdf0-32504dc62814)\n\n\nYou can also find mems that directly link to the mem you are viewing and mems that are created from (or linked to) previous occurrences of recurring **Calendar** events under **Related** mems.\n\nHover over any of the mems displayed on the right-hand side of the screen to preview their contents.\n",
        "tags": [
            "tutorials"
        ],
        "created": "2023-05-11T18:28:10.251Z",
        "updated": "2023-05-11T18:28:10.251Z"
    },
    {
        "id": "pdNFNY0Dd9YTxKpFdNUO",
        "title": "How to share and collaborate",
        "markdown": "# How to share and collaborate\n\n\nSharing and collaborating on work in Mem is easy and seamless, whether or not your collaborators have a pre-existing Mem account. You can also publish mems to a public URL, meaning that anyone with the link to the mem can view it online.\n### Sharing a mem\n\n**With an individual:**\n\nTo share a mem with someone (with or without a Mem account), you can do one of two things. Either:\n- Type **@** followed by the individual's first name and last name (**@FirstName LastName**). If they have a Mem account already, you'll be able to select their name from the pop-up that appears on screen. If they don't, select **Invite FirstName LastName** within the pop-up and then enter their email address on the following screen to invite them to view/edit the mem you're sharing.\n\n![](https://storage.googleapis.com/memvp-25499.appspot.com/images/image.png2c2724cc-18c7-4dcc-9571-6a34050567fa)\n\n- Click on the blue **Share **button at the top of the screen, and start typing the name or email address of the person you want to share the mem with in the search field.\n\n![](https://storage.googleapis.com/memvp-25499.appspot.com/images/image.pngcfba4c7d-96fc-4042-a266-eb035abf5f0e)\n\n\n\n\n**With a group:**\nTo share a mem with multiple people, select **Groups** in the left-hand sidebar and either select a pre-existing group or select **Create new group **in the top right-hand corner. Choose a name for the group, and then add the individuals you want to it. You'll then be able to share any mem with every person in that group simultaneously by typing **@GroupName** into the relevant mem.\n\n**To a public URL:**\nTo publish a mem to a public URL that anyone can view, click on the blue **Share **status icon at the top of the editor and then select **Publish**. You will then receive a public link that you can copy and share with others. Individuals with this link will be able to view the mem in publishing mode, but will not be able to edit it.\n\n### Adding comments\n\nYou can add comments to shared or private mems by selecting a portion of text and pressing **‚åò‚áßM**, or by clicking on the gray **lightning bolt** icon that appears in the right-hand margin next to text you are currently editing, and selecting **Comment**. Once you have written your comment, you can either select **Post as comment**, allowing everyone shared in the mem to see it, or **Keep private** if you do not want others shared in the mem to be able to view it.\n",
        "tags": [
            "tutorials"
        ],
        "created": "2023-05-11T18:27:10.250Z",
        "updated": "2023-05-11T18:27:10.250Z"
    }
]